{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d51e03b3-7d30-41e7-8512-bea5df8501d2",
   "metadata": {},
   "source": [
    "# THE FOLDER\n",
    "### All intermediate and final data is created in this folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0532ea6-1c1c-469c-9eb8-43609bb11a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "THEFOLDER = \"C:\\\\Users\\\\USERNAME\\\\OneDrive\\\\Desktop\\\\GlobalPCL4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7f5544-3b11-488c-b420-791434284510",
   "metadata": {},
   "source": [
    "# Tile western Conus PCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e8c5fd-6008-4371-8cc3-987d743dd172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import product\n",
    "import rasterio as rio\n",
    "from rasterio import windows\n",
    "\n",
    "in_path = \"C:\\\\Users\\\\USERNAME\\\\OneDrive\\\\Desktop\\\\PCLCONUS\\\\Input\\\\PCL\\\\\"\n",
    "input_filename = 'pcl_west_wgs.tif'\n",
    "\n",
    "out_path = f\"{THEFOLDER}\\\\PCLTILES\\\\\"\n",
    "output_filename = 'pcltile_{}-{}.tif'\n",
    "\n",
    "widthtile = 5000\n",
    "heighttile = 5000\n",
    "\n",
    "def get_tiles(ds, width=widthtile, height=heighttile):\n",
    "    nols, nrows = ds.meta['width'], ds.meta['height']\n",
    "    offsets = product(range(0, nols, width), range(0, nrows, height))\n",
    "    big_window = windows.Window(col_off=0, row_off=0, width=nols, height=nrows)\n",
    "    for col_off, row_off in offsets:\n",
    "        window = windows.Window(col_off=col_off, row_off=row_off, width=width, height=height).intersection(big_window)\n",
    "        transform = windows.transform(window, ds.transform)\n",
    "        yield window, transform\n",
    "\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "tile_numbers = []\n",
    "\n",
    "with rio.open(os.path.join(in_path, input_filename)) as inds:\n",
    "    tile_width, tile_height = widthtile, heighttile\n",
    "    nodata = inds.nodata\n",
    "    meta = inds.meta.copy()\n",
    "    for window, transform in get_tiles(inds):\n",
    "        data = inds.read(window=window)\n",
    "        if nodata is not None and not (data == nodata).all():\n",
    "            meta['transform'] = transform\n",
    "            meta['width'], meta['height'] = window.width, window.height\n",
    "            tile_number = f\"{int(window.col_off)}-{int(window.row_off)}\"\n",
    "            tile_numbers.append(tile_number)\n",
    "            outpath = os.path.join(out_path, output_filename.format(int(window.col_off), int(window.row_off)))\n",
    "            with rio.open(outpath, 'w', **meta) as outds:\n",
    "                outds.write(data)\n",
    "\n",
    "# Print or store the tile numbers\n",
    "TILENUMBER = tile_numbers\n",
    "\n",
    "del in_path, input_filename, tile_numbers\n",
    "del out_path, output_filename, widthtile, heighttile, tile_width, tile_height\n",
    "del meta, nodata, window, inds, get_tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2515cc9-8689-421c-9ed2-c1badb3b9342",
   "metadata": {},
   "source": [
    "# Downlaod training data and create training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abea73d6-2bbd-49e7-908e-f1dd5fd8158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from osgeo import gdal\n",
    "import rioxarray\n",
    "import planetary_computer\n",
    "from pystac_client import Client\n",
    "import osmnx as ox\n",
    "import rasterio\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.windows import from_bounds, Window\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "from shapely.geometry import box, Point\n",
    "from geopandas import GeoDataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.plot import show\n",
    "\n",
    "#TILENUMBER = ['75000-35000', '75000-40000', '75000-45000']\n",
    "TILENUMBER = ['75000-35000', '75000-40000']\n",
    "\n",
    "CHIP_SIZE = 128\n",
    "\n",
    "def delete_non_resampled_files(resampled_files, tif_dir):\n",
    "    for file in os.listdir(tif_dir):\n",
    "        if file not in resampled_files and file.endswith('.tif'):\n",
    "            file_path = os.path.join(tif_dir, file)\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted: {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete {file_path}: {e}\")\n",
    "\n",
    "def process_dem(tif_path, tif_dir, tile_number):\n",
    "    tif_data = rioxarray.open_rasterio(tif_path)\n",
    "    bbox_of_interest = tif_data.rio.bounds()\n",
    "    catalog = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
    "    search = catalog.search(collections=[\"cop-dem-glo-30\"], bbox=bbox_of_interest)\n",
    "    items = list(search.get_items())\n",
    "    \n",
    "    def process_item(item, idx):\n",
    "        signed_asset = planetary_computer.sign(item.assets[\"data\"])\n",
    "        data = rioxarray.open_rasterio(signed_asset.href).squeeze().drop(\"band\")\n",
    "        data.rio.write_crs(\"EPSG:4326\", inplace=True)\n",
    "        output_tif_path = os.path.join(tif_dir, f\"output_dataDEM_{idx}.tif\")\n",
    "        data.rio.to_raster(output_tif_path)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        for i, item in enumerate(items):\n",
    "            executor.submit(process_item, item, i)\n",
    "\n",
    "    output_tif = os.path.join(tif_dir, f\"outputtile_DEM_{tile_number}.tif\")\n",
    "    merge_command = [\n",
    "        \"python\", \"C:\\\\Users\\\\USERNAME\\\\anaconda3\\\\envs\\\\globalpcl\\\\Scripts\\\\gdal_merge.py\",\n",
    "        \"--config\", \"CHECK_DISK_FREE_SPACE\", \"FALSE\",\n",
    "        \"-o\", output_tif,\n",
    "        \"-n\", \"-9999\", \"-a_nodata\", \"-9999\"] + glob.glob(os.path.join(tif_dir, \"output_dataDEM_*.tif\"))\n",
    "\n",
    "    process_hag = subprocess.run(merge_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    if process_hag.returncode != 0:\n",
    "        print(f\"Error in merging DEM: {process_hag.stderr}\")\n",
    "        return None\n",
    "\n",
    "    src_ds = gdal.Open(output_tif, gdal.GA_ReadOnly)\n",
    "    target_ds = gdal.Open(tif_path, gdal.GA_ReadOnly)\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    output_resampled_path = os.path.join(tif_dir, f\"output_resampled_dataDEM_{tile_number}.tif\")\n",
    "    out_ds = driver.Create(output_resampled_path, target_ds.RasterXSize, target_ds.RasterYSize, 1, src_ds.GetRasterBand(1).DataType)\n",
    "    out_ds.SetGeoTransform(target_ds.GetGeoTransform())\n",
    "    out_ds.SetProjection(target_ds.GetProjection())\n",
    "    gdal.ReprojectImage(src_ds, out_ds, src_ds.GetProjection(), target_ds.GetProjection(), gdal.GRA_Bilinear)\n",
    "    src_ds, target_ds, out_ds = None, None, None\n",
    "\n",
    "    os.remove(output_tif)\n",
    "    for tif in glob.glob(os.path.join(tif_dir, \"output_dataDEM_*.tif\")):\n",
    "        try:\n",
    "            os.remove(tif)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {tif}: {e}\")\n",
    "\n",
    "    return output_resampled_path\n",
    "\n",
    "    del tif_data, bbox_of_interest, catalog, search, items\n",
    "    del output_tif, merge_command, process_hag\n",
    "    del src_ds, target_ds, driver, output_resampled_path\n",
    "\n",
    "\n",
    "def process_lidar(tif_path, tif_dir, tile_number):\n",
    "    lidar_dir = r\"C:\\Users\\USERNAME\\OneDrive\\Desktop\\GlobalData\\LIDAR2\"\n",
    "    lidar_tifs = glob.glob(os.path.join(lidar_dir, \"*.tif\"))\n",
    "\n",
    "    # Get the bounding box of the input tif_path\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        bbox = src.bounds\n",
    "        input_geom = box(bbox.left, bbox.bottom, bbox.right, bbox.top)\n",
    "\n",
    "    # Find overlapping LIDAR tiles\n",
    "    overlapping_tifs = []\n",
    "    for tif in lidar_tifs:\n",
    "        with rasterio.open(tif) as src:\n",
    "            lidar_bbox = src.bounds\n",
    "            lidar_geom = box(lidar_bbox.left, lidar_bbox.bottom, lidar_bbox.right, lidar_bbox.top)\n",
    "            if input_geom.intersects(lidar_geom):\n",
    "                overlapping_tifs.append(tif)\n",
    "\n",
    "    if not overlapping_tifs:\n",
    "        print(f\"No overlapping LIDAR tiles found for {tile_number}\")\n",
    "        return None\n",
    "\n",
    "    output_tif = os.path.join(tif_dir, f\"outputtile_lidar_{tile_number}.tif\")\n",
    "    merge_command = [\n",
    "        \"python\", \"C:\\\\Users\\\\USERNAME\\\\anaconda3\\\\envs\\\\globalpcl\\\\Scripts\\\\gdal_merge.py\",\n",
    "        \"--config\", \"CHECK_DISK_FREE_SPACE\", \"FALSE\",\n",
    "        \"-o\", output_tif,\n",
    "        \"-n\", \"255\", \"-a_nodata\", \"255\"] + overlapping_tifs\n",
    "\n",
    "    process_hag = subprocess.run(merge_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    if process_hag.returncode != 0:\n",
    "        print(f\"Error in merging LIDAR: {process_hag.stderr}\")\n",
    "        return None\n",
    "\n",
    "    src_ds = gdal.Open(output_tif, gdal.GA_ReadOnly)\n",
    "    target_ds = gdal.Open(tif_path, gdal.GA_ReadOnly)\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    output_resampled_path = os.path.join(tif_dir, f\"output_resampled_dataLIDAR_{tile_number}.tif\")\n",
    "    out_ds = driver.Create(output_resampled_path, target_ds.RasterXSize, target_ds.RasterYSize, 1, src_ds.GetRasterBand(1).DataType)\n",
    "    out_ds.SetGeoTransform(target_ds.GetGeoTransform())\n",
    "    out_ds.SetProjection(target_ds.GetProjection())\n",
    "    gdal.ReprojectImage(src_ds, out_ds, src_ds.GetProjection(), target_ds.GetProjection(), gdal.GRA_Bilinear)\n",
    "    src_ds, target_ds, out_ds = None, None, None\n",
    "\n",
    "    os.remove(output_tif)\n",
    "    return output_resampled_path\n",
    "\n",
    "    del lidar_tifs, bbox, input_geom, overlapping_tifs, lidar_bbox, lidar_geom\n",
    "    del output_tif, merge_command, process_hag\n",
    "    del src_ds, target_ds, driver, output_resampled_path\n",
    "\n",
    "\n",
    "def process_rivers(tif_path, tif_dir, tile_number):\n",
    "    dem_data = rioxarray.open_rasterio(tif_path)\n",
    "    bbox = dem_data.rio.bounds()\n",
    "    custom_filter = '[\"waterway\"~\"river\"]'\n",
    "    graph = ox.graph_from_bbox(bbox[3], bbox[1], bbox[2], bbox[0], custom_filter=custom_filter, simplify=True, retain_all=True, truncate_by_edge=True)\n",
    "    gdf = ox.graph_to_gdfs(graph, nodes=False)\n",
    "\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        window = from_bounds(*src.bounds, src.transform)\n",
    "        transform = rasterio.windows.transform(window, src.transform)\n",
    "        raster = np.zeros((int(window.height), int(window.width)), dtype=np.uint8)\n",
    "        shapes = ((geom, 1) for geom in gdf['geometry'])\n",
    "        burned = rasterize(shapes, out=raster, fill=0, transform=transform, all_touched=True)\n",
    "        distance_grid = scipy.ndimage.distance_transform_edt(burned == 0)\n",
    "        decay_grid = np.exp(-0.07 * distance_grid)\n",
    "\n",
    "        clipped_meta = src.meta.copy()\n",
    "        clipped_meta.update({\"driver\": \"GTiff\", \"height\": int(window.height), \"width\": int(window.width), \"transform\": transform, \"dtype\": rasterio.float32, \"count\": 1, \"compress\": 'lzw'})\n",
    "        output_path = os.path.join(tif_dir, f'exponential_decay_CO_river_{tile_number}.tif')\n",
    "        with rasterio.open(output_path, 'w', **clipped_meta) as dst:\n",
    "            dst.write(decay_grid.astype(np.float32), 1)\n",
    "\n",
    "    src_ds = gdal.Open(output_path, gdal.GA_ReadOnly)\n",
    "    target_ds = gdal.Open(tif_path, gdal.GA_ReadOnly)\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    output_resampled_path = os.path.join(tif_dir, f\"output_resampled_dataRivers_{tile_number}.tif\")\n",
    "    out_ds = driver.Create(output_resampled_path, target_ds.RasterXSize, target_ds.RasterYSize, 1, src_ds.GetRasterBand(1).DataType)\n",
    "    out_ds.SetGeoTransform(target_ds.GetGeoTransform())\n",
    "    out_ds.SetProjection(target_ds.GetProjection())\n",
    "    gdal.ReprojectImage(src_ds, out_ds, src_ds.GetProjection(), target_ds.GetProjection(), gdal.GRA_Bilinear)\n",
    "    src_ds, target_ds, out_ds = None, None, None\n",
    "    os.remove(output_path)\n",
    "\n",
    "    return output_resampled_path\n",
    "\n",
    "    del dem_data, bbox, custom_filter, graph, gdf\n",
    "    del window, transform, raster, shapes, burned, distance_grid, decay_grid\n",
    "    del clipped_meta, output_path\n",
    "    del src_ds, target_ds, driver, output_resampled_path\n",
    "\n",
    "\n",
    "def process_roads(tif_path, tif_dir, tile_number):\n",
    "    extent_data = rioxarray.open_rasterio(tif_path)\n",
    "    bbox = extent_data.rio.bounds()\n",
    "    graph = ox.graph_from_bbox(bbox[3], bbox[1], bbox[2], bbox[0], network_type='drive', simplify=True)\n",
    "    gdf = ox.graph_to_gdfs(graph, nodes=False)\n",
    "\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        window = from_bounds(*src.bounds, src.transform)\n",
    "        transform = rasterio.windows.transform(window, src.transform)\n",
    "        raster = np.zeros((int(window.height), int(window.width)), dtype=np.uint8)\n",
    "        shapes = ((geom, 1) for geom in gdf['geometry'])\n",
    "        burned = rasterize(shapes, out=raster, fill=0, transform=transform, all_touched=True)\n",
    "        distance_grid = scipy.ndimage.distance_transform_edt(burned == 0)\n",
    "        decay_grid = np.exp(-0.07 * distance_grid)\n",
    "\n",
    "        clipped_meta = src.meta.copy()\n",
    "        clipped_meta.update({\"driver\": \"GTiff\", \"height\": int(window.height), \"width\": int(window.width), \"transform\": transform, \"dtype\": rasterio.float32, \"count\": 1, \"compress\": 'lzw'})\n",
    "        output_path = os.path.join(tif_dir, f'exponential_decay_CO_roads_{tile_number}.tif')\n",
    "        with rasterio.open(output_path, 'w', **clipped_meta) as dst:\n",
    "            dst.write(decay_grid.astype(np.float32), 1)\n",
    "\n",
    "    src_ds = gdal.Open(output_path, gdal.GA_ReadOnly)\n",
    "    target_ds = gdal.Open(tif_path, gdal.GA_ReadOnly)\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    output_resampled_path = os.path.join(tif_dir, f\"output_resampled_dataRoads_{tile_number}.tif\")\n",
    "    out_ds = driver.Create(output_resampled_path, target_ds.RasterXSize, target_ds.RasterYSize, 1, src_ds.GetRasterBand(1).DataType)\n",
    "    out_ds.SetGeoTransform(target_ds.GetGeoTransform())\n",
    "    out_ds.SetProjection(target_ds.GetProjection())\n",
    "    gdal.ReprojectImage(src_ds, out_ds, src_ds.GetProjection(), target_ds.GetProjection(), gdal.GRA_Bilinear)\n",
    "    src_ds, target_ds, out_ds = None, None, None\n",
    "    os.remove(output_path)\n",
    "\n",
    "    return output_resampled_path\n",
    "\n",
    "    del extent_data, bbox, graph, gdf\n",
    "    del window, transform, raster, shapes, burned, distance_grid, decay_grid\n",
    "    del clipped_meta, output_path\n",
    "    del src_ds, target_ds, driver, output_resampled_path\n",
    "\n",
    "\n",
    "def generate_random_points(geometry, num_points):\n",
    "    points = []\n",
    "    min_x, min_y, max_x, max_y = geometry.bounds\n",
    "    while len(points) < num_points:\n",
    "        random_point = Point(np.random.uniform(min_x, max_x), np.random.uniform(min_y, max_y))\n",
    "        if random_point.within(geometry):\n",
    "            points.append(random_point)\n",
    "    return points\n",
    "\n",
    "    del bounds, crs, img, rect, buffered_rect, random_points\n",
    "    del gdf_points, gdf_points_wgs84, lat_long\n",
    "\n",
    "\n",
    "def process_chips(tif_path, tif_dir, lat_long, chip_size=128):\n",
    "    resampled_lidar_path = os.path.join(tif_dir, f\"output_resampled_dataLIDAR_{tile_number}.tif\")\n",
    "    resampled_dem_path = os.path.join(tif_dir, f\"output_resampled_dataDEM_{tile_number}.tif\")\n",
    "    resampled_rivers_path = os.path.join(tif_dir, f\"output_resampled_dataRivers_{tile_number}.tif\")\n",
    "    resampled_roads_path = os.path.join(tif_dir, f\"output_resampled_dataRoads_{tile_number}.tif\")\n",
    "\n",
    "    training_chips_dir = os.path.join(THEFOLDER, \"trainingchips\")\n",
    "    os.makedirs(training_chips_dir, exist_ok=True)\n",
    "\n",
    "    for i, (lat, lon) in enumerate(lat_long):\n",
    "        try:\n",
    "            paths = [resampled_lidar_path, resampled_dem_path, resampled_rivers_path, resampled_roads_path, tif_path]\n",
    "            labels = ['lidar', 'dem', 'rivers', 'roads', 'pcllabels']\n",
    "            \n",
    "            for path, label in zip(paths, labels):\n",
    "                with rasterio.open(path) as src:\n",
    "                    col, row = src.index(lon, lat)\n",
    "                    window = Window(col - chip_size // 2, row - chip_size // 2, chip_size, chip_size)\n",
    "                    chip_data = src.read(1, window=window)\n",
    "                    \n",
    "                    out_meta = src.meta.copy()\n",
    "                    out_meta.update({\n",
    "                        \"driver\": \"GTiff\",\n",
    "                        \"height\": chip_size,\n",
    "                        \"width\": chip_size,\n",
    "                        \"transform\": src.window_transform(window)\n",
    "                    })\n",
    "\n",
    "                    chip_output_dir = os.path.join(training_chips_dir, label)\n",
    "                    os.makedirs(chip_output_dir, exist_ok=True)\n",
    "                    \n",
    "                    chip_output_path = os.path.join(chip_output_dir, f\"{label.upper()}_Chip_{tile_number}_{i}.tif\")\n",
    "\n",
    "                    if chip_data.shape == (chip_size, chip_size):# and np.any(chip_data != src.nodata):\n",
    "                        with rasterio.open(chip_output_path, \"w\", **out_meta) as dest:\n",
    "                            dest.write(chip_data, 1)\n",
    "                    else:\n",
    "                        print(f\"Skipping {label} chip {i} because it is not properly shaped or is filled with nodata.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing chip {i}: {e}\")\n",
    "            \n",
    "    del resampled_lidar_path, resampled_dem_path, resampled_rivers_path, resampled_roads_path\n",
    "    del training_chips_dir, paths, labels, col, row, window, chip_data, out_meta, chip_output_dir, chip_output_path\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for tile_number in TILENUMBER:\n",
    "        tif_path = f\"{THEFOLDER}\\\\PCLTILES\\\\pcltile_{tile_number}.tif\"\n",
    "        tif_dir = f\"{THEFOLDER}\\\\TIFFOUTPUT\\\\{tile_number}\"\n",
    "        os.makedirs(tif_dir, exist_ok=True)\n",
    "\n",
    "        resampled_files = [\n",
    "            process_dem(tif_path, tif_dir, tile_number),\n",
    "            process_lidar(tif_path, tif_dir, tile_number),\n",
    "            process_rivers(tif_path, tif_dir, tile_number),\n",
    "            process_roads(tif_path, tif_dir, tile_number)\n",
    "        ]\n",
    "\n",
    "        delete_non_resampled_files([os.path.basename(f) for f in resampled_files], tif_dir)\n",
    "\n",
    "        # Generate random points within the tile bounds\n",
    "        with rasterio.open(tif_path) as src:\n",
    "            bounds = src.bounds\n",
    "            crs = src.crs\n",
    "            img = src.read(1)\n",
    "\n",
    "        rect = box(bounds.left, bounds.bottom, bounds.right, bounds.top)\n",
    "        buffered_rect = rect.buffer(-0.15)\n",
    "        # THE NUMBER OF RANDOM POINTS\n",
    "        random_points = generate_random_points(buffered_rect, 5000)\n",
    "        gdf_points = GeoDataFrame(geometry=random_points, crs=crs).to_crs(crs)\n",
    "        gdf_points_wgs84 = gdf_points.to_crs(epsg=4326)\n",
    "        lat_long = gdf_points_wgs84.geometry.apply(lambda geom: (geom.y, geom.x)).tolist()\n",
    "\n",
    "        process_chips(tif_path, tif_dir, lat_long)\n",
    "\n",
    "        print(f\"Processing for tile {tile_number} completed.\")\n",
    "\n",
    "    print(\"All processing completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c661ed08-dac4-46e6-a7c9-90f28403b5ce",
   "metadata": {},
   "source": [
    "# Load Image Chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e94e298-e4b8-4914-aaa4-242d7fceb121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "training_chips_dir = os.path.join(THEFOLDER, \"trainingchips\")\n",
    "\n",
    "# Paths to datasets\n",
    "featurepath1 = os.path.join(training_chips_dir, \"lidar\")\n",
    "featurepath2 = os.path.join(training_chips_dir, \"dem\")\n",
    "featurepath3 = os.path.join(training_chips_dir, \"roads\")\n",
    "featurepath4 = os.path.join(training_chips_dir, \"rivers\")\n",
    "labelspath = os.path.join(training_chips_dir, \"pcllabels\")\n",
    "\n",
    "# Function to load GeoTIFF images as numpy arrays\n",
    "def load_geotiff(path):\n",
    "    with rasterio.open(path) as src:\n",
    "        data = src.read(1)\n",
    "        if np.all(data == 0):\n",
    "            return None  # Return None if the image is all zeros\n",
    "        return data\n",
    "\n",
    "# Function to load and print progress\n",
    "def load_images(path, skip_zeros=False):\n",
    "    files = [f for f in os.listdir(path) if f.endswith('.tif')]\n",
    "    images = []\n",
    "    for i, f in enumerate(files):\n",
    "        image = load_geotiff(os.path.join(path, f))\n",
    "        if image is not None:\n",
    "            images.append(image)\n",
    "        elif skip_zeros:\n",
    "            print(f\"Skipping {f} because it is all zeros\")\n",
    "        if (i + 1) % 5000 == 0:\n",
    "            print(f\"Loaded {i + 1} images from {path}\")\n",
    "    return images\n",
    "\n",
    "# Load datasets\n",
    "hag_images = load_images(featurepath1)\n",
    "dem_images = load_images(featurepath2, skip_zeros=True)  # Skip DEM images that are all zeros\n",
    "roads_images = load_images(featurepath3)\n",
    "rivers_images = load_images(featurepath4)\n",
    "label_images = load_images(labelspath)\n",
    "\n",
    "# Ensure all datasets have the same number of images\n",
    "min_length = min(len(hag_images), len(dem_images), len(roads_images), len(rivers_images), len(label_images))\n",
    "hag_images = hag_images[:min_length]\n",
    "dem_images = dem_images[:min_length]\n",
    "roads_images = roads_images[:min_length]\n",
    "rivers_images = rivers_images[:min_length]\n",
    "label_images = label_images[:min_length]\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "hag_images = np.array(hag_images).astype('float32')\n",
    "dem_images = np.array(dem_images).astype('float32')\n",
    "roads_images = np.array(roads_images).astype('float32')\n",
    "rivers_images = np.array(rivers_images).astype('float32')\n",
    "label_images = np.array(label_images).astype('float32')\n",
    "\n",
    "# Normalize images independently\n",
    "hag_max = hag_images.max()\n",
    "dem_max = dem_images.max()\n",
    "roads_max = roads_images.max()\n",
    "rivers_max = rivers_images.max()\n",
    "\n",
    "hag_images /= hag_max\n",
    "dem_images /= dem_max\n",
    "roads_images /= roads_max\n",
    "rivers_images /= rivers_max\n",
    "\n",
    "print(f\"HAG max value: {hag_max}\")\n",
    "print(f\"DEM max value: {dem_max}\")\n",
    "print(f\"Roads max value: {roads_max}\")\n",
    "print(f\"Rivers max value: {rivers_max}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2e8c64-e369-46ef-a716-567d34fcb51f",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7bf191-aeca-467a-b2a4-3a70f46e9fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# Stack features along the last dimension\n",
    "feature_images = np.stack((hag_images, dem_images, roads_images, rivers_images), axis=-1)\n",
    "\n",
    "# Free up memory by deleting the original arrays\n",
    "# del hag_images\n",
    "# del dem_images\n",
    "# del roads_images\n",
    "# del rivers_images\n",
    "\n",
    "# If you want to ensure that the memory is freed immediately\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "CHIP_SIZE=128\n",
    "\n",
    "# Normalize labels if they range from 0 to 100\n",
    "label_images /= 100\n",
    "\n",
    "# Reshape labels for CNN input\n",
    "label_images = np.expand_dims(label_images, axis=-1)\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential([\n",
    "    #Conv2D(16, (3, 3), activation='relu', input_shape=(128, 128, 4)),\n",
    "    Conv2D(16, (3, 3), activation='relu', input_shape=(CHIP_SIZE, CHIP_SIZE, 4)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    #Dense(128 * 128, activation='sigmoid'),\n",
    "    #tf.keras.layers.Reshape((128, 128, 1))\n",
    "    Dense(CHIP_SIZE * CHIP_SIZE, activation='sigmoid'),\n",
    "    tf.keras.layers.Reshape((CHIP_SIZE, CHIP_SIZE, 1))\n",
    "])\n",
    "\n",
    "# # Define custom weights for each feature\n",
    "# weights = np.array([1.0, 0.8, 0.5, 0.3])  \n",
    "# sample_weights = np.dot(feature_images, weights)\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "# Compile and train the model with sample weights\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "model.fit(feature_images, label_images, batch_size=64, epochs=3, validation_split=0.3)#, sample_weight=sample_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db34ca-78f3-400d-a650-7f2eab7084ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(f'{THEFOLDER}/model_TESTSAVE', save_format='tf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5b0f05-9afd-4f71-9a00-61fd202dd383",
   "metadata": {},
   "source": [
    "# Prepare dataset for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f845b86a-a536-468b-9fe9-70a29b06ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE DATA\n",
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from osgeo import gdal\n",
    "import rioxarray\n",
    "import planetary_computer\n",
    "from pystac_client import Client\n",
    "import osmnx as ox\n",
    "import rasterio\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.windows import from_bounds, Window\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "from shapely.geometry import box\n",
    "from geopandas import GeoDataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.plot import show\n",
    "import rasterio as rio\n",
    "from itertools import product\n",
    "from rasterio import windows\n",
    "\n",
    "TILENUMBER = ['70000-40000']\n",
    "CHIP_SIZE = 128  \n",
    "\n",
    "def delete_non_resampled_files(resampled_files, tif_dir):\n",
    "    for file in os.listdir(tif_dir):\n",
    "        if file not in resampled_files and file.endswith('.tif'):\n",
    "            file_path = os.path.join(tif_dir, file)\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted: {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete {file_path}: {e}\")\n",
    "\n",
    "def process_dem(tif_path, tif_dir, tile_number):\n",
    "    tif_data = rioxarray.open_rasterio(tif_path)\n",
    "    bbox_of_interest = tif_data.rio.bounds()\n",
    "    catalog = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
    "    search = catalog.search(collections=[\"cop-dem-glo-30\"], bbox=bbox_of_interest)\n",
    "    items = list(search.get_items())\n",
    "    \n",
    "    def process_item(item, idx):\n",
    "        signed_asset = planetary_computer.sign(item.assets[\"data\"])\n",
    "        data = rioxarray.open_rasterio(signed_asset.href).squeeze().drop(\"band\")\n",
    "        data.rio.write_crs(\"EPSG:4326\", inplace=True)\n",
    "        output_tif_path = os.path.join(tif_dir, f\"output_dataDEM_{idx}.tif\")\n",
    "        data.rio.to_raster(output_tif_path)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        for i, item in enumerate(items):\n",
    "            executor.submit(process_item, item, i)\n",
    "\n",
    "    output_tif = os.path.join(tif_dir, f\"outputtile_DEM_{tile_number}.tif\")\n",
    "    merge_command = [\n",
    "        \"python\", \"C:\\\\Users\\\\USERNAME\\\\anaconda3\\\\envs\\\\globalpcl\\\\Scripts\\\\gdal_merge.py\",\n",
    "        \"--config\", \"CHECK_DISK_FREE_SPACE\", \"FALSE\",\n",
    "        \"-o\", output_tif,\n",
    "        \"-n\", \"-9999\", \"-a_nodata\", \"-9999\"] + glob.glob(os.path.join(tif_dir, \"output_dataDEM_*.tif\"))\n",
    "\n",
    "    process_hag = subprocess.run(merge_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    if process_hag.returncode != 0:\n",
    "        print(f\"Error in merging DEM: {process_hag.stderr}\")\n",
    "        return None\n",
    "\n",
    "    src_ds = gdal.Open(output_tif, gdal.GA_ReadOnly)\n",
    "    target_ds = gdal.Open(tif_path, gdal.GA_ReadOnly)\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    output_resampled_path = os.path.join(tif_dir, f\"output_resampled_dataDEM_{tile_number}.tif\")\n",
    "    out_ds = driver.Create(output_resampled_path, target_ds.RasterXSize, target_ds.RasterYSize, 1, src_ds.GetRasterBand(1).DataType)\n",
    "    out_ds.SetGeoTransform(target_ds.GetGeoTransform())\n",
    "    out_ds.SetProjection(target_ds.GetProjection())\n",
    "    gdal.ReprojectImage(src_ds, out_ds, src_ds.GetProjection(), target_ds.GetProjection(), gdal.GRA_Bilinear)\n",
    "    src_ds, target_ds, out_ds = None, None, None\n",
    "\n",
    "    os.remove(output_tif)\n",
    "    for tif in glob.glob(os.path.join(tif_dir, \"output_dataDEM_*.tif\")):\n",
    "        try:\n",
    "            os.remove(tif)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {tif}: {e}\")\n",
    "\n",
    "    return output_resampled_path\n",
    "    del tif_data, bbox_of_interest, catalog, search, items\n",
    "    del output_tif, merge_command, process_hag\n",
    "    del src_ds, target_ds, driver, output_resampled_path\n",
    "\n",
    "def process_lidar(tif_path, tif_dir, tile_number):\n",
    "    lidar_dir = r\"C:\\Users\\USERNAME\\OneDrive\\Desktop\\GlobalData\\LIDAR2\"\n",
    "    lidar_tifs = glob.glob(os.path.join(lidar_dir, \"*.tif\"))\n",
    "\n",
    "    # Get the bounding box of the input tif_path\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        bbox = src.bounds\n",
    "        input_geom = box(bbox.left, bbox.bottom, bbox.right, bbox.top)\n",
    "\n",
    "    # Find overlapping LIDAR tiles\n",
    "    overlapping_tifs = []\n",
    "    for tif in lidar_tifs:\n",
    "        with rasterio.open(tif) as src:\n",
    "            lidar_bbox = src.bounds\n",
    "            lidar_geom = box(lidar_bbox.left, lidar_bbox.bottom, lidar_bbox.right, lidar_bbox.top)\n",
    "            if input_geom.intersects(lidar_geom):\n",
    "                overlapping_tifs.append(tif)\n",
    "\n",
    "    if not overlapping_tifs:\n",
    "        print(f\"No overlapping LIDAR tiles found for {tile_number}\")\n",
    "        return None\n",
    "\n",
    "    output_tif = os.path.join(tif_dir, f\"outputtile_lidar_{tile_number}.tif\")\n",
    "    merge_command = [\n",
    "        \"python\", \"C:\\\\Users\\\\USERNAME\\\\anaconda3\\\\envs\\\\globalpcl\\\\Scripts\\\\gdal_merge.py\",\n",
    "        \"--config\", \"CHECK_DISK_FREE_SPACE\", \"FALSE\",\n",
    "        \"-o\", output_tif,\n",
    "        \"-n\", \"255\", \"-a_nodata\", \"255\"] + overlapping_tifs\n",
    "\n",
    "    process_hag = subprocess.run(merge_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    if process_hag.returncode != 0:\n",
    "        print(f\"Error in merging LIDAR: {process_hag.stderr}\")\n",
    "        return None\n",
    "\n",
    "    src_ds = gdal.Open(output_tif, gdal.GA_ReadOnly)\n",
    "    target_ds = gdal.Open(tif_path, gdal.GA_ReadOnly)\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    output_resampled_path = os.path.join(tif_dir, f\"output_resampled_dataLIDAR_{tile_number}.tif\")\n",
    "    out_ds = driver.Create(output_resampled_path, target_ds.RasterXSize, target_ds.RasterYSize, 1, src_ds.GetRasterBand(1).DataType)\n",
    "    out_ds.SetGeoTransform(target_ds.GetGeoTransform())\n",
    "    out_ds.SetProjection(target_ds.GetProjection())\n",
    "    gdal.ReprojectImage(src_ds, out_ds, src_ds.GetProjection(), target_ds.GetProjection(), gdal.GRA_Bilinear)\n",
    "    src_ds, target_ds, out_ds = None, None, None\n",
    "\n",
    "    os.remove(output_tif)\n",
    "    return output_resampled_path\n",
    "    del lidar_tifs, bbox, input_geom, overlapping_tifs, lidar_bbox, lidar_geom\n",
    "    del output_tif, merge_command, process_hag\n",
    "    del src_ds, target_ds, driver, output_resampled_path\n",
    "\n",
    "def process_rivers(tif_path, tif_dir, tile_number):\n",
    "    dem_data = rioxarray.open_rasterio(tif_path)\n",
    "    bbox = dem_data.rio.bounds()\n",
    "    custom_filter = '[\"waterway\"~\"river\"]'\n",
    "    graph = ox.graph_from_bbox(bbox[3], bbox[1], bbox[2], bbox[0], custom_filter=custom_filter, simplify=True, retain_all=True, truncate_by_edge=True)\n",
    "    gdf = ox.graph_to_gdfs(graph, nodes=False)\n",
    "\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        window = from_bounds(*src.bounds, src.transform)\n",
    "        transform = rasterio.windows.transform(window, src.transform)\n",
    "        raster = np.zeros((int(window.height), int(window.width)), dtype=np.uint8)\n",
    "        shapes = ((geom, 1) for geom in gdf['geometry'])\n",
    "        burned = rasterize(shapes, out=raster, fill=0, transform=transform, all_touched=True)\n",
    "        distance_grid = scipy.ndimage.distance_transform_edt(burned == 0)\n",
    "        decay_grid = np.exp(-0.07 * distance_grid)\n",
    "\n",
    "        clipped_meta = src.meta.copy()\n",
    "        clipped_meta.update({\"driver\": \"GTiff\", \"height\": int(window.height), \"width\": int(window.width), \"transform\": transform, \"dtype\": rasterio.float32, \"count\": 1, \"compress\": 'lzw'})\n",
    "        output_path = os.path.join(tif_dir, f'exponential_decay_CO_river_{tile_number}.tif')\n",
    "        with rasterio.open(output_path, 'w', **clipped_meta) as dst:\n",
    "            dst.write(decay_grid.astype(np.float32), 1)\n",
    "\n",
    "    src_ds = gdal.Open(output_path, gdal.GA_ReadOnly)\n",
    "    target_ds = gdal.Open(tif_path, gdal.GA_ReadOnly)\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    output_resampled_path = os.path.join(tif_dir, f\"output_resampled_dataRivers_{tile_number}.tif\")\n",
    "    out_ds = driver.Create(output_resampled_path, target_ds.RasterXSize, target_ds.RasterYSize, 1, src_ds.GetRasterBand(1).DataType)\n",
    "    out_ds.SetGeoTransform(target_ds.GetGeoTransform())\n",
    "    out_ds.SetProjection(target_ds.GetProjection())\n",
    "    gdal.ReprojectImage(src_ds, out_ds, src_ds.GetProjection(), target_ds.GetProjection(), gdal.GRA_Bilinear)\n",
    "    src_ds, target_ds, out_ds = None, None, None\n",
    "    os.remove(output_path)\n",
    "\n",
    "    return output_resampled_path\n",
    "    del dem_data, bbox, custom_filter, graph, gdf\n",
    "    del window, transform, raster, shapes, burned, distance_grid, decay_grid\n",
    "    del clipped_meta, output_path\n",
    "    del src_ds, target_ds, driver, output_resampled_path\n",
    "\n",
    "def process_roads(tif_path, tif_dir, tile_number):\n",
    "    extent_data = rioxarray.open_rasterio(tif_path)\n",
    "    bbox = extent_data.rio.bounds()\n",
    "    graph = ox.graph_from_bbox(bbox[3], bbox[1], bbox[2], bbox[0], network_type='drive', simplify=True)\n",
    "    gdf = ox.graph_to_gdfs(graph, nodes=False)\n",
    "\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        window = from_bounds(*src.bounds, src.transform)\n",
    "        transform = rasterio.windows.transform(window, src.transform)\n",
    "        raster = np.zeros((int(window.height), int(window.width)), dtype=np.uint8)\n",
    "        shapes = ((geom, 1) for geom in gdf['geometry'])\n",
    "        burned = rasterize(shapes, out=raster, fill=0, transform=transform, all_touched=True)\n",
    "        distance_grid = scipy.ndimage.distance_transform_edt(burned == 0)\n",
    "        decay_grid = np.exp(-0.07 * distance_grid)\n",
    "\n",
    "        clipped_meta = src.meta.copy()\n",
    "        clipped_meta.update({\"driver\": \"GTiff\", \"height\": int(window.height), \"width\": int(window.width), \"transform\": transform, \"dtype\": rasterio.float32, \"count\": 1, \"compress\": 'lzw'})\n",
    "        output_path = os.path.join(tif_dir, f'exponential_decay_CO_roads_{tile_number}.tif')\n",
    "        with rasterio.open(output_path, 'w', **clipped_meta) as dst:\n",
    "            dst.write(decay_grid.astype(np.float32), 1)\n",
    "\n",
    "    src_ds = gdal.Open(output_path, gdal.GA_ReadOnly)\n",
    "    target_ds = gdal.Open(tif_path, gdal.GA_ReadOnly)\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    output_resampled_path = os.path.join(tif_dir, f\"output_resampled_dataRoads_{tile_number}.tif\")\n",
    "    out_ds = driver.Create(output_resampled_path, target_ds.RasterXSize, target_ds.RasterYSize, 1, src_ds.GetRasterBand(1).DataType)\n",
    "    out_ds.SetGeoTransform(target_ds.GetGeoTransform())\n",
    "    out_ds.SetProjection(target_ds.GetProjection())\n",
    "    gdal.ReprojectImage(src_ds, out_ds, src_ds.GetProjection(), target_ds.GetProjection(), gdal.GRA_Bilinear)\n",
    "    src_ds, target_ds, out_ds = None, None, None\n",
    "    os.remove(output_path)\n",
    "\n",
    "    return output_resampled_path\n",
    "    del extent_data, bbox, graph, gdf\n",
    "    del window, transform, raster, shapes, burned, distance_grid, decay_grid\n",
    "    del clipped_meta, output_path\n",
    "    del src_ds, target_ds, driver, output_resampled_path\n",
    "\n",
    "# OFFSET_X = 20  # Horizontal offset\n",
    "# OFFSET_Y = 20  # Vertical offset\n",
    "\n",
    "def get_tiles(ds, width=CHIP_SIZE, height=CHIP_SIZE):\n",
    "    offset_x = 128  # Horizontal offset\n",
    "    offset_y = 128 # Vertical offset\n",
    "\n",
    "    \n",
    "    nols, nrows = ds.meta['width'], ds.meta['height']\n",
    "    #offsets = product(range(0, nols, width), range(0, nrows, height))\n",
    "    offsets = product(range(0, nols, offset_x), range(0, nrows, offset_y))\n",
    "\n",
    "    big_window = windows.Window(col_off=0, row_off=0, width=nols, height=nrows)\n",
    "    for col_off, row_off in offsets:\n",
    "        window = windows.Window(col_off=col_off, row_off=row_off, width=width, height=height).intersection(big_window)\n",
    "        transform = windows.transform(window, ds.transform)\n",
    "        yield window, transform\n",
    "\n",
    "def process_file(label, input_filepath, output_folder):\n",
    "    #OFFSET_X = 20\n",
    "    #OFFSET_Y = 20\n",
    "    with rio.open(input_filepath) as inds:\n",
    "        nodata = inds.nodata  # Get the NoData value from the dataset\n",
    "        meta = inds.meta.copy()\n",
    "        \n",
    "        for window, transform in get_tiles(inds):\n",
    "            if window.width == CHIP_SIZE and window.height == CHIP_SIZE:  # Check if the tile dimensions are as expected\n",
    "                data = inds.read(window=window)\n",
    "                if nodata is not None:\n",
    "                    valid_data_mask = (data != nodata)\n",
    "                else:\n",
    "                    valid_data_mask = (data == data)\n",
    "                \n",
    "                if valid_data_mask.any():  # Check if there's any valid data within the tile\n",
    "                    meta['transform'] = transform\n",
    "                    meta['width'], meta['height'] = window.width, window.height\n",
    "                    outpath = os.path.join(output_folder, output_filename.format(int(window.col_off), int(window.row_off)))\n",
    "                    with rio.open(outpath, 'w', **meta) as outds:\n",
    "                        outds.write(data)\n",
    "    print(f\"Processing for {label} completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for tile_number in TILENUMBER:\n",
    "        tif_path = f\"{THEFOLDER}\\\\PCLTILES\\\\pcltile_{tile_number}.tif\"\n",
    "        tif_dir = f\"{THEFOLDER}\\\\INFERENCETILES\\\\{tile_number}\"\n",
    "        os.makedirs(tif_dir, exist_ok=True)\n",
    "\n",
    "        resampled_files = [\n",
    "            process_dem(tif_path, tif_dir, tile_number),\n",
    "            process_lidar(tif_path, tif_dir, tile_number),\n",
    "            process_rivers(tif_path, tif_dir, tile_number),\n",
    "            process_roads(tif_path, tif_dir, tile_number)\n",
    "        ]\n",
    "\n",
    "        delete_non_resampled_files([os.path.basename(f) for f in resampled_files], tif_dir)\n",
    "\n",
    "        # Define input files as a dictionary\n",
    "        input_files = {\n",
    "            'lidar': f'output_resampled_dataLIDAR_{tile_number}.tif',\n",
    "            'dem': f'output_resampled_dataDEM_{tile_number}.tif',\n",
    "            'roads': f'output_resampled_dataRoads_{tile_number}.tif',\n",
    "            'rivers': f'output_resampled_dataRivers_{tile_number}.tif'\n",
    "        }\n",
    "        output_filename = 'tile_{}-{}.tif'\n",
    "\n",
    "        # Define the base output path\n",
    "        out_base_path = f\"{THEFOLDER}\\\\INFERENCETILES\"\n",
    "        os.makedirs(out_base_path, exist_ok=True)\n",
    "\n",
    "        # Process each file\n",
    "        for label, filename in input_files.items():\n",
    "            input_filepath = os.path.join(tif_dir, filename)\n",
    "            output_folder = os.path.join(out_base_path, label)\n",
    "            os.makedirs(output_folder, exist_ok=True)\n",
    "            process_file(label, input_filepath, output_folder)\n",
    "\n",
    "        print(f\"Processing for tile {tile_number} completed.\")\n",
    "\n",
    "    print(\"All processing completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c90e7f-166d-4a63-b863-30971350e8e4",
   "metadata": {},
   "source": [
    "# Create identifier_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59222e83-486c-4737-8ba9-b7b29389642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Define the directory path\n",
    "#directory_path = 'C:\\\\Users\\\\USERNAME\\\\OneDrive\\\\Desktop\\\\PCLCONUS\\\\Input\\\\inferencetiles\\\\hag'\n",
    "\n",
    "# Regular expression to extract the identifier part of the filename 'tile_{identifier}.tif'\n",
    "pattern = re.compile(r'tile_(\\d+-\\d+)\\.tif')\n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(output_folder)\n",
    "\n",
    "# Use a set to avoid duplicate identifiers\n",
    "identifiers = set()\n",
    "\n",
    "# Extract identifiers from filenames\n",
    "for file in files:\n",
    "    match = pattern.search(file)\n",
    "    if match:\n",
    "        identifiers.add(match.group(1))\n",
    "\n",
    "# Convert the set to a sorted list\n",
    "identifier_list = sorted(list(identifiers))\n",
    "print(len(identifier_list))\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c23927-e451-403f-a88f-bcf0c6e8e238",
   "metadata": {},
   "source": [
    "# Predict each tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab40c88-caee-409d-8627-8570c67f2abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "\n",
    "import os\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# tilename = '0-0'\n",
    "# input_hag_path = f\"C:\\\\Users\\\\USERNAME\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\hag\\\\tile_{tilename}.tif\"\n",
    "# input_dem_path = f\"C:\\\\Users\\\\USERNAME\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\dem\\\\tile_{tilename}.tif\"\n",
    "# input_roads_path = f\"C:\\\\Users\\\\USERNAME\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\roads\\\\tile_{tilename}.tif\"\n",
    "# input_rivers_path = f\"C:\\\\Users\\\\USERNAME\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\rivers\\\\tile_{tilename}.tif\"\n",
    "\n",
    "def load_and_preprocess_image(hag_path, dem_path, roads_path, rivers_path):\n",
    "    with rasterio.open(hag_path) as src:\n",
    "        hag_image = src.read(1)\n",
    "    with rasterio.open(dem_path) as src:\n",
    "        dem_image = src.read(1)\n",
    "    with rasterio.open(roads_path) as src:\n",
    "        roads_image = src.read(1)\n",
    "    with rasterio.open(rivers_path) as src:\n",
    "        rivers_image = src.read(1)\n",
    "\n",
    "    # Normalize and stack the images\n",
    "    hag_image = np.array(hag_image).astype('float32') / hag_max\n",
    "    dem_image = np.array(dem_image).astype('float32') / dem_max\n",
    "    roads_image = np.array(roads_image).astype('float32') / roads_max\n",
    "    rivers_image = np.array(rivers_image).astype('float32') / rivers_max\n",
    "\n",
    "    # Stack images along the last dimension\n",
    "    combined_image = np.stack([hag_image, dem_image, roads_image, rivers_image], axis=-1)\n",
    "\n",
    "    # Add batch dimension\n",
    "    combined_image = np.expand_dims(combined_image, axis=0)\n",
    "    return combined_image\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(identifier_list)):\n",
    "    tilename = identifier_list[i]\n",
    "    #print(tilename)\n",
    "    #input_hag_path = f\"{out_base_path}\\\\lidar\\\\tile_{tilename}.tif\"\n",
    "    #input_dem_path = f\"{out_base_path}\\\\dem\\\\tile_{tilename}.tif\"\n",
    "    #input_roads_path = f\"{out_base_path}\\\\roads\\\\tile_{tilename}.tif\"\n",
    "    #input_rivers_path = f\"{out_base_path}\\\\tile_{tilename}.tif\"\n",
    "\n",
    "    input_hag_path = os.path.join(out_base_path, \"lidar\", f\"tile_{tilename}.tif\")\n",
    "    input_dem_path = os.path.join(out_base_path, \"dem\", f\"tile_{tilename}.tif\")\n",
    "    input_roads_path = os.path.join(out_base_path, \"roads\", f\"tile_{tilename}.tif\")\n",
    "    input_rivers_path = os.path.join(out_base_path, \"rivers\", f\"tile_{tilename}.tif\")\n",
    "\n",
    "\n",
    "    input_image = load_and_preprocess_image(input_hag_path, input_dem_path, input_roads_path, input_rivers_path)\n",
    "    predicted_image = model.predict(input_image)\n",
    "    predicted_image = np.squeeze(predicted_image)\n",
    "    \n",
    "    # Debug print to check if all outputs are the same\n",
    "    #print(\"Unique values in predicted output:\", np.unique(predicted_image))\n",
    "    \n",
    "    # Adjust the scaling factor based on how the labels were scaled during training\n",
    "    predicted_image *= 100\n",
    "    \n",
    "    #output_image_path = f\"{THEFOLDER}\\\\predictions\\\\predicted_tile_{tilename}.tif\"\n",
    "    predictions_folder = os.path.join(THEFOLDER, \"predictions\")\n",
    "    os.makedirs(predictions_folder, exist_ok=True)\n",
    "    output_image_path = os.path.join(predictions_folder, f\"predicted_tile_{tilename}.tif\")\n",
    "\n",
    "    \n",
    "    with rasterio.open(input_dem_path) as src: \n",
    "        profile = src.profile\n",
    "    \n",
    "    with rasterio.open(output_image_path, 'w', **profile) as dst:\n",
    "        dst.write(predicted_image.astype(rasterio.uint8), 1)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0625d6be-3373-4dcc-919c-9887040112d9",
   "metadata": {},
   "source": [
    "# Merge predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a445b3-a8d1-402c-87a3-13e23fa05676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "\n",
    "TILENUMBER = '70000-40000'\n",
    "\n",
    "# Define the base folder and output paths\n",
    "#THEFOLDER = r\"C:\\Users\\USERNAME\\OneDrive\\Desktop\\GlobalPCL\"\n",
    "predictions_folder = os.path.join(THEFOLDER, \"predictions\")\n",
    "output_dir = os.path.join(THEFOLDER, \"mergedoutput\")\n",
    "\n",
    "# Create necessary directories if they don't exist\n",
    "os.makedirs(predictions_folder, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_base_name = \"predMerged_\"  # Base name for output files\n",
    "\n",
    "# Get a list of TIFF files\n",
    "tifs = glob.glob(os.path.join(predictions_folder, \"*.tif\"))\n",
    "\n",
    "# Define chunk size for processing\n",
    "chunk_size = 300\n",
    "\n",
    "# Calculate the number of chunks needed\n",
    "num_chunks = len(tifs) // chunk_size\n",
    "if len(tifs) % chunk_size != 0:\n",
    "    num_chunks += 1  # Add one more chunk for the remaining files\n",
    "\n",
    "# Loop through the TIFF files in chunks\n",
    "for chunk_id in range(num_chunks):\n",
    "    start_idx = chunk_id * chunk_size\n",
    "    end_idx = min((chunk_id + 1) * chunk_size, len(tifs))\n",
    "    chunk_tifs = tifs[start_idx:end_idx]\n",
    "    \n",
    "    output_tif = os.path.join(output_dir, f\"{output_base_name}{chunk_id + 1}.tif\")\n",
    "\n",
    "    merge_command_hag = [\n",
    "        \"python\",\n",
    "        \"C:\\\\Users\\\\USERNAME\\\\anaconda3\\\\envs\\\\globalpcl\\\\Scripts\\\\gdal_merge.py\",\n",
    "        \"--config\", \"CHECK_DISK_FREE_SPACE\", \"FALSE\",\n",
    "        \"-o\", output_tif,\n",
    "        \"-n\", \"-9999\",\n",
    "        \"-a_nodata\", \"-9999\",\n",
    "    ] + chunk_tifs\n",
    "\n",
    "    # Run the gdal_merge command for the current chunk\n",
    "    process_hag = subprocess.run(merge_command_hag, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    # Check if the command for the current chunk was successful\n",
    "    if process_hag.returncode != 0:\n",
    "        # An error occurred, print the error\n",
    "        print(f\"Error occurred while merging TIFF files for chunk {chunk_id + 1}:\")\n",
    "        print(process_hag.stderr)\n",
    "    else:\n",
    "        print(f\"TIFF files merged successfully for chunk {chunk_id + 1}. Output: {output_tif}\")\n",
    "\n",
    "# Merge all chunks into a final output file\n",
    "final_output_tif = os.path.join(THEFOLDER, \"FINALOUTPUTTILES\", f\"predMerged_PCL_{TILENUMBER}.tif\")\n",
    "os.makedirs(os.path.dirname(final_output_tif), exist_ok=True)\n",
    "\n",
    "chunk_tifs = glob.glob(os.path.join(output_dir, \"*.tif\"))\n",
    "\n",
    "merge_command_final = [\n",
    "    \"python\",\n",
    "    \"C:\\\\Users\\\\USERNAME\\\\anaconda3\\\\envs\\\\globalpcl\\\\Scripts\\\\gdal_merge.py\",\n",
    "    \"--config\", \"CHECK_DISK_FREE_SPACE\", \"FALSE\",\n",
    "    \"-o\", final_output_tif,\n",
    "    \"-n\", \"-9999\",\n",
    "    \"-a_nodata\", \"-9999\",\n",
    "] + chunk_tifs\n",
    "\n",
    "# Run the gdal_merge command for the final merge\n",
    "process_final = subprocess.run(merge_command_final, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "# Check if the command for the final merge was successful\n",
    "if process_final.returncode != 0:\n",
    "    # An error occurred, print the error\n",
    "    print(\"Error occurred while merging final TIFF files:\")\n",
    "    print(process_final.stderr)\n",
    "else:\n",
    "    print(\"Final TIFF files merged successfully.\")\n",
    "\n",
    "# Clean up temporary chunk files\n",
    "for tif in chunk_tifs:\n",
    "    try:\n",
    "        os.remove(tif)\n",
    "        print(f\"Deleted {tif}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to delete {tif}: {e}\")\n",
    "\n",
    "print(\"Done\")\n",
    "\n",
    "import shutil\n",
    "\n",
    "if os.path.exists(predictions_folder):\n",
    "    shutil.rmtree(predictions_folder)\n",
    "os.makedirs(predictions_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5426bf-6c81-46f5-953c-07fbbb1d3c93",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b478e5e-eb95-41f5-a218-8d803468ee68",
   "metadata": {},
   "source": [
    "# This is in development for this to work, you would need to download the corrisponding lidar dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7547dd9-e9d8-4e7a-a9b3-1d6b22779871",
   "metadata": {},
   "source": [
    "# Global Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e676f8-195d-4c84-9bf8-06dce1f9cc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.warp import transform\n",
    "import numpy as np\n",
    "import rioxarray\n",
    "from pystac_client import Client\n",
    "import planetary_computer\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "from osgeo import gdal\n",
    "\n",
    "def create_tile(center_lat, center_lon, reference_raster_path, output_tile_path):\n",
    "    # Open the reference raster to get its properties\n",
    "    with rasterio.open(reference_raster_path) as ref_raster:\n",
    "        ref_transform = ref_raster.transform\n",
    "        ref_crs = ref_raster.crs\n",
    "        ref_res = ref_transform[0]\n",
    "        ref_width = ref_raster.width\n",
    "        ref_height = ref_raster.height\n",
    "        new_profile = ref_raster.profile.copy()\n",
    "    \n",
    "    # Convert lat/lon to the same coordinate system as the reference raster\n",
    "    dst_crs = ref_crs\n",
    "    src_crs = 'EPSG:4326'\n",
    "    center_x, center_y = transform(src_crs, dst_crs, [center_lon], [center_lat])\n",
    "    center_x, center_y = center_x[0], center_y[0]\n",
    "    \n",
    "    # Define the new transform for the 5000x5000 tile\n",
    "    tile_width = 5000\n",
    "    tile_height = 5000\n",
    "    tile_transform = from_origin(\n",
    "        center_x - (tile_width // 2) * ref_res,\n",
    "        center_y + (tile_height // 2) * ref_res,\n",
    "        ref_res,\n",
    "        ref_res\n",
    "    )\n",
    "    \n",
    "    # Create the new raster data (a tile filled with ones)\n",
    "    tile_data = np.ones((tile_height, tile_width), dtype=np.uint8)\n",
    "    \n",
    "    # Define the new raster profile\n",
    "    new_profile.update({\n",
    "        'height': tile_height,\n",
    "        'width': tile_width,\n",
    "        'transform': tile_transform,\n",
    "        'dtype': 'uint8'\n",
    "    })\n",
    "    \n",
    "    # Write the new tile to a file\n",
    "    with rasterio.open(output_tile_path, 'w', **new_profile) as dst_raster:\n",
    "        dst_raster.write(tile_data, 1)\n",
    "\n",
    "# Define the center latitude and longitude\n",
    "#center_lat = 40.7128  # Example: New York City latitude\n",
    "#center_lon = -74.0060  # Example: New York City longitude\n",
    "\n",
    "# Define the center latitude and longitude\n",
    "#center_lat = 51.5074  # Example: London\n",
    "#center_lon = -0.1278  # Example: UK\n",
    "\n",
    "center_lat = 35.715735\n",
    "center_lon = -82.314088\n",
    "\n",
    "\n",
    "\n",
    "# out_path = f\"{THEFOLDER}\\\\PCLTILES\\\\\"\n",
    "# output_filename = 'pcltile_75000-40000.tif'\n",
    "\n",
    "# # Define the reference raster path and output tile path\n",
    "# #reference_raster_path = r\"C:\\Users\\USERNAME\\OneDrive\\Desktop\\PCLCONUS\\Input\\PCLTILES\\pcltile_75000-40000.tif\"\n",
    "# reference_raster_path = os.path.join(out_path, \"output_filename\")\n",
    "\n",
    "# output_tile_path = f\"{THEFOLDER}\\pcltile_London-UK.tif\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the folder and file paths\n",
    "#THEFOLDER = \"C:/Users/USERNAME/OneDrive/Desktop/GlobalPCL\"\n",
    "out_path = os.path.join(THEFOLDER, \"PCLTILES\")\n",
    "output_filename = 'pcltile_75000-40000.tif'\n",
    "reference_raster_path = os.path.join(out_path, output_filename)\n",
    "output_tile_path = os.path.join(THEFOLDER, 'pcltile_ak.tif')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the tile\n",
    "create_tile(center_lat, center_lon, reference_raster_path, output_tile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745b37eb-978f-4916-b186-f5601d8686b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE DATA\n",
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from osgeo import gdal\n",
    "import rioxarray\n",
    "import planetary_computer\n",
    "from pystac_client import Client\n",
    "import osmnx as ox\n",
    "import rasterio\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.windows import from_bounds, Window\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "from shapely.geometry import box\n",
    "from geopandas import GeoDataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.plot import show\n",
    "import rasterio as rio\n",
    "from itertools import product\n",
    "from rasterio import windows\n",
    "\n",
    "TILENUMBER = ['ak']\n",
    "\n",
    "CHIP_SIZE = 128  \n",
    "\n",
    "def delete_non_resampled_files(resampled_files, tif_dir):\n",
    "    for file in os.listdir(tif_dir):\n",
    "        if file not in resampled_files and file.endswith('.tif'):\n",
    "            file_path = os.path.join(tif_dir, file)\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted: {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete {file_path}: {e}\")\n",
    "\n",
    "def process_dem(tif_path, tif_dir, tile_number):\n",
    "    tif_data = rioxarray.open_rasterio(tif_path)\n",
    "    bbox_of_interest = tif_data.rio.bounds()\n",
    "    catalog = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
    "    search = catalog.search(collections=[\"cop-dem-glo-30\"], bbox=bbox_of_interest)\n",
    "    items = list(search.get_items())\n",
    "    \n",
    "    def process_item(item, idx):\n",
    "        signed_asset = planetary_computer.sign(item.assets[\"data\"])\n",
    "        data = rioxarray.open_rasterio(signed_asset.href).squeeze().drop(\"band\")\n",
    "        data.rio.write_crs(\"EPSG:4326\", inplace=True)\n",
    "        output_tif_path = os.path.join(tif_dir, f\"output_dataDEM_{idx}.tif\")\n",
    "        data.rio.to_raster(output_tif_path)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        for i, item in enumerate(items):\n",
    "            executor.submit(process_item, item, i)\n",
    "\n",
    "    output_tif = os.path.join(tif_dir, f\"outputtile_DEM_{tile_number}.tif\")\n",
    "    merge_command = [\n",
    "        \"python\", \"C:\\\\Users\\\\USERNAME\\\\anaconda3\\\\envs\\\\globalpcl\\\\Scripts\\\\gdal_merge.py\",\n",
    "        \"--config\", \"CHECK_DISK_FREE_SPACE\", \"FALSE\",\n",
    "        \"-o\", output_tif,\n",
    "        \"-n\", \"-9999\", \"-a_nodata\", \"-9999\"] + glob.glob(os.path.join(tif_dir, \"output_dataDEM_*.tif\"))\n",
    "\n",
    "    process_hag = subprocess.run(merge_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    if process_hag.returncode != 0:\n",
    "        print(f\"Error in merging DEM: {process_hag.stderr}\")\n",
    "        return None\n",
    "\n",
    "    src_ds = gdal.Open(output_tif, gdal.GA_ReadOnly)\n",
    "    target_ds = gdal.Open(tif_path, gdal.GA_ReadOnly)\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    output_resampled_path = os.path.join(tif_dir, f\"output_resampled_dataDEM_{tile_number}.tif\")\n",
    "    out_ds = driver.Create(output_resampled_path, target_ds.RasterXSize, target_ds.RasterYSize, 1, src_ds.GetRasterBand(1).DataType)\n",
    "    out_ds.SetGeoTransform(target_ds.GetGeoTransform())\n",
    "    out_ds.SetProjection(target_ds.GetProjection())\n",
    "    gdal.ReprojectImage(src_ds, out_ds, src_ds.GetProjection(), target_ds.GetProjection(), gdal.GRA_Bilinear)\n",
    "    src_ds, target_ds, out_ds = None, None, None\n",
    "\n",
    "    os.remove(output_tif)\n",
    "    for tif in glob.glob(os.path.join(tif_dir, \"output_dataDEM_*.tif\")):\n",
    "        try:\n",
    "            os.remove(tif)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {tif}: {e}\")\n",
    "\n",
    "    return output_resampled_path\n",
    "    del tif_data, bbox_of_interest, catalog, search, items\n",
    "    del output_tif, merge_command, process_hag\n",
    "    del src_ds, target_ds, driver, output_resampled_path\n",
    "\n",
    "def process_lidar(tif_path, tif_dir, tile_number):\n",
    "    lidar_dir = r\"C:\\Users\\USERNAME\\OneDrive\\Desktop\\GlobalData\\LIDAR2\"\n",
    "    lidar_tifs = glob.glob(os.path.join(lidar_dir, \"*.tif\"))\n",
    "\n",
    "    # Get the bounding box of the input tif_path\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        bbox = src.bounds\n",
    "        input_geom = box(bbox.left, bbox.bottom, bbox.right, bbox.top)\n",
    "\n",
    "    # Find overlapping LIDAR tiles\n",
    "    overlapping_tifs = []\n",
    "    for tif in lidar_tifs:\n",
    "        with rasterio.open(tif) as src:\n",
    "            lidar_bbox = src.bounds\n",
    "            lidar_geom = box(lidar_bbox.left, lidar_bbox.bottom, lidar_bbox.right, lidar_bbox.top)\n",
    "            if input_geom.intersects(lidar_geom):\n",
    "                overlapping_tifs.append(tif)\n",
    "\n",
    "    if not overlapping_tifs:\n",
    "        print(f\"No overlapping LIDAR tiles found for {tile_number}\")\n",
    "        return None\n",
    "\n",
    "    output_tif = os.path.join(tif_dir, f\"outputtile_lidar_{tile_number}.tif\")\n",
    "    merge_command = [\n",
    "        \"python\", \"C:\\\\Users\\\\USERNAME\\\\anaconda3\\\\envs\\\\globalpcl\\\\Scripts\\\\gdal_merge.py\",\n",
    "        \"--config\", \"CHECK_DISK_FREE_SPACE\", \"FALSE\",\n",
    "        \"-o\", output_tif,\n",
    "        \"-n\", \"255\", \"-a_nodata\", \"255\"] + overlapping_tifs\n",
    "\n",
    "    process_hag = subprocess.run(merge_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    if process_hag.returncode != 0:\n",
    "        print(f\"Error in merging LIDAR: {process_hag.stderr}\")\n",
    "        return None\n",
    "\n",
    "    src_ds = gdal.Open(output_tif, gdal.GA_ReadOnly)\n",
    "    target_ds = gdal.Open(tif_path, gdal.GA_ReadOnly)\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    output_resampled_path = os.path.join(tif_dir, f\"output_resampled_dataLIDAR_{tile_number}.tif\")\n",
    "    out_ds = driver.Create(output_resampled_path, target_ds.RasterXSize, target_ds.RasterYSize, 1, src_ds.GetRasterBand(1).DataType)\n",
    "    out_ds.SetGeoTransform(target_ds.GetGeoTransform())\n",
    "    out_ds.SetProjection(target_ds.GetProjection())\n",
    "    gdal.ReprojectImage(src_ds, out_ds, src_ds.GetProjection(), target_ds.GetProjection(), gdal.GRA_Bilinear)\n",
    "    src_ds, target_ds, out_ds = None, None, None\n",
    "\n",
    "    os.remove(output_tif)\n",
    "    return output_resampled_path\n",
    "    del lidar_tifs, bbox, input_geom, overlapping_tifs, lidar_bbox, lidar_geom\n",
    "    del output_tif, merge_command, process_hag\n",
    "    del src_ds, target_ds, driver, output_resampled_path\n",
    "\n",
    "def process_rivers(tif_path, tif_dir, tile_number):\n",
    "    dem_data = rioxarray.open_rasterio(tif_path)\n",
    "    bbox = dem_data.rio.bounds()\n",
    "    custom_filter = '[\"waterway\"~\"river\"]'\n",
    "    graph = ox.graph_from_bbox(bbox[3], bbox[1], bbox[2], bbox[0], custom_filter=custom_filter, simplify=True, retain_all=True, truncate_by_edge=True)\n",
    "    gdf = ox.graph_to_gdfs(graph, nodes=False)\n",
    "\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        window = from_bounds(*src.bounds, src.transform)\n",
    "        transform = rasterio.windows.transform(window, src.transform)\n",
    "        raster = np.zeros((int(window.height), int(window.width)), dtype=np.uint8)\n",
    "        shapes = ((geom, 1) for geom in gdf['geometry'])\n",
    "        burned = rasterize(shapes, out=raster, fill=0, transform=transform, all_touched=True)\n",
    "        distance_grid = scipy.ndimage.distance_transform_edt(burned == 0)\n",
    "        decay_grid = np.exp(-0.07 * distance_grid)\n",
    "\n",
    "        clipped_meta = src.meta.copy()\n",
    "        clipped_meta.update({\"driver\": \"GTiff\", \"height\": int(window.height), \"width\": int(window.width), \"transform\": transform, \"dtype\": rasterio.float32, \"count\": 1, \"compress\": 'lzw'})\n",
    "        output_path = os.path.join(tif_dir, f'exponential_decay_CO_river_{tile_number}.tif')\n",
    "        with rasterio.open(output_path, 'w', **clipped_meta) as dst:\n",
    "            dst.write(decay_grid.astype(np.float32), 1)\n",
    "\n",
    "    src_ds = gdal.Open(output_path, gdal.GA_ReadOnly)\n",
    "    target_ds = gdal.Open(tif_path, gdal.GA_ReadOnly)\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    output_resampled_path = os.path.join(tif_dir, f\"output_resampled_dataRivers_{tile_number}.tif\")\n",
    "    out_ds = driver.Create(output_resampled_path, target_ds.RasterXSize, target_ds.RasterYSize, 1, src_ds.GetRasterBand(1).DataType)\n",
    "    out_ds.SetGeoTransform(target_ds.GetGeoTransform())\n",
    "    out_ds.SetProjection(target_ds.GetProjection())\n",
    "    gdal.ReprojectImage(src_ds, out_ds, src_ds.GetProjection(), target_ds.GetProjection(), gdal.GRA_Bilinear)\n",
    "    src_ds, target_ds, out_ds = None, None, None\n",
    "    os.remove(output_path)\n",
    "\n",
    "    return output_resampled_path\n",
    "    del dem_data, bbox, custom_filter, graph, gdf\n",
    "    del window, transform, raster, shapes, burned, distance_grid, decay_grid\n",
    "    del clipped_meta, output_path\n",
    "    del src_ds, target_ds, driver, output_resampled_path\n",
    "\n",
    "def process_roads(tif_path, tif_dir, tile_number):\n",
    "    extent_data = rioxarray.open_rasterio(tif_path)\n",
    "    bbox = extent_data.rio.bounds()\n",
    "    graph = ox.graph_from_bbox(bbox[3], bbox[1], bbox[2], bbox[0], network_type='drive', simplify=True)\n",
    "    gdf = ox.graph_to_gdfs(graph, nodes=False)\n",
    "\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        window = from_bounds(*src.bounds, src.transform)\n",
    "        transform = rasterio.windows.transform(window, src.transform)\n",
    "        raster = np.zeros((int(window.height), int(window.width)), dtype=np.uint8)\n",
    "        shapes = ((geom, 1) for geom in gdf['geometry'])\n",
    "        burned = rasterize(shapes, out=raster, fill=0, transform=transform, all_touched=True)\n",
    "        distance_grid = scipy.ndimage.distance_transform_edt(burned == 0)\n",
    "        decay_grid = np.exp(-0.07 * distance_grid)\n",
    "\n",
    "        clipped_meta = src.meta.copy()\n",
    "        clipped_meta.update({\"driver\": \"GTiff\", \"height\": int(window.height), \"width\": int(window.width), \"transform\": transform, \"dtype\": rasterio.float32, \"count\": 1, \"compress\": 'lzw'})\n",
    "        output_path = os.path.join(tif_dir, f'exponential_decay_CO_roads_{tile_number}.tif')\n",
    "        with rasterio.open(output_path, 'w', **clipped_meta) as dst:\n",
    "            dst.write(decay_grid.astype(np.float32), 1)\n",
    "\n",
    "    src_ds = gdal.Open(output_path, gdal.GA_ReadOnly)\n",
    "    target_ds = gdal.Open(tif_path, gdal.GA_ReadOnly)\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    output_resampled_path = os.path.join(tif_dir, f\"output_resampled_dataRoads_{tile_number}.tif\")\n",
    "    out_ds = driver.Create(output_resampled_path, target_ds.RasterXSize, target_ds.RasterYSize, 1, src_ds.GetRasterBand(1).DataType)\n",
    "    out_ds.SetGeoTransform(target_ds.GetGeoTransform())\n",
    "    out_ds.SetProjection(target_ds.GetProjection())\n",
    "    gdal.ReprojectImage(src_ds, out_ds, src_ds.GetProjection(), target_ds.GetProjection(), gdal.GRA_Bilinear)\n",
    "    src_ds, target_ds, out_ds = None, None, None\n",
    "    os.remove(output_path)\n",
    "\n",
    "    return output_resampled_path\n",
    "    del extent_data, bbox, graph, gdf\n",
    "    del window, transform, raster, shapes, burned, distance_grid, decay_grid\n",
    "    del clipped_meta, output_path\n",
    "    del src_ds, target_ds, driver, output_resampled_path\n",
    "\n",
    "# OFFSET_X = 20  # Horizontal offset\n",
    "# OFFSET_Y = 20  # Vertical offset\n",
    "\n",
    "def get_tiles(ds, width=CHIP_SIZE, height=CHIP_SIZE):\n",
    "    offset_x = 128  # Horizontal offset\n",
    "    offset_y = 128 # Vertical offset\n",
    "\n",
    "    \n",
    "    nols, nrows = ds.meta['width'], ds.meta['height']\n",
    "    #offsets = product(range(0, nols, width), range(0, nrows, height))\n",
    "    offsets = product(range(0, nols, offset_x), range(0, nrows, offset_y))\n",
    "\n",
    "    big_window = windows.Window(col_off=0, row_off=0, width=nols, height=nrows)\n",
    "    for col_off, row_off in offsets:\n",
    "        window = windows.Window(col_off=col_off, row_off=row_off, width=width, height=height).intersection(big_window)\n",
    "        transform = windows.transform(window, ds.transform)\n",
    "        yield window, transform\n",
    "\n",
    "def process_file(label, input_filepath, output_folder):\n",
    "    #OFFSET_X = 20\n",
    "    #OFFSET_Y = 20\n",
    "    with rio.open(input_filepath) as inds:\n",
    "        nodata = inds.nodata  # Get the NoData value from the dataset\n",
    "        meta = inds.meta.copy()\n",
    "        \n",
    "        for window, transform in get_tiles(inds):\n",
    "            if window.width == CHIP_SIZE and window.height == CHIP_SIZE:  # Check if the tile dimensions are as expected\n",
    "                data = inds.read(window=window)\n",
    "                if nodata is not None:\n",
    "                    valid_data_mask = (data != nodata)\n",
    "                else:\n",
    "                    valid_data_mask = (data == data)\n",
    "                \n",
    "                if valid_data_mask.any():  # Check if there's any valid data within the tile\n",
    "                    meta['transform'] = transform\n",
    "                    meta['width'], meta['height'] = window.width, window.height\n",
    "                    outpath = os.path.join(output_folder, output_filename.format(int(window.col_off), int(window.row_off)))\n",
    "                    with rio.open(outpath, 'w', **meta) as outds:\n",
    "                        outds.write(data)\n",
    "    print(f\"Processing for {label} completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for tile_number in TILENUMBER:\n",
    "        tif_path = f\"{THEFOLDER}\\\\pcltile_{tile_number}.tif\"\n",
    "        tif_dir = f\"{THEFOLDER}\\\\INFERENCETILESGLOBAL\\\\{tile_number}\"\n",
    "        os.makedirs(tif_dir, exist_ok=True)\n",
    "\n",
    "        resampled_files = [\n",
    "            process_dem(tif_path, tif_dir, tile_number),\n",
    "            process_lidar(tif_path, tif_dir, tile_number),\n",
    "            process_rivers(tif_path, tif_dir, tile_number),\n",
    "            process_roads(tif_path, tif_dir, tile_number)\n",
    "        ]\n",
    "\n",
    "        delete_non_resampled_files([os.path.basename(f) for f in resampled_files], tif_dir)\n",
    "\n",
    "        # Define input files as a dictionary\n",
    "        input_files = {\n",
    "            'lidar': f'output_resampled_dataLIDAR_{tile_number}.tif',\n",
    "            'dem': f'output_resampled_dataDEM_{tile_number}.tif',\n",
    "            'roads': f'output_resampled_dataRoads_{tile_number}.tif',\n",
    "            'rivers': f'output_resampled_dataRivers_{tile_number}.tif'\n",
    "        }\n",
    "        output_filename = 'tile_{}-{}.tif'\n",
    "\n",
    "        # Define the base output path\n",
    "        out_base_path = f\"{THEFOLDER}\\\\INFERENCETILESGLOBAL\"\n",
    "        os.makedirs(out_base_path, exist_ok=True)\n",
    "\n",
    "        # Process each file\n",
    "        for label, filename in input_files.items():\n",
    "            input_filepath = os.path.join(tif_dir, filename)\n",
    "            output_folder = os.path.join(out_base_path, label)\n",
    "            os.makedirs(output_folder, exist_ok=True)\n",
    "            process_file(label, input_filepath, output_folder)\n",
    "\n",
    "        print(f\"Processing for tile {tile_number} completed.\")\n",
    "\n",
    "    print(\"All processing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59589d7-e772-4230-bab2-b8feea7a8534",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict\n",
    "\n",
    "import os\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# tilename = '0-0'\n",
    "# input_hag_path = f\"C:\\\\Users\\\\USERNAME\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\hag\\\\tile_{tilename}.tif\"\n",
    "# input_dem_path = f\"C:\\\\Users\\\\USERNAME\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\dem\\\\tile_{tilename}.tif\"\n",
    "# input_roads_path = f\"C:\\\\Users\\\\USERNAME\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\roads\\\\tile_{tilename}.tif\"\n",
    "# input_rivers_path = f\"C:\\\\Users\\\\USERNAME\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\rivers\\\\tile_{tilename}.tif\"\n",
    "\n",
    "def load_and_preprocess_image(hag_path, dem_path, roads_path, rivers_path):\n",
    "    with rasterio.open(hag_path) as src:\n",
    "        hag_image = src.read(1)\n",
    "    with rasterio.open(dem_path) as src:\n",
    "        dem_image = src.read(1)\n",
    "    with rasterio.open(roads_path) as src:\n",
    "        roads_image = src.read(1)\n",
    "    with rasterio.open(rivers_path) as src:\n",
    "        rivers_image = src.read(1)\n",
    "\n",
    "    # Normalize and stack the images\n",
    "    hag_image = np.array(hag_image).astype('float32') / hag_max\n",
    "    dem_image = np.array(dem_image).astype('float32') / dem_max\n",
    "    roads_image = np.array(roads_image).astype('float32') / roads_max\n",
    "    rivers_image = np.array(rivers_image).astype('float32') / rivers_max\n",
    "\n",
    "    # Stack images along the last dimension\n",
    "    combined_image = np.stack([hag_image, dem_image, roads_image, rivers_image], axis=-1)\n",
    "\n",
    "    # Add batch dimension\n",
    "    combined_image = np.expand_dims(combined_image, axis=0)\n",
    "    return combined_image\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(identifier_list)):\n",
    "    tilename = identifier_list[i]\n",
    "    #print(tilename)\n",
    "    #input_hag_path = f\"{out_base_path}\\\\lidar\\\\tile_{tilename}.tif\"\n",
    "    #input_dem_path = f\"{out_base_path}\\\\dem\\\\tile_{tilename}.tif\"\n",
    "    #input_roads_path = f\"{out_base_path}\\\\roads\\\\tile_{tilename}.tif\"\n",
    "    #input_rivers_path = f\"{out_base_path}\\\\tile_{tilename}.tif\"\n",
    "\n",
    "    input_hag_path = os.path.join(out_base_path, \"lidar\", f\"tile_{tilename}.tif\")\n",
    "    input_dem_path = os.path.join(out_base_path, \"dem\", f\"tile_{tilename}.tif\")\n",
    "    input_roads_path = os.path.join(out_base_path, \"roads\", f\"tile_{tilename}.tif\")\n",
    "    input_rivers_path = os.path.join(out_base_path, \"rivers\", f\"tile_{tilename}.tif\")\n",
    "\n",
    "\n",
    "    input_image = load_and_preprocess_image(input_hag_path, input_dem_path, input_roads_path, input_rivers_path)\n",
    "    predicted_image = model.predict(input_image)\n",
    "    predicted_image = np.squeeze(predicted_image)\n",
    "    \n",
    "    # Debug print to check if all outputs are the same\n",
    "    #print(\"Unique values in predicted output:\", np.unique(predicted_image))\n",
    "    \n",
    "    # Adjust the scaling factor based on how the labels were scaled during training\n",
    "    predicted_image *= 100\n",
    "    \n",
    "    #output_image_path = f\"{THEFOLDER}\\\\predictions\\\\predicted_tile_{tilename}.tif\"\n",
    "    predictions_folder = os.path.join(THEFOLDER, \"predictions\")\n",
    "    os.makedirs(predictions_folder, exist_ok=True)\n",
    "    output_image_path = os.path.join(predictions_folder, f\"predicted_tile_{tilename}.tif\")\n",
    "\n",
    "    \n",
    "    with rasterio.open(input_dem_path) as src: \n",
    "        profile = src.profile\n",
    "    \n",
    "    with rasterio.open(output_image_path, 'w', **profile) as dst:\n",
    "        dst.write(predicted_image.astype(rasterio.uint8), 1)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8d7355-c42e-476f-8c2c-7f9f7ff54caf",
   "metadata": {},
   "source": [
    "# Merge global Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fecd7d-4990-45ab-9917-0836ad153dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "\n",
    "#TILENUMBER = '70000-40000'\n",
    "\n",
    "# Define the base folder and output paths\n",
    "#THEFOLDER = r\"C:\\Users\\USERNAME\\OneDrive\\Desktop\\GlobalPCL\"\n",
    "predictions_folder = os.path.join(THEFOLDER, \"predictions\")\n",
    "output_dir = os.path.join(THEFOLDER, \"mergedoutput\")\n",
    "\n",
    "# Create necessary directories if they don't exist\n",
    "os.makedirs(predictions_folder, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_base_name = \"predMerged_\"  # Base name for output files\n",
    "\n",
    "# Get a list of TIFF files\n",
    "tifs = glob.glob(os.path.join(predictions_folder, \"*.tif\"))\n",
    "\n",
    "# Define chunk size for processing\n",
    "chunk_size = 300\n",
    "\n",
    "# Calculate the number of chunks needed\n",
    "num_chunks = len(tifs) // chunk_size\n",
    "if len(tifs) % chunk_size != 0:\n",
    "    num_chunks += 1  # Add one more chunk for the remaining files\n",
    "\n",
    "# Loop through the TIFF files in chunks\n",
    "for chunk_id in range(num_chunks):\n",
    "    start_idx = chunk_id * chunk_size\n",
    "    end_idx = min((chunk_id + 1) * chunk_size, len(tifs))\n",
    "    chunk_tifs = tifs[start_idx:end_idx]\n",
    "    \n",
    "    output_tif = os.path.join(output_dir, f\"{output_base_name}{chunk_id + 1}.tif\")\n",
    "\n",
    "    merge_command_hag = [\n",
    "        \"python\",\n",
    "        \"C:\\\\Users\\\\USERNAME\\\\anaconda3\\\\envs\\\\globalpcl\\\\Scripts\\\\gdal_merge.py\",\n",
    "        \"--config\", \"CHECK_DISK_FREE_SPACE\", \"FALSE\",\n",
    "        \"-o\", output_tif,\n",
    "        \"-n\", \"-9999\",\n",
    "        \"-a_nodata\", \"-9999\",\n",
    "    ] + chunk_tifs\n",
    "\n",
    "    # Run the gdal_merge command for the current chunk\n",
    "    process_hag = subprocess.run(merge_command_hag, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    # Check if the command for the current chunk was successful\n",
    "    if process_hag.returncode != 0:\n",
    "        # An error occurred, print the error\n",
    "        print(f\"Error occurred while merging TIFF files for chunk {chunk_id + 1}:\")\n",
    "        print(process_hag.stderr)\n",
    "    else:\n",
    "        print(f\"TIFF files merged successfully for chunk {chunk_id + 1}. Output: {output_tif}\")\n",
    "\n",
    "# Merge all chunks into a final output file\n",
    "final_output_tif = os.path.join(THEFOLDER, \"FINALOUTPUTTILES\", f\"predMerged_PCL_{TILENUMBER}.tif\")\n",
    "os.makedirs(os.path.dirname(final_output_tif), exist_ok=True)\n",
    "\n",
    "chunk_tifs = glob.glob(os.path.join(output_dir, \"*.tif\"))\n",
    "\n",
    "merge_command_final = [\n",
    "    \"python\",\n",
    "    \"C:\\\\Users\\\\USERNAME\\\\anaconda3\\\\envs\\\\globalpcl\\\\Scripts\\\\gdal_merge.py\",\n",
    "    \"--config\", \"CHECK_DISK_FREE_SPACE\", \"FALSE\",\n",
    "    \"-o\", final_output_tif,\n",
    "    \"-n\", \"-9999\",\n",
    "    \"-a_nodata\", \"-9999\",\n",
    "] + chunk_tifs\n",
    "\n",
    "# Run the gdal_merge command for the final merge\n",
    "process_final = subprocess.run(merge_command_final, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "# Check if the command for the final merge was successful\n",
    "if process_final.returncode != 0:\n",
    "    # An error occurred, print the error\n",
    "    print(\"Error occurred while merging final TIFF files:\")\n",
    "    print(process_final.stderr)\n",
    "else:\n",
    "    print(\"Final TIFF files merged successfully.\")\n",
    "\n",
    "# Clean up temporary chunk files\n",
    "for tif in chunk_tifs:\n",
    "    try:\n",
    "        os.remove(tif)\n",
    "        print(f\"Deleted {tif}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to delete {tif}: {e}\")\n",
    "\n",
    "print(\"Done\")\n",
    "\n",
    "import shutil\n",
    "\n",
    "if os.path.exists(predictions_folder):\n",
    "    shutil.rmtree(predictions_folder)\n",
    "os.makedirs(predictions_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef758eee-d53c-4ff0-b379-1ad1c6ebac98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
