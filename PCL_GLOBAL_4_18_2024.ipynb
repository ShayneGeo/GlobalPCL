{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac69fa52-5728-4d1d-8e5b-17225699d52b",
   "metadata": {},
   "source": [
    "# Merge LIDAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d630c9-d574-4e91-8fde-75b36ff20996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "\n",
    "# Download the 6 Colorado tiles from\n",
    "# https://langnico.github.io/globalcanopyheight/assets/tile_index.html\n",
    "\n",
    "# Define input folder containing lidar tiles\n",
    "tif_dir = r\"C:\\Users\\OneDrive\\Desktop\\GLOBALPCL\\lidarHAG\"\n",
    "\n",
    "# Define merged output location\n",
    "output_tif  = r\"C:\\Users\\OneDrive\\Desktop\\GLOBALPCL\\lidarHAG\\LIDAR_GEDI_CO.tif\"\n",
    "\n",
    "tifs = glob.glob(os.path.join(tif_dir, \"*Map*.tif\"))\n",
    "\n",
    "# Prepare the gdal_merge command for HAG\n",
    "merge_command_hag = [\n",
    "    \"python\",\n",
    "    # Change this to your local gdal repository \n",
    "    \"C:\\\\Users\\\\anaconda3\\\\envs\\\\lidarpods\\\\Scripts\\\\gdal_merge.py\",\n",
    "    \"--config\", \"CHECK_DISK_FREE_SPACE\", \"FALSE\",\n",
    "    #\"-ot\", \"Byte\",\n",
    "    \"-o\", output_tif,\n",
    "    \"-n\", \"255\",\n",
    "    \"-a_nodata\",\"255\",\n",
    "    \n",
    "    \n",
    "] + tifs\n",
    "\n",
    "# Run the gdal_merge command for HAG and capture the output\n",
    "process_hag = subprocess.run(merge_command_hag, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "# Check if the command for HAG was successful\n",
    "if process_hag.returncode != 0:\n",
    "    # An error occurred, print the error\n",
    "    print(\"Error occurred while merging TIFF files HAG:\")\n",
    "    print(process_hag.stderr)\n",
    "else:\n",
    "    print(\"TIFF files merged successfully for HAG.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe024bf-832e-43db-a802-1b9b12174bad",
   "metadata": {},
   "source": [
    "# Tiling the PCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b545a384-5fe5-448f-88af-9c6cf81319d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import product\n",
    "import rasterio as rio\n",
    "from rasterio import windows\n",
    "\n",
    "# set path to input PCL\n",
    "in_path = 'C:\\\\Users\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\Input\\\\'\n",
    "input_filename = 'pcl_west_wgs_CO.tif'\n",
    "\n",
    "# set path for tiled PCL\n",
    "out_path = 'C:\\\\Users\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\Tiles\\\\'\n",
    "output_filename = 'tile_{}-{}.tif'\n",
    "\n",
    "widthtile = 200\n",
    "heighttile = 200\n",
    "\n",
    "def get_tiles(ds, width=widthtile, height=heighttile):\n",
    "    nols, nrows = ds.meta['width'], ds.meta['height']\n",
    "    offsets = product(range(0, nols, width), range(0, nrows, height))\n",
    "    big_window = windows.Window(col_off=0, row_off=0, width=nols, height=nrows)\n",
    "    for col_off, row_off in  offsets:\n",
    "        window =windows.Window(col_off=col_off, row_off=row_off, width=width, height=height).intersection(big_window)\n",
    "        transform = windows.transform(window, ds.transform)\n",
    "        yield window, transform\n",
    "\n",
    "# this also check for empty tiles (does not exist in land locked CO)            \n",
    "with rio.open(os.path.join(in_path, input_filename)) as inds:\n",
    "    tile_width, tile_height = widthtile, heighttile\n",
    "    nodata = inds.nodata  # Get the NoData value from the dataset\n",
    "\n",
    "    meta = inds.meta.copy()\n",
    "\n",
    "    for window, transform in get_tiles(inds):\n",
    "        print(window)\n",
    "        data = inds.read(window=window)\n",
    "        # Check if the entire tile is NoData\n",
    "        if nodata is not None and not (data == nodata).all():\n",
    "            meta['transform'] = transform\n",
    "            meta['width'], meta['height'] = window.width, window.height\n",
    "            outpath = os.path.join(out_path, output_filename.format(int(window.col_off), int(window.row_off)))\n",
    "            with rio.open(outpath, 'w', **meta) as outds:\n",
    "                outds.write(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392810d7-ce6e-4ff3-be0e-ac4bd2622677",
   "metadata": {},
   "source": [
    "# Create a list of filepaths to the tiles of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cd4119-29e2-4a89-b1a9-302141bbfee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# Path to the directory\n",
    "directory_path = 'C:\\\\Users\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\Tiles\\\\'\n",
    "\n",
    "# Pattern to match all .tif files\n",
    "pattern = '*.tif'\n",
    "\n",
    "# Construct full pattern path\n",
    "full_pattern = directory_path + pattern\n",
    "\n",
    "# List all .tif files in the directory\n",
    "tif_files = glob.glob(full_pattern)\n",
    "\n",
    "# Initialize an empty list to store filenames without extensions\n",
    "filenames_without_extension = []\n",
    "\n",
    "# Loop through each file in the list for naming convention \n",
    "for file_path in tif_files:\n",
    "    filename_with_extension = os.path.basename(file_path)\n",
    "    filename_without_extension = os.path.splitext(filename_with_extension)[0]\n",
    "    filenames_without_extension.append(filename_without_extension)\n",
    "\n",
    "print(filenames_without_extension[0])\n",
    "print(len(filenames_without_extension))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60243129-83a0-4b79-b93d-f7201c77f3fc",
   "metadata": {},
   "source": [
    "# Get the data from planitary computer and export as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6431df6-a404-4b19-b972-713588e9c0b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#https://planetarycomputer.microsoft.com/dataset/io-lulc-9-class#Example-Notebook\n",
    "\n",
    "import rasterio\n",
    "from rasterio.warp import transform as warp_transform\n",
    "import dask.array as da\n",
    "import dask.distributed\n",
    "from matplotlib.colors import ListedColormap\n",
    "import pystac_client\n",
    "import pyproj\n",
    "import cartopy.crs as ccrs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import planetary_computer\n",
    "import rasterio\n",
    "import rasterio.features\n",
    "import stackstac\n",
    "from pystac.extensions.eo import EOExtension as eo\n",
    "import dask.dataframe as dd\n",
    "import datetime\n",
    "import odc.stac\n",
    "import rioxarray\n",
    "import time\n",
    "\n",
    "# LIDAR HAG \n",
    "with rasterio.open(r\"C:\\Users\\OneDrive\\Desktop\\GLOBALPCL\\lidarHAG\\LIDAR_GEDI_CO.tif\") as src:\n",
    "    arrayHAG = src.read(1)\n",
    "    hag_transform = src.transform\n",
    "    hag_crs = src.crs\n",
    "\n",
    "for w in range(len(tif_files)):\n",
    "    #try:\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # Path to your GeoTIFF files\n",
    "    tif_path = tif_files[w]\n",
    "\n",
    "    # Open the GeoTIFF file\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        array = src.read()\n",
    "        # Get the dimensions of the array\n",
    "        bands, height, width = array.shape\n",
    "        # Get the affine transformation for the GeoTIFF\n",
    "        affine_transform = src.transform\n",
    "\n",
    "        # Get the dimensions of the image\n",
    "        width = src.width\n",
    "        height = src.height\n",
    "\n",
    "        # Initialize a list to hold the coordinates\n",
    "        wgs_coords = []\n",
    "\n",
    "        # Loop over each pixel in the image\n",
    "        for row in range(height):\n",
    "            for col in range(width):\n",
    "                # Get the x, y coordinates of the center of the pixel\n",
    "                x, y = affine_transform * (col + 0.5, row + 0.5)\n",
    "\n",
    "                # Transform the x, y coordinates to longitude, latitude (WGS84)\n",
    "                lon, lat = warp_transform(src.crs, 'EPSG:4326', [x], [y])\n",
    "\n",
    "                # Append the WGS84 coordinates to the list\n",
    "                wgs_coords.append((lon[0], lat[0]))\n",
    "\n",
    "    print(len(wgs_coords))\n",
    "\n",
    "    catalog = pystac_client.Client.open(\n",
    "        \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "        modifier=planetary_computer.sign_inplace,\n",
    "    )\n",
    "\n",
    "    central_lon = sum(lon for lon, _ in wgs_coords) / len(wgs_coords)\n",
    "    central_lat = sum(lat for _, lat in wgs_coords) / len(wgs_coords)\n",
    "\n",
    "    offset_lon, offset_lat = 0.25, 0.15  \n",
    "\n",
    "    # Create the bounding box around Fort Collins, Colorado\n",
    "    bbox_of_interest = [\n",
    "        central_lon - offset_lon,  # Min longitude (west boundary)\n",
    "        central_lat - offset_lat,  # Min latitude (south boundary)\n",
    "        central_lon + offset_lon,  # Max longitude (east boundary)\n",
    "        central_lat + offset_lat   # Max latitude (north boundary)\n",
    "    ]\n",
    "\n",
    "    print(\"Bounding box: \", bbox_of_interest)\n",
    "\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "    # LULC Global 9 - Classes #############################\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "\n",
    "    # 0 - No Data\n",
    "    # 1 - Water\n",
    "    # 2 - Trees\n",
    "    # 4 - Flooded vegetation\n",
    "    # 5 - Crops\n",
    "    # 7 - Built area\n",
    "    # 8 - Bare ground\n",
    "    # 9 - Snow/ice\n",
    "    # 10 - Clouds\n",
    "    # 11 - Rangeland\n",
    "\n",
    "    search = catalog.search(collections=[\"io-lulc-9-class\"], bbox=bbox_of_interest)\n",
    "\n",
    "    items = search.item_collection()\n",
    "\n",
    "    item = items[0]\n",
    "\n",
    "    LULC_stack = (\n",
    "        stackstac.stack(\n",
    "            items,\n",
    "            dtype=np.ubyte,\n",
    "            fill_value=255,\n",
    "            bounds_latlon=bbox_of_interest,\n",
    "            sortby_date=False,\n",
    "        )\n",
    "        .assign_coords(\n",
    "            time=pd.to_datetime([item.properties[\"start_datetime\"] for item in items])\n",
    "            .tz_convert(None)\n",
    "            .to_numpy()\n",
    "        )\n",
    "        .sortby(\"time\")\n",
    "    )\n",
    "    # Assuming the 'classes' attribute is accessible from 'item'\n",
    "    print(item.properties)\n",
    "\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "    # SENTINEL 2 ##########################################\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "    \n",
    "    # Sentinel adjust time window\n",
    "    time_of_interest = \"2021-04-01/2021-08-01\"\n",
    "\n",
    "    search = catalog.search(\n",
    "        collections=[\"sentinel-2-l2a\"],\n",
    "        bbox=bbox_of_interest,\n",
    "        datetime=time_of_interest,\n",
    "        query={\"eo:cloud_cover\": {\"lt\": 10}},\n",
    "    )\n",
    "\n",
    "    items = search.item_collection()\n",
    "\n",
    "    least_cloudy_item = min(items, key=lambda item: eo.ext(item).cloud_cover)\n",
    "\n",
    "    utm_crs = pyproj.CRS.from_user_input(f'EPSG:326{(int((central_lon + 180) / 6) % 60) + 1}')\n",
    "\n",
    "    # Create a stack from the least cloudy item\n",
    "    if least_cloudy_item:\n",
    "        sentinel_stack = stackstac.stack(\n",
    "            [least_cloudy_item],\n",
    "            bounds_latlon=bbox_of_interest,\n",
    "            epsg=utm_crs.to_epsg(),\n",
    "            resolution=10,\n",
    "            assets=[\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\"],  \n",
    "            fill_value=0\n",
    "        )\n",
    "\n",
    "        print(\"Stack created successfully from the least cloudy item\")\n",
    "    else:\n",
    "        print(\"No least cloudy item found to create a stack\")\n",
    "\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "    # Landsat #############################################\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "\n",
    "    # time_of_interest = \"2021-04-01/2021-08-01\"\n",
    "\n",
    "    # search = catalog.search(\n",
    "    #     collections=[\"landsat-c2-l2\"],\n",
    "    #     bbox=bbox_of_interest,\n",
    "    #     datetime=time_of_interest,\n",
    "    #     query={\"eo:cloud_cover\": {\"lt\": 10}},\n",
    "    # )\n",
    "\n",
    "    # items = search.item_collection()\n",
    "\n",
    "    # selected_item = min(items, key=lambda item: eo.ext(item).cloud_cover)\n",
    "\n",
    "    # utm_crs = pyproj.CRS.from_user_input(f'EPSG:326{(int((central_lon + 180) / 6) % 60) + 1}')\n",
    "\n",
    "    # bands_of_interest = [\"green\", \"nir08\"]\n",
    "    # Landsat_stack = odc.stac.stac_load(\n",
    "    #     [selected_item], bands=bands_of_interest, bbox=bbox_of_interest\n",
    "    # ).isel(time=0)\n",
    "\n",
    "\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "\n",
    "    # items = list(catalog.get_collection(\"hgb\").get_all_items())\n",
    "    # item = items[0]\n",
    "\n",
    "    # da = rioxarray.open_rasterio(\n",
    "    #     item.assets[\"aboveground\"].href, chunks=dict(x=2560, y=2560)\n",
    "    # )\n",
    "\n",
    "    # # Transform our data array to a dataset by selecting the only data variable ('band')\n",
    "    # # renaming it to something useful ('biomass')\n",
    "    # ds = da.to_dataset(dim=\"band\").rename({1: \"biomass\"})\n",
    "    # ds\n",
    "\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "\n",
    "\n",
    "\n",
    "    # Define a list of geographic coordinates for the points of interest\n",
    "    points = wgs_coords\n",
    "\n",
    "\n",
    "    # Initialize the WGS 84 geographic coordinate system (EPSG:4326)\n",
    "    geographic = pyproj.CRS('EPSG:4326')\n",
    "\n",
    "    # Open the TIFF file\n",
    "    # with rasterio.open(tif_path) as src:\n",
    "    #     # Read the raster data\n",
    "    #     array = src.read()\n",
    "    #     # Get the dimensions of the array\n",
    "    #     bands, height, width = array.shape\n",
    "\n",
    "    df_PCL = pd.DataFrame(array.reshape(bands, -1).T, columns=[f'Band_{i+1}' for i in range(bands)])\n",
    "    df_PCL.columns = [\"PCLVALUE\"]\n",
    "\n",
    "    #df_PCL = pd.DataFrame(arrayHAG.reshape(bands, -1).T, columns=[f'Band_{i+1}' for i in range(bands)])\n",
    "    #df_PCL.columns = [\"hag\"]\n",
    "\n",
    "    df_list = []\n",
    "    df2_list = []\n",
    "    #df3_list = []\n",
    "    #df4_list = []\n",
    "    hag_values = []\n",
    "    geographic = pyproj.CRS('EPSG:4326')\n",
    "\n",
    "    for lon, lat in points:\n",
    "        utm_crs = pyproj.CRS.from_user_input(f'EPSG:326{(int((lon + 180) / 6) % 60) + 1}')\n",
    "        transformer = pyproj.Transformer.from_crs(geographic, utm_crs, always_xy=True)\n",
    "\n",
    "        # Convert geographic coordinates to UTM coordinates\n",
    "        utm_x, utm_y = transformer.transform(lon, lat)\n",
    "\n",
    "\n",
    "        # Extract values across years for the given point LULC\n",
    "        values_across_years_LULC = LULC_stack.sel(x=utm_x, y=utm_y, method='nearest')#.values\n",
    "        df_list.append(values_across_years_LULC)\n",
    "\n",
    "        values_across_sentinalbands = sentinel_stack.sel(x=utm_x, y=utm_y, method='nearest')#.values\n",
    "        df2_list.append(values_across_sentinalbands)\n",
    "\n",
    "        #values_across_landsatbands = Landsat_stack.sel(x=utm_x, y=utm_y, method='nearest')#.values\n",
    "        #df3_list.append(values_across_landsatbands)\n",
    "\n",
    "        # Transform geographic coordinates to pixel coordinates in the HAG raster\n",
    "        x, y = warp_transform(geographic, hag_crs, [lon], [lat])\n",
    "        col, row = ~hag_transform * (x[0], y[0])  # Apply the inverse of the affine transform\n",
    "        col, row = int(col), int(row)  # Convert to integer pixel indices\n",
    "    \n",
    "        # Extract the HAG value if the indices are within the raster dimensions\n",
    "        if (0 <= col < arrayHAG.shape[1]) and (0 <= row < arrayHAG.shape[0]):\n",
    "            hag_value = arrayHAG[row, col]\n",
    "        else:\n",
    "            hag_value = np.nan  # Use NaN for coordinates outside the raster\n",
    "    \n",
    "        hag_values.append(hag_value)\n",
    "\n",
    "    \n",
    "\n",
    "    df_COMPUTED = da.compute(*df_list) \n",
    "    df = pd.concat([pd.DataFrame(row).T for row in df_COMPUTED], ignore_index=True)\n",
    "    time_coords = LULC_stack.time.values\n",
    "    time_columns = [f'LULC_{pd.to_datetime(str(time)).strftime(\"%Y\")}' for time in time_coords]\n",
    "    df.columns = time_columns\n",
    "    print(\"LULC\")\n",
    "\n",
    "    df2_COMPUTED = da.compute(*df2_list) \n",
    "    df2 = pd.concat([pd.DataFrame(row) for row in df2_COMPUTED], ignore_index=True)\n",
    "    df2.columns = [\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\"]\n",
    "    print(\"SENTINEL\")\n",
    "\n",
    "    #df3_COMPUTED = da.compute(*df3_list) \n",
    "    #df3 = pd.concat([pd.DataFrame(row) for row in df3_COMPUTED], ignore_index=True)\n",
    "    # df3.columns = [\"green\"]\n",
    "    # print(\"LANDSAT\")\n",
    "\n",
    "    # df4_COMPUTED = da.compute(*df4_list) \n",
    "    # df4 = pd.concat([pd.DataFrame(row) for row in df4_COMPUTED], ignore_index=True)\n",
    "    # df4.columns = [\"HAG\"]\n",
    "    # print(\"HAG\")\n",
    "\n",
    "    combined_df = pd.concat([df, df2, df_PCL], axis=1)\n",
    "    combined_df\n",
    "    combined_df['HAG'] = hag_values\n",
    "\n",
    "    # Determine time\n",
    "    end_time = datetime.datetime.now()\n",
    "    time_diff = end_time - start_time\n",
    "    print(f\"The operation took {time_diff} seconds.\")\n",
    "\n",
    "    # save dataframe of information\n",
    "    combined_df.to_csv(f'C:\\\\Users\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\OutputCSV\\\\df_LULC_SENT_{filenames_without_extension[w]}.csv', index=False)\n",
    "    time.sleep(10)\n",
    "    #except:\n",
    "    #    continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f873b90b-b8da-4033-8a66-4178f3330032",
   "metadata": {},
   "source": [
    "# Merge the data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f9f971-74ee-41c6-8469-6f45b6ab2553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directory containing the CSV files\n",
    "directory = \"C:\\\\Users\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\OutputCSV\"\n",
    "\n",
    "# Initialize an empty DataFrame to hold all the data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        # Read the CSV file and append it to the combined DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "# Now combined_df contains all the data from the CSV files in the directory\n",
    "print(combined_df.shape)\n",
    "\n",
    "# Filter out samples where all B01 to B12 columns are zeros\n",
    "# ###! I NEED TO FIGURE OUT WHY SOME Sentinel information is is all 0s !###\n",
    "cols_to_check = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12']\n",
    "combined_df = combined_df.loc[~(combined_df[cols_to_check] == 0).all(axis=1)]\n",
    "\n",
    "\n",
    "print(combined_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf683d0-7ece-4a2b-9a8c-92a294208a16",
   "metadata": {},
   "source": [
    "# Data Balance test 1 - this is an idea, may be a better workaround for the \n",
    "# imbalanced dataset (consider weights, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5112ad91-8d3c-4440-84b8-b38813f497da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming combined_df is your DataFrame\n",
    "# First, count the rows where PCLVALUE > 80\n",
    "count_above_80 = (combined_df['PCLVALUE'] > 80).sum()\n",
    "\n",
    "# Then, filter rows where PCLVALUE < 20\n",
    "filtered_below_20 = combined_df[combined_df['PCLVALUE'] < 80]\n",
    "\n",
    "# Now, randomly sample from the filtered_below_20 to match the count of rows above 80\n",
    "subset_below_20 = filtered_below_20.sample(n=count_above_80)\n",
    "\n",
    "# If you need to combine both subsets into a single DataFrame\n",
    "final_subset = pd.concat([combined_df[combined_df['PCLVALUE'] > 80], subset_below_20])\n",
    "\n",
    "print(len(final_subset))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming combined_df is your DataFrame and 'PCLVALUE' is the column of interest\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(final_subset['PCLVALUE'], bins=30, alpha=0.7, color='blue')\n",
    "plt.title('Histogram of PCLVALUE')\n",
    "plt.xlabel('PCLVALUE')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6b4cf6-55bc-46cf-8e04-212338794090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Define a threshold to consider a PCLVALUE as high\n",
    "# high_value_threshold = combined_df['PCLVALUE'].quantile(0.8)  # for example, take the 90th percentile\n",
    "\n",
    "# # Filter the dataset for all high PCLVALUE rows\n",
    "# high_value_df = combined_df[combined_df['PCLVALUE'] > high_value_threshold]\n",
    "\n",
    "# # Sample with replacement to increase the number of high PCLVALUE rows\n",
    "# # You can adjust the `n` to control how many times you want to replicate the high-end data\n",
    "# oversampled_high_value_df = high_value_df.sample(n=2000000, replace=True)\n",
    "\n",
    "# # Concatenate the original dataframe with the oversampled high PCLVALUE dataframe\n",
    "# combined_df_oversampled = pd.concat([combined_df, oversampled_high_value_df])\n",
    "\n",
    "# # Shuffle the dataframe if necessary\n",
    "# combined_df_oversampled = combined_df_oversampled.sample(frac=1).reset_index(drop=True)\n",
    "# combined_df_oversampled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108af514-be6d-47bd-8168-4dd2a10a692c",
   "metadata": {},
   "source": [
    "# Model fit and evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118641cf-77ee-470e-a343-4cdb9022f4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, callbacks\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "data = combined_df  # Assuming 'combined_df' is loaded correctly\n",
    "X = data[['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12', 'HAG']]\n",
    "y = data['PCLVALUE'] / 100  # Normalize \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Building the model\n",
    "# this is a simple nn model - need more data and training\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Early stopping monitor\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0002),\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mean_absolute_error'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Train the model with early stopping\n",
    "model.fit(X_train_scaled, y_train, epochs=1, batch_size=256, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Plotting actual vs predicted values\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5, color='black', s=10)  # Adjusted marker size\n",
    "plt.title('Modeled vs. Predicted PCLVALUE')\n",
    "plt.xlabel('Modeled PCLVALUE')\n",
    "plt.ylabel('Predicted PCLVALUE')\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--')  # Diagonal line\n",
    "plt.grid(True)  # Added grid for better readability\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a85401a-bc8c-4883-9513-7c2aa3068278",
   "metadata": {},
   "source": [
    "# Test learning rates - gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4405211f-47bf-4d9d-87b3-17640ee5d62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "# Directory for the images\n",
    "if not os.path.exists('plots'):\n",
    "    os.makedirs('plots')\n",
    "\n",
    "learning_rates = [0.00001 * i for i in range(1, 35)]  # Learning rates from 0.0001 to 0.001\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1, activation='relu')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=lr),\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['mean_absolute_error'])\n",
    "\n",
    "    model.fit(X_train_scaled, y_train, epochs=5, batch_size=256, validation_split=0.2)\n",
    "    \n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5, color='black', s=1)\n",
    "    plt.title(f'Learning Rate: {lr:.5f}')\n",
    "    plt.xlabel('Actual PCLVALUE')\n",
    "    plt.ylabel('Predicted PCLVALUE')\n",
    "    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.savefig(f'plots/lr_{lr:.5f}.png')\n",
    "    plt.close()\n",
    "\n",
    "# Create the GIF\n",
    "images = []\n",
    "image_paths = sorted(glob.glob('plots/*.png'), key=os.path.getmtime)\n",
    "\n",
    "for filename in image_paths:\n",
    "    images.append(Image.open(filename))\n",
    "images[0].save('C:\\\\Users\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\gif\\\\learning_rate.gif',\n",
    "               save_all=True, append_images=images[1:], optimize=False, duration=400, loop=0)\n",
    "\n",
    "print(\"GIF saved as 'learning_rate.gif'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34721bbb-1a6f-452f-840c-99f739bf35e3",
   "metadata": {},
   "source": [
    "# Model search - keras tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea3a96f-0a27-4892-a050-5f99fcd266f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "\n",
    "def build_model(hp):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "        hp.Int('units', min_value=32, max_value=512, step=32),\n",
    "        activation='relu',\n",
    "        input_shape=(X_train_scaled.shape[1],)\n",
    "    ))\n",
    "    model.add(tf.keras.layers.Dense(\n",
    "        hp.Int('units', min_value=32, max_value=512, step=32),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4, 1e-5])\n",
    "        ),\n",
    "        loss='mean_squared_error',\n",
    "        metrics=['mean_absolute_error']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_mean_absolute_error',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=1,\n",
    "    project_name='PCL_model2'\n",
    ")\n",
    "\n",
    "tuner.search(X_train_scaled, y_train, epochs=1, validation_split=0.2)\n",
    "\n",
    "# Assuming you have already run the hyperparameter tuning with a tuner object\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# To print the summary of the best model's architecture\n",
    "best_model.summary()\n",
    "\n",
    "# To see the best hyperparameters\n",
    "print(best_hyperparameters.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaa99c1-0a0b-4173-9046-64f999c93366",
   "metadata": {},
   "source": [
    "# This code ingests a tile, downloads data, and fits the above model, then exports tif (a bit slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f7f7fc-4610-4f04-b85d-9153cbe10779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.warp import transform as warp_transform\n",
    "#https://planetarycomputer.microsoft.com/dataset/io-lulc-9-class#Example-Notebook\n",
    "import dask.array as da\n",
    "import dask.distributed\n",
    "from matplotlib.colors import ListedColormap\n",
    "import pystac_client\n",
    "import pyproj\n",
    "import cartopy.crs as ccrs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import planetary_computer\n",
    "import rasterio\n",
    "import rasterio.features\n",
    "import stackstac\n",
    "from pystac.extensions.eo import EOExtension as eo\n",
    "import dask.dataframe as dd\n",
    "import datetime\n",
    "import odc.stac\n",
    "import rioxarray\n",
    "\n",
    "tilenumber = \"0-2200\"\n",
    "\n",
    "tif_files = [f\"C:\\\\Users\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\Tiles\\\\tile_{tilenumber}.tif\"]\n",
    "\n",
    "with rasterio.open(r\"C:\\Users\\OneDrive\\Desktop\\GLOBALPCL\\lidarHAG\\LIDAR_GEDI_CO.tif\") as src:\n",
    "    arrayHAG = src.read(1)  # Ensure you read only one band if that's all you need\n",
    "    hag_transform = src.transform\n",
    "    hag_crs = src.crs\n",
    "\n",
    "for w in range(len(tif_files)):\n",
    "    #try:\n",
    "    # Path to your GeoTIFF file\n",
    "    tif_path = tif_files[0]\n",
    "\n",
    "    # Open the GeoTIFF file\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        # Get the affine transformation for the GeoTIFF\n",
    "        affine_transform = src.transform\n",
    "\n",
    "        # Get the dimensions of the image\n",
    "        width = src.width\n",
    "        height = src.height\n",
    "\n",
    "        # Initialize a list to hold the coordinates\n",
    "        wgs_coords = []\n",
    "\n",
    "        # Loop over each pixel in the image\n",
    "        for row in range(height):\n",
    "            for col in range(width):\n",
    "                # Get the x, y coordinates of the center of the pixel\n",
    "                x, y = affine_transform * (col + 0.5, row + 0.5)\n",
    "\n",
    "                # Transform the x, y coordinates to longitude, latitude (WGS84)\n",
    "                lon, lat = warp_transform(src.crs, 'EPSG:4326', [x], [y])\n",
    "\n",
    "                # Append the WGS84 coordinates to the list\n",
    "                wgs_coords.append((lon[0], lat[0]))\n",
    "\n",
    "    print(len(wgs_coords))\n",
    "\n",
    "    # Record the start time\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    catalog = pystac_client.Client.open(\n",
    "        \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "        modifier=planetary_computer.sign_inplace,\n",
    "    )\n",
    "\n",
    "    central_lon = sum(lon for lon, _ in wgs_coords) / len(wgs_coords)\n",
    "    central_lat = sum(lat for _, lat in wgs_coords) / len(wgs_coords)\n",
    "\n",
    "    #central_lon, central_lat = -105.556671,40.370342\n",
    "\n",
    "    offset_lon, offset_lat = 0.25, 0.15  # Adjust these values as needed for the desired area\n",
    "\n",
    "    # Create the bounding box around Fort Collins, Colorado\n",
    "    bbox_of_interest = [\n",
    "        central_lon - offset_lon,  # Min longitude (west boundary)\n",
    "        central_lat - offset_lat,  # Min latitude (south boundary)\n",
    "        central_lon + offset_lon,  # Max longitude (east boundary)\n",
    "        central_lat + offset_lat   # Max latitude (north boundary)\n",
    "    ]\n",
    "\n",
    "    print(\"Bounding box: \", bbox_of_interest)\n",
    "\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "    # LULC Global 9 - Classes #############################\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "\n",
    "    # 0 - No Data\n",
    "    # 1 - Water\n",
    "    # 2 - Trees\n",
    "    # 4 - Flooded vegetation\n",
    "    # 5 - Crops\n",
    "    # 7 - Built area\n",
    "    # 8 - Bare ground\n",
    "    # 9 - Snow/ice\n",
    "    # 10 - Clouds\n",
    "    # 11 - Rangeland\n",
    "\n",
    "    search = catalog.search(collections=[\"io-lulc-9-class\"], bbox=bbox_of_interest)\n",
    "\n",
    "    items = search.item_collection()\n",
    "\n",
    "    item = items[0]\n",
    "\n",
    "    LULC_stack = (\n",
    "        stackstac.stack(\n",
    "            items,\n",
    "            dtype=np.ubyte,\n",
    "            fill_value=255,\n",
    "            bounds_latlon=bbox_of_interest,\n",
    "            sortby_date=False,\n",
    "        )\n",
    "        .assign_coords(\n",
    "            time=pd.to_datetime([item.properties[\"start_datetime\"] for item in items])\n",
    "            .tz_convert(None)\n",
    "            .to_numpy()\n",
    "        )\n",
    "        .sortby(\"time\")\n",
    "    )\n",
    "    # Assuming the 'classes' attribute is accessible from 'item'\n",
    "    print(item.properties)\n",
    "\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "    # SENTINEL 2 ##########################################\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "\n",
    "    time_of_interest = \"2021-04-01/2021-08-01\"\n",
    "\n",
    "    search = catalog.search(\n",
    "        collections=[\"sentinel-2-l2a\"],\n",
    "        bbox=bbox_of_interest,\n",
    "        datetime=time_of_interest,\n",
    "        query={\"eo:cloud_cover\": {\"lt\": 50}},\n",
    "    )\n",
    "\n",
    "    items = search.item_collection()\n",
    "\n",
    "    least_cloudy_item = min(items, key=lambda item: eo.ext(item).cloud_cover)\n",
    "\n",
    "    utm_crs = pyproj.CRS.from_user_input(f'EPSG:326{(int((central_lon + 180) / 6) % 60) + 1}')\n",
    "\n",
    "    # Create a stack from the least cloudy item\n",
    "    if least_cloudy_item:\n",
    "        sentinel_stack = stackstac.stack(\n",
    "            [least_cloudy_item],\n",
    "            bounds_latlon=bbox_of_interest,\n",
    "            epsg=utm_crs.to_epsg(),\n",
    "            resolution=10,\n",
    "            assets=[\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\"],  \n",
    "            fill_value=0\n",
    "        )\n",
    "\n",
    "        print(\"Stack created successfully from the least cloudy item\")\n",
    "    else:\n",
    "        print(\"No least cloudy item found to create a stack\")\n",
    "\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "    # Landsat #############################################\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "\n",
    "    # time_of_interest = \"2021-04-01/2021-08-01\"\n",
    "\n",
    "    # search = catalog.search(\n",
    "    #     collections=[\"landsat-c2-l2\"],\n",
    "    #     bbox=bbox_of_interest,\n",
    "    #     datetime=time_of_interest,\n",
    "    #     query={\"eo:cloud_cover\": {\"lt\": 10}},\n",
    "    # )\n",
    "\n",
    "    # items = search.item_collection()\n",
    "\n",
    "    # selected_item = min(items, key=lambda item: eo.ext(item).cloud_cover)\n",
    "\n",
    "    # utm_crs = pyproj.CRS.from_user_input(f'EPSG:326{(int((central_lon + 180) / 6) % 60) + 1}')\n",
    "\n",
    "    # bands_of_interest = [\"green\", \"nir08\"]\n",
    "    # Landsat_stack = odc.stac.stac_load(\n",
    "    #     [selected_item], bands=bands_of_interest, bbox=bbox_of_interest\n",
    "    # ).isel(time=0)\n",
    "\n",
    "\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "\n",
    "    # items = list(catalog.get_collection(\"hgb\").get_all_items())\n",
    "    # item = items[0]\n",
    "\n",
    "    # da = rioxarray.open_rasterio(\n",
    "    #     item.assets[\"aboveground\"].href, chunks=dict(x=2560, y=2560)\n",
    "    # )\n",
    "\n",
    "    # # Transform our data array to a dataset by selecting the only data variable ('band')\n",
    "    # # renaming it to something useful ('biomass')\n",
    "    # ds = da.to_dataset(dim=\"band\").rename({1: \"biomass\"})\n",
    "    # ds\n",
    "\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "    #######################################################\n",
    "\n",
    "    # Define a list of geographic coordinates for the points of interest\n",
    "    points = wgs_coords\n",
    "\n",
    "    # Initialize the WGS 84 geographic coordinate system (EPSG:4326)\n",
    "    geographic = pyproj.CRS('EPSG:4326')\n",
    "\n",
    "    # Open the TIFF file\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        # Read the raster data\n",
    "        array = src.read()\n",
    "        # Get the dimensions of the array\n",
    "        bands, height, width = array.shape\n",
    "\n",
    "    df_PCL = pd.DataFrame(array.reshape(bands, -1).T, columns=[f'Band_{i+1}' for i in range(bands)])\n",
    "    df_PCL.columns = [\"PCLVALUE\"]\n",
    "\n",
    "    df_list = []\n",
    "    df2_list = []\n",
    "    df3_list = []\n",
    "    hag_values = []\n",
    "\n",
    "    for lon, lat in points:\n",
    "        utm_crs = pyproj.CRS.from_user_input(f'EPSG:326{(int((lon + 180) / 6) % 60) + 1}')\n",
    "        transformer = pyproj.Transformer.from_crs(geographic, utm_crs, always_xy=True)\n",
    "\n",
    "        # Convert geographic coordinates to UTM coordinates\n",
    "        utm_x, utm_y = transformer.transform(lon, lat)\n",
    "\n",
    "\n",
    "        # Extract values across years for the given point LULC\n",
    "        values_across_years_LULC = LULC_stack.sel(x=utm_x, y=utm_y, method='nearest')#.values\n",
    "        df_list.append(values_across_years_LULC)\n",
    "\n",
    "        values_across_sentinalbands = sentinel_stack.sel(x=utm_x, y=utm_y, method='nearest')#.values\n",
    "        df2_list.append(values_across_sentinalbands)\n",
    "\n",
    "        #values_across_landsatbands = Landsat_stack.sel(x=utm_x, y=utm_y, method='nearest')#.values\n",
    "        #df3_list.append(values_across_landsatbands)\n",
    "\n",
    "            # Transform geographic coordinates to pixel coordinates in the HAG raster\n",
    "        x, y = warp_transform(geographic, hag_crs, [lon], [lat])\n",
    "        col, row = ~hag_transform * (x[0], y[0])  # Apply the inverse of the affine transform\n",
    "        col, row = int(col), int(row)  # Convert to integer pixel indices\n",
    "    \n",
    "        # Extract the HAG value if the indices are within the raster dimensions\n",
    "        if (0 <= col < arrayHAG.shape[1]) and (0 <= row < arrayHAG.shape[0]):\n",
    "            hag_value = arrayHAG[row, col]\n",
    "        else:\n",
    "            hag_value = np.nan  # Use NaN for coordinates outside the raster\n",
    "    \n",
    "        hag_values.append(hag_value)\n",
    "\n",
    "\n",
    "\n",
    "    df_COMPUTED = da.compute(*df_list) \n",
    "    df = pd.concat([pd.DataFrame(row).T for row in df_COMPUTED], ignore_index=True)\n",
    "    time_coords = LULC_stack.time.values\n",
    "    time_columns = [f'LULC_{pd.to_datetime(str(time)).strftime(\"%Y\")}' for time in time_coords]\n",
    "    df.columns = time_columns\n",
    "    print(\"LULC\")\n",
    "\n",
    "    df2_COMPUTED = da.compute(*df2_list) \n",
    "    df2 = pd.concat([pd.DataFrame(row) for row in df2_COMPUTED], ignore_index=True)\n",
    "    df2.columns = [\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\"]\n",
    "    print(\"SENTINEL\")\n",
    "\n",
    "    #df3_COMPUTED = da.compute(*df3_list) \n",
    "    #df3 = pd.concat([pd.DataFrame(row) for row in df3_COMPUTED], ignore_index=True)\n",
    "    # df3.columns = [\"green\"]\n",
    "    # print(\"LANDSAT\")\n",
    "\n",
    "    combined_df_pred = pd.concat([df, df2, df_PCL], axis=1)\n",
    "    combined_df_pred\n",
    "\n",
    "    combined_df_pred = pd.concat([df, df2, df_PCL], axis=1)\n",
    "    combined_df_pred\n",
    "    combined_df_pred['HAG'] = hag_values\n",
    "\n",
    "    # USE THE SAME VARIABLES AS MODEL FIT\n",
    "    X_combined = combined_df_pred[['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12', 'HAG']]\n",
    "    \n",
    "    # THis is incorrect \n",
    "    X_combined_scaled = scaler.transform(X_combined)  # Assuming 'scaler' is your previously fitted MinMaxScaler\n",
    "    \n",
    "    combined_predictions = model.predict(X_combined_scaled)\n",
    "    \n",
    "    # Define output GeoTIFF file path\n",
    "    output_tif_path = f\"C:\\\\Users\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\OutputTIF\\\\prediction_tile_{tilenumber}_4.tif\"\n",
    "    \n",
    "    # Get the GeoTIFF metadata from the input GeoTIFF file\n",
    "    with rasterio.open(tif_files[0]) as src:\n",
    "        meta = src.meta.copy()\n",
    "        transform = src.transform\n",
    "    \n",
    "    # Update metadata for the output GeoTIFF\n",
    "    meta.update(\n",
    "        dtype=rasterio.float32,\n",
    "        count=1,  # Number of bands\n",
    "        nodata=None,  # Set nodata value if applicable\n",
    "    )\n",
    "    \n",
    "    predictions_reshaped = combined_predictions.reshape((meta['height'], meta['width']))\n",
    "    \n",
    "    # Confirm that predictions_reshaped now has the correct shape\n",
    "    print(predictions_reshaped.shape)\n",
    "    \n",
    "    # Write the reshaped predictions to the GeoTIFF\n",
    "    with rasterio.open(output_tif_path, 'w', **meta) as dst:\n",
    "        # Ensure that the transform attribute is correctly set\n",
    "        dst.transform = transform\n",
    "        dst.write(predictions_reshaped.astype(rasterio.float32), 1)\n",
    "    \n",
    "    print(f\"Predictions exported to {output_tif_path}\")\n",
    "\n",
    "    \n",
    "    # Determine time\n",
    "    end_time = datetime.datetime.now()\n",
    "    time_diff = end_time - start_time\n",
    "    print(f\"The operation took {time_diff} seconds.\")\n",
    "        \n",
    "    #except:\n",
    "    #    continue\n",
    "print(\"done!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9268a828-e9ca-4830-9179-9d643d398969",
   "metadata": {},
   "source": [
    "# End for now "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
