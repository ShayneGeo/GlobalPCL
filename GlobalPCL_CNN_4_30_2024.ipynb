{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca98617e-0c1b-4605-bfd2-2ca435ed3e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # THIS MAKES A LIST OF LAT LONG\n",
    "# import rasterio\n",
    "# from rasterio.features import geometry_mask\n",
    "# from shapely.geometry import Point, box\n",
    "# from geopandas import GeoDataFrame\n",
    "# import numpy as np\n",
    "\n",
    "# # Load the GeoTIFF file\n",
    "# file_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\Input\\pcl_west_wgs_CO_3.tif\"\n",
    "# with rasterio.open(file_path) as src:\n",
    "#     # Extract the bounds of the raster\n",
    "#     bounds = src.bounds\n",
    "#     crs = src.crs\n",
    "\n",
    "# # Create a box from the bounds\n",
    "# rect = box(bounds.left, bounds.bottom, bounds.right, bounds.top)\n",
    "\n",
    "# # Apply a negative buffer to contract the boundary of the rectangle\n",
    "# buffered_rect = rect.buffer(-0.00001)  # Change 1000 to your desired buffer distance in units of the CRS\n",
    "\n",
    "# # Check if the buffered rectangle is empty (can happen with large negative buffers)\n",
    "# if buffered_rect.is_empty:\n",
    "#     raise ValueError(\"Buffer size is too large\")\n",
    "\n",
    "# # Function to generate random points within a geometry\n",
    "# def generate_random_points(geometry, num_points):\n",
    "#     points = []\n",
    "#     min_x, min_y, max_x, max_y = geometry.bounds\n",
    "#     while len(points) < num_points:\n",
    "#         random_point = Point(np.random.uniform(min_x, max_x), np.random.uniform(min_y, max_y))\n",
    "#         if random_point.within(geometry):\n",
    "#             points.append(random_point)\n",
    "#     return points\n",
    "\n",
    "# # Generate random points within the buffered rectangle\n",
    "# random_points = generate_random_points(buffered_rect, 500)\n",
    "\n",
    "# # Convert these points to a GeoDataFrame\n",
    "# gdf_points = GeoDataFrame(geometry=random_points, crs=crs)\n",
    "\n",
    "# # Convert geometries from the CRS to WGS84 for latitude and longitude\n",
    "# gdf_points_wgs84 = gdf_points.to_crs(epsg=4326)\n",
    "\n",
    "# # Extract the latitude and longitude\n",
    "# lat_long = gdf_points_wgs84.geometry.apply(lambda geom: (geom.y, geom.x)).tolist()\n",
    "\n",
    "# # # Print the latitude and longitude coordinates\n",
    "# # for lat, lon in lat_long:\n",
    "# #     print(f\"Latitude: {lat}, Longitude: {lon}\")\n",
    "# print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df143648-1d22-4542-9b2c-f441041daf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.plot import show\n",
    "from shapely.geometry import box, Point\n",
    "from geopandas import GeoDataFrame\n",
    "import numpy as np\n",
    "\n",
    "# Load the GeoTIFF file\n",
    "file_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\Input\\pcl_west_wgs_CO_3.tif\"\n",
    "with rasterio.open(file_path) as src:\n",
    "    bounds = src.bounds\n",
    "    crs = src.crs\n",
    "    img = src.read(1)  # Read the first band\n",
    "\n",
    "# Create a box from the bounds\n",
    "rect = box(bounds.left, bounds.bottom, bounds.right, bounds.top)\n",
    "\n",
    "# Apply a negative buffer to contract the boundary of the rectangle\n",
    "buffered_rect = rect.buffer(-0.15)  # Adjust buffer size as needed\n",
    "\n",
    "# Generate random points within the buffered rectangle\n",
    "def generate_random_points(geometry, num_points):\n",
    "    points = []\n",
    "    min_x, min_y, max_x, max_y = geometry.bounds\n",
    "    while len(points) < num_points:\n",
    "        random_point = Point(np.random.uniform(min_x, max_x), np.random.uniform(min_y, max_y))\n",
    "        if random_point.within(geometry):\n",
    "            points.append(random_point)\n",
    "    return points\n",
    "\n",
    "random_points = generate_random_points(buffered_rect, 20000)\n",
    "\n",
    "# Convert these points to a GeoDataFrame\n",
    "gdf_points = GeoDataFrame(geometry=random_points, crs=crs)\n",
    "\n",
    "# Convert points GeoDataFrame to the same CRS as the raster for accurate overlay\n",
    "gdf_points = gdf_points.to_crs(crs)\n",
    "\n",
    "# Convert geometries from the CRS to WGS84 for latitude and longitude\n",
    "gdf_points_wgs84 = gdf_points.to_crs(epsg=4326)\n",
    "\n",
    "# Extract the latitude and longitude\n",
    "lat_long = gdf_points_wgs84.geometry.apply(lambda geom: (geom.y, geom.x)).tolist()\n",
    "\n",
    "# Plotting the raster and the points\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "show(img, ax=ax, transform=src.transform, cmap='gray')  # Show the raster\n",
    "gdf_points.plot(ax=ax, color='red', markersize=10)  # Plot the points\n",
    "plt.title('Random Points on Raster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585859ee-6bec-4110-87f1-fb3d0ae311df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "\n",
    "# Paths to the raster file\n",
    "resampled_PCL_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\Input\\pcl_west_wgs_CO_3.tif\"\n",
    "resampled_lidar_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\CNNPCLDEMO\\Input\\Resampled_LIDAR_GEDI_CO_11.tif\"\n",
    "resampled_dem_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\CNNPCLDEMO\\Input\\Resampled_DEM_CO.tif\"\n",
    "resampled_rivers_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\CNNPCLDEMO\\Input\\Resampled_RIVERS_CO.tif\"\n",
    "resampled_roads_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\CNNPCLDEMO\\Input\\Resampled_ROADS_CO.tif\"\n",
    "\n",
    "# Size of the chip around the point (in pixels)\n",
    "chip_size = 64  \n",
    "\n",
    "for i in range(len(random_points)):\n",
    "    try:\n",
    "        # Define the coordinates of the point (replace these with your actual coordinates)\n",
    "        x_coord = lat_long[i][1] \n",
    "        y_coord = lat_long[i][0]\n",
    "        \n",
    "        # Open the resampled LIDAR raster\n",
    "        with rasterio.open(resampled_PCL_path) as pcl:\n",
    "            # Convert the geographic coordinates to pixel coordinates\n",
    "            col, row = pcl.index(x_coord, y_coord)\n",
    "            \n",
    "            # Calculate the window\n",
    "            window = Window(col - chip_size, row - chip_size, 2 * chip_size, 2 * chip_size)\n",
    "            \n",
    "            # Read the data within the window\n",
    "            chip_data = pcl.read(window=window)\n",
    "            \n",
    "            # Prepare metadata for the chip raster\n",
    "            out_meta = pcl.meta.copy()\n",
    "            out_meta.update({\n",
    "                \"driver\": \"GTiff\",\n",
    "                \"height\": chip_data.shape[1],\n",
    "                \"width\": chip_data.shape[2],\n",
    "                \"transform\": pcl.window_transform(window)\n",
    "            })\n",
    "        \n",
    "            # Save the chip data to a new file\n",
    "            chip_output_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\labels4\\\\PCL_Chip_{i}.tif\"\n",
    "            #with rasterio.open(chip_output_path, \"w\", **out_meta) as dest:\n",
    "            #    dest.write(chip_data)\n",
    "            #print(chip_data.shape)\n",
    "            try:\n",
    "                if chip_data.shape == (1,2 * chip_size,2 * chip_size) and not np.allclose(chip_data, 0):\n",
    "                    with rasterio.open(chip_output_path, \"w\", **out_meta) as dest:\n",
    "                        dest.write(chip_data)\n",
    "                else:\n",
    "                    print(f\"Skipping chip {i} because its shape is not 200x200.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred while saving chip {i}: {e}\")\n",
    "        \n",
    "        #print(\"Image chip created and saved to:\", chip_output_path)\n",
    "        \n",
    "        with rasterio.open(resampled_lidar_path) as lidar:\n",
    "            # Convert the geographic coordinates to pixel coordinates\n",
    "            col, row = lidar.index(x_coord, y_coord)\n",
    "            \n",
    "            # Calculate the window\n",
    "            window = Window(col - chip_size, row - chip_size, 2 * chip_size, 2 * chip_size)\n",
    "            \n",
    "            # Read the data within the window\n",
    "            chip_data = lidar.read(window=window)\n",
    "            \n",
    "            # Prepare metadata for the chip raster\n",
    "            out_meta = lidar.meta.copy()\n",
    "            out_meta.update({\n",
    "                \"driver\": \"GTiff\",\n",
    "                \"height\": chip_data.shape[1],\n",
    "                \"width\": chip_data.shape[2],\n",
    "                \"transform\": lidar.window_transform(window)\n",
    "            })\n",
    "        \n",
    "            # Save the chip data to a new file\n",
    "            chip_output_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inputfeatures4\\\\hag\\\\LIDAR_Chip_{i}.tif\"\n",
    "            #with rasterio.open(chip_output_path, \"w\", **out_meta) as dest:\n",
    "            #    dest.write(chip_data)\n",
    "            try:\n",
    "                if chip_data.shape == (1,2 * chip_size,2 * chip_size) and not np.isnan(chip_data).all():\n",
    "                    with rasterio.open(chip_output_path, \"w\", **out_meta) as dest:\n",
    "                        dest.write(chip_data)\n",
    "                else:\n",
    "                    print(f\"Skipping chip {i} because its shape is not 200x200.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred while saving chip {i}: {e}\")\n",
    "\n",
    "        with rasterio.open(resampled_dem_path) as dem:\n",
    "            # Convert the geographic coordinates to pixel coordinates\n",
    "            col, row = dem.index(x_coord, y_coord)\n",
    "            \n",
    "            # Calculate the window\n",
    "            window = Window(col - chip_size, row - chip_size, 2 * chip_size, 2 * chip_size)\n",
    "            \n",
    "            # Read the data within the window\n",
    "            chip_data = dem.read(window=window)\n",
    "            \n",
    "            # Prepare metadata for the chip raster\n",
    "            out_meta = dem.meta.copy()\n",
    "            out_meta.update({\n",
    "                \"driver\": \"GTiff\",\n",
    "                \"height\": chip_data.shape[1],\n",
    "                \"width\": chip_data.shape[2],\n",
    "                \"transform\": dem.window_transform(window)\n",
    "            })\n",
    "        \n",
    "            # Save the chip data to a new file\n",
    "            chip_output_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inputfeatures4\\\\dem\\\\DEM_Chip_{i}.tif\"\n",
    "            #with rasterio.open(chip_output_path, \"w\", **out_meta) as dest:\n",
    "            #    dest.write(chip_data)\n",
    "            try:\n",
    "                if chip_data.shape == (1,2 * chip_size,2 * chip_size) and not np.isnan(chip_data).all():\n",
    "                    with rasterio.open(chip_output_path, \"w\", **out_meta) as dest:\n",
    "                        dest.write(chip_data)\n",
    "                else:\n",
    "                    print(f\"Skipping chip {i} because its shape is not 200x200.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred while saving chip {i}: {e}\")\n",
    "\n",
    "        with rasterio.open(resampled_rivers_path) as rivers:\n",
    "            # Convert the geographic coordinates to pixel coordinates\n",
    "            col, row = rivers.index(x_coord, y_coord)\n",
    "            \n",
    "            # Calculate the window\n",
    "            window = Window(col - chip_size, row - chip_size, 2 * chip_size, 2 * chip_size)\n",
    "            \n",
    "            # Read the data within the window\n",
    "            chip_data = rivers.read(window=window)\n",
    "            \n",
    "            # Prepare metadata for the chip raster\n",
    "            out_meta = rivers.meta.copy()\n",
    "            out_meta.update({\n",
    "                \"driver\": \"GTiff\",\n",
    "                \"height\": chip_data.shape[1],\n",
    "                \"width\": chip_data.shape[2],\n",
    "                \"transform\": rivers.window_transform(window)\n",
    "            })\n",
    "        \n",
    "            # Save the chip data to a new file\n",
    "            chip_output_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inputfeatures4\\\\rivers\\\\River_Chip_{i}.tif\"\n",
    "\n",
    "            try:\n",
    "                if chip_data.shape == (1,2 * chip_size,2 * chip_size) and not np.isnan(chip_data).all():\n",
    "                    with rasterio.open(chip_output_path, \"w\", **out_meta) as dest:\n",
    "                        dest.write(chip_data)\n",
    "                else:\n",
    "                    print(f\"Skipping chip {i} because its shape is not 200x200.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred while saving chip {i}: {e}\")\n",
    "\n",
    "        with rasterio.open(resampled_rivers_path) as roads:\n",
    "            # Convert the geographic coordinates to pixel coordinates\n",
    "            col, row = roads.index(x_coord, y_coord)\n",
    "            \n",
    "            # Calculate the window\n",
    "            window = Window(col - chip_size, row - chip_size, 2 * chip_size, 2 * chip_size)\n",
    "            \n",
    "            # Read the data within the window\n",
    "            chip_data = roads.read(window=window)\n",
    "            \n",
    "            # Prepare metadata for the chip raster\n",
    "            out_meta = roads.meta.copy()\n",
    "            out_meta.update({\n",
    "                \"driver\": \"GTiff\",\n",
    "                \"height\": chip_data.shape[1],\n",
    "                \"width\": chip_data.shape[2],\n",
    "                \"transform\": roads.window_transform(window)\n",
    "            })\n",
    "        \n",
    "            # Save the chip data to a new file\n",
    "            chip_output_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inputfeatures4\\\\roads\\\\Roads_Chip_{i}.tif\"\n",
    "\n",
    "            try:\n",
    "                if chip_data.shape == (1,2 * chip_size,2 * chip_size) and not np.isnan(chip_data).all():\n",
    "                    with rasterio.open(chip_output_path, \"w\", **out_meta) as dest:\n",
    "                        dest.write(chip_data)\n",
    "                else:\n",
    "                    print(f\"Skipping chip {i} because its shape is not 200x200.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred while saving chip {i}: {e}\")\n",
    "\n",
    "        \n",
    "        print(\"Image chip created and saved to:\", chip_output_path)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64649bc4-a076-4689-8e91-56519a6043e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import rasterio\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# # Paths to datasets\n",
    "# featurepath1 = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\CNNPCLDEMO\\inputfeatures4\\hag\"\n",
    "# featurepath2 = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\CNNPCLDEMO\\inputfeatures4\\dem\"\n",
    "# featurepath2 = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\CNNPCLDEMO\\inputfeatures4\\roads\"\n",
    "# featurepath2 = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\CNNPCLDEMO\\inputfeatures4\\rivers\"\n",
    "\n",
    "# labelspath = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\CNNPCLDEMO\\labels4\"\n",
    "\n",
    "# # Function to load GeoTIFF images as numpy arrays\n",
    "# def load_geotiff(path):\n",
    "#     with rasterio.open(path) as src:\n",
    "#         return src.read(1)\n",
    "# print(\"1\")\n",
    "# # Load datasets\n",
    "# hag_images = [load_geotiff(os.path.join(featurepath1, f)) for f in os.listdir(featurepath1) if f.endswith('.tif')]\n",
    "# dem_images = [load_geotiff(os.path.join(featurepath2, f)) for f in os.listdir(featurepath2) if f.endswith('.tif')]\n",
    "# label_images = [load_geotiff(os.path.join(labelspath, f)) for f in os.listdir(labelspath) if f.endswith('.tif')]\n",
    "# print(\"2\")\n",
    "\n",
    "# # Convert lists to numpy arrays and stack features along the last dimension\n",
    "# hag_images = np.array(hag_images).astype('float32')\n",
    "# dem_images = np.array(dem_images).astype('float32')\n",
    "# feature_images = np.stack((hag_images, dem_images), axis=-1)  # Stack along the channel axis\n",
    "\n",
    "# label_images = np.array(label_images).astype('float32')\n",
    "# print(\"3\")\n",
    "\n",
    "# # Normalize feature images to range [0, 1] and label images to a specific range\n",
    "# feature_max = np.max(feature_images, axis=(1, 2, 3), keepdims=True)\n",
    "# feature_images /= feature_max\n",
    "# label_images /= 100  # Normalize labels if they range from 0 to 100\n",
    "\n",
    "# # Reshape labels for CNN input\n",
    "# label_images = np.expand_dims(label_images, axis=-1)\n",
    "# print(\"4\")\n",
    "\n",
    "# # Define the CNN model\n",
    "# model = Sequential([\n",
    "#     Conv2D(16, (3, 3), activation='relu', input_shape=(128, 128, 4)),  # Adjust input shape for two channels\n",
    "#     MaxPooling2D((2, 2)),\n",
    "#     Dropout(0.25),\n",
    "#     Conv2D(32, (3, 3), activation='relu'),\n",
    "#     MaxPooling2D((2, 2)),\n",
    "#     Dropout(0.25),\n",
    "#     Conv2D(64, (3, 3), activation='relu'),\n",
    "#     Flatten(),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(128 * 128, activation='sigmoid'),\n",
    "#     tf.keras.layers.Reshape((128, 128, 1))\n",
    "# ])\n",
    "\n",
    "# # Compile and train the model\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "# model.fit(feature_images, label_images, batch_size=64, epochs=1, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6259585c-2c33-4f30-93e2-356b815a1508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Paths to datasets\n",
    "featurepath1 = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\CNNPCLDEMO\\inputfeatures4\\hag\"\n",
    "featurepath2 = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\CNNPCLDEMO\\inputfeatures4\\dem\"\n",
    "featurepath3 = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\CNNPCLDEMO\\inputfeatures4\\roads\"\n",
    "featurepath4 = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\CNNPCLDEMO\\inputfeatures4\\rivers\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "labelspath = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\CNNPCLDEMO\\labels4\"\n",
    "\n",
    "# Function to load GeoTIFF images as numpy arrays\n",
    "def load_geotiff(path):\n",
    "    with rasterio.open(path) as src:\n",
    "        return src.read(1)\n",
    "\n",
    "# Load datasets\n",
    "hag_images = [load_geotiff(os.path.join(featurepath1, f)) for f in os.listdir(featurepath1) if f.endswith('.tif')]\n",
    "dem_images = [load_geotiff(os.path.join(featurepath2, f)) for f in os.listdir(featurepath2) if f.endswith('.tif')]\n",
    "roads_images = [load_geotiff(os.path.join(featurepath3, f)) for f in os.listdir(featurepath3) if f.endswith('.tif')]\n",
    "rivers_images = [load_geotiff(os.path.join(featurepath4, f)) for f in os.listdir(featurepath4) if f.endswith('.tif')]\n",
    "\n",
    "label_images = [load_geotiff(os.path.join(labelspath, f)) for f in os.listdir(labelspath) if f.endswith('.tif')]\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "hag_images = np.array(hag_images).astype('float32')\n",
    "dem_images = np.array(dem_images).astype('float32')\n",
    "roads_images = np.array(roads_images).astype('float32')\n",
    "rivers_images = np.array(rivers_images).astype('float32')\n",
    "\n",
    "label_images = np.array(label_images).astype('float32')\n",
    "\n",
    "# Normalize images independently\n",
    "hag_max = hag_images.max()\n",
    "dem_max = dem_images.max()\n",
    "roads_max = roads_images.max()\n",
    "rivers_max = rivers_images.max()\n",
    "\n",
    "\n",
    "hag_images /= hag_max\n",
    "dem_images /= dem_max\n",
    "roads_images /= roads_max\n",
    "rivers_images /= rivers_max\n",
    "\n",
    "\n",
    "# Stack features along the last dimension\n",
    "feature_images = np.stack((hag_images, dem_images, roads_images, rivers_images), axis=-1)\n",
    "\n",
    "# Normalize labels if they range from 0 to 100\n",
    "label_images /= 100\n",
    "\n",
    "# Reshape labels for CNN input\n",
    "label_images = np.expand_dims(label_images, axis=-1)\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(16, (3, 3), activation='relu', input_shape=(128, 128, 4)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(128 * 128, activation='sigmoid'),\n",
    "    tf.keras.layers.Reshape((128, 128, 1))\n",
    "])\n",
    "\n",
    "lr = 0.01\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "model.fit(feature_images, label_images, batch_size=128, epochs=10, validation_split=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a929a5d9-9d8d-4db2-8bba-53efcaeda100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\SavedModels\\\\model_4_30_LR01.h5')  # Saves the model in HDF5 format\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Path to the model\n",
    "model_path = 'C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\SavedModels\\\\model_4_30_LR01.h5'\n",
    "\n",
    "# Load the model\n",
    "model = load_model(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bcebcb-c941-4739-892a-335eac91dbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this ensures all tiles are widthtile by heighttile\n",
    "import os\n",
    "from itertools import product\n",
    "import rasterio as rio\n",
    "from rasterio import windows\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "in_path = 'C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\Input\\\\'\n",
    "input_filename1 = 'Resampled_LIDAR_GEDI_CO_11.tif'\n",
    "input_filename2 = 'Resampled_DEM_CO.tif'\n",
    "input_filename3 = 'Resampled_ROADS_CO.tif'\n",
    "input_filename4 = 'Resampled_RIVERS_CO.tif'\n",
    "\n",
    "\n",
    "\n",
    "out_path = 'C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\hag\\\\'\n",
    "output_filename = 'tile_{}-{}.tif'\n",
    "widthtile = 128\n",
    "heighttile = 128\n",
    "\n",
    "\n",
    "def get_tiles(ds, width=widthtile, height=heighttile):\n",
    "    nols, nrows = ds.meta['width'], ds.meta['height']\n",
    "    offsets = product(range(0, nols, width), range(0, nrows, height))\n",
    "    #offsets = product(range(0, nols, 1000), range(0, nrows, 1000))\n",
    "\n",
    "    big_window = windows.Window(col_off=0, row_off=0, width=nols, height=nrows)\n",
    "    for col_off, row_off in offsets:\n",
    "        window = windows.Window(col_off=col_off, row_off=row_off, width=width, height=height).intersection(big_window)\n",
    "        transform = windows.transform(window, ds.transform)\n",
    "        yield window, transform\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with rio.open(os.path.join(in_path, input_filename1)) as inds:\n",
    "    tile_width, tile_height = widthtile, heighttile\n",
    "    nodata = inds.nodata  # Get the NoData value from the dataset\n",
    "    meta = inds.meta.copy()\n",
    "\n",
    "    for window, transform in get_tiles(inds):\n",
    "        if window.width == tile_width and window.height == tile_height:  # Check if the tile dimensions are as expected\n",
    "            data = inds.read(window=window)\n",
    "            if nodata is not None:\n",
    "                # Modified check for NoData to include tolerance for floating-point rasters\n",
    "                valid_data_mask = (data != nodata)\n",
    "            else:\n",
    "                # If NoData value is not set, consider all data as valid\n",
    "                valid_data_mask = (data == data)\n",
    "\n",
    "            if valid_data_mask.any():  # Check if there's any valid data within the tile\n",
    "                meta['transform'] = transform\n",
    "                meta['width'], meta['height'] = window.width, window.height\n",
    "                outpath = os.path.join(out_path, output_filename.format(int(window.col_off), int(window.row_off)))\n",
    "                with rio.open(outpath, 'w', **meta) as outds:\n",
    "                    outds.write(data)\n",
    "\n",
    "\n",
    "\n",
    "out_path = 'C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\dem\\\\'\n",
    "#output_filename = 'tile_{}-{}.tif'\n",
    "#widthtile = 128\n",
    "#heighttile = 128\n",
    "\n",
    "with rio.open(os.path.join(in_path, input_filename2)) as inds:\n",
    "    tile_width, tile_height = widthtile, heighttile\n",
    "    nodata = inds.nodata  # Get the NoData value from the dataset\n",
    "    meta = inds.meta.copy()\n",
    "\n",
    "    for window, transform in get_tiles(inds):\n",
    "        if window.width == tile_width and window.height == tile_height:  # Check if the tile dimensions are as expected\n",
    "            data = inds.read(window=window)\n",
    "            if nodata is not None:\n",
    "                # Modified check for NoData to include tolerance for floating-point rasters\n",
    "                valid_data_mask = (data != nodata)\n",
    "            else:\n",
    "                # If NoData value is not set, consider all data as valid\n",
    "                valid_data_mask = (data == data)\n",
    "\n",
    "            if valid_data_mask.any():  # Check if there's any valid data within the tile\n",
    "                meta['transform'] = transform\n",
    "                meta['width'], meta['height'] = window.width, window.height\n",
    "                outpath = os.path.join(out_path, output_filename.format(int(window.col_off), int(window.row_off)))\n",
    "                with rio.open(outpath, 'w', **meta) as outds:\n",
    "                    outds.write(data)\n",
    "\n",
    "out_path = 'C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\roads\\\\'\n",
    "#output_filename = 'tile_{}-{}.tif'\n",
    "#widthtile = 128\n",
    "#heighttile = 128\n",
    "\n",
    "with rio.open(os.path.join(in_path, input_filename3)) as inds:\n",
    "    tile_width, tile_height = widthtile, heighttile\n",
    "    nodata = inds.nodata  # Get the NoData value from the dataset\n",
    "    meta = inds.meta.copy()\n",
    "\n",
    "    for window, transform in get_tiles(inds):\n",
    "        if window.width == tile_width and window.height == tile_height:  # Check if the tile dimensions are as expected\n",
    "            data = inds.read(window=window)\n",
    "            if nodata is not None:\n",
    "                # Modified check for NoData to include tolerance for floating-point rasters\n",
    "                valid_data_mask = (data != nodata)\n",
    "            else:\n",
    "                # If NoData value is not set, consider all data as valid\n",
    "                valid_data_mask = (data == data)\n",
    "\n",
    "            if valid_data_mask.any():  # Check if there's any valid data within the tile\n",
    "                meta['transform'] = transform\n",
    "                meta['width'], meta['height'] = window.width, window.height\n",
    "                outpath = os.path.join(out_path, output_filename.format(int(window.col_off), int(window.row_off)))\n",
    "                with rio.open(outpath, 'w', **meta) as outds:\n",
    "                    outds.write(data)\n",
    "\n",
    "out_path = 'C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\rivers\\\\'\n",
    "#output_filename = 'tile_{}-{}.tif'\n",
    "#widthtile = 128\n",
    "#heighttile = 128\n",
    "\n",
    "with rio.open(os.path.join(in_path, input_filename4)) as inds:\n",
    "    tile_width, tile_height = widthtile, heighttile\n",
    "    nodata = inds.nodata  # Get the NoData value from the dataset\n",
    "    meta = inds.meta.copy()\n",
    "\n",
    "    for window, transform in get_tiles(inds):\n",
    "        if window.width == tile_width and window.height == tile_height:  # Check if the tile dimensions are as expected\n",
    "            data = inds.read(window=window)\n",
    "            if nodata is not None:\n",
    "                # Modified check for NoData to include tolerance for floating-point rasters\n",
    "                valid_data_mask = (data != nodata)\n",
    "            else:\n",
    "                # If NoData value is not set, consider all data as valid\n",
    "                valid_data_mask = (data == data)\n",
    "\n",
    "            if valid_data_mask.any():  # Check if there's any valid data within the tile\n",
    "                meta['transform'] = transform\n",
    "                meta['width'], meta['height'] = window.width, window.height\n",
    "                outpath = os.path.join(out_path, output_filename.format(int(window.col_off), int(window.row_off)))\n",
    "                with rio.open(outpath, 'w', **meta) as outds:\n",
    "                    outds.write(data)\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc222197-c06f-4509-a5f9-5b7fbbde8391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import rasterio\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "\n",
    "# tilename = '0-0'\n",
    "# input_image_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\HAGTiles3\\\\tile_{tilename}.tif\"\n",
    "\n",
    "# # Ensure 'feature_max' is correctly defined (use a known value or calculate it)\n",
    "# #feature_max = 100  # Example value, adjust based on actual maximum height from your data\n",
    "\n",
    "# def load_and_preprocess_image(path):\n",
    "#     with rasterio.open(path) as src:\n",
    "#         image = src.read(1)\n",
    "#     print(\"Max value in image before normalization:\", np.max(image))  # Debug print\n",
    "#     image = np.array(image).astype('float32') / feature_max\n",
    "#     image = np.expand_dims(image, axis=-1)\n",
    "#     image = np.expand_dims(image, axis=0)\n",
    "#     return image\n",
    "\n",
    "# input_image = load_and_preprocess_image(input_image_path)\n",
    "# predicted_image = model.predict(input_image)\n",
    "# predicted_image = np.squeeze(predicted_image)\n",
    "\n",
    "# # Debug print to check if all outputs are the same\n",
    "# print(\"Unique values in predicted output:\", np.unique(predicted_image))\n",
    "\n",
    "# predicted_image *= 100\n",
    "\n",
    "# output_image_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\testexample\\\\predictions\\\\predicted_tile_{tilename}.tif\"\n",
    "# with rasterio.open(input_image_path) as src:\n",
    "#     profile = src.profile\n",
    "\n",
    "# with rasterio.open(output_image_path, 'w', **profile) as dst:\n",
    "#     dst.write(predicted_image.astype(rasterio.uint8), 1)\n",
    "\n",
    "# print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72b5713-d913-4986-9b6d-1cca7ea9b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Define the directory path\n",
    "directory_path = 'C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\hag'\n",
    "\n",
    "# Regular expression to extract the identifier part of the filename 'tile_{identifier}.tif'\n",
    "pattern = re.compile(r'tile_(\\d+-\\d+)\\.tif')\n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(directory_path)\n",
    "\n",
    "# Use a set to avoid duplicate identifiers\n",
    "identifiers = set()\n",
    "\n",
    "# Extract identifiers from filenames\n",
    "for file in files:\n",
    "    match = pattern.search(file)\n",
    "    if match:\n",
    "        identifiers.add(match.group(1))\n",
    "\n",
    "# Convert the set to a sorted list\n",
    "identifier_list = sorted(list(identifiers))\n",
    "\n",
    "#identifier_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ae4fc6-2e28-402c-85a7-c1cf8d51336e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92e8bb7-33f5-447e-9979-031d52c08bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#tilename = '0-0'\n",
    "# input_hag_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\hag\\\\tile_{tilename}.tif\"\n",
    "# input_dem_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\dem\\\\tile_{tilename}.tif\"\n",
    "# input_roads_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\roads\\\\tile_{tilename}.tif\"\n",
    "# input_rivers_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\rivers\\\\tile_{tilename}.tif\"\n",
    "\n",
    "def load_and_preprocess_image(hag_path, dem_path, roads_path, rivers_path):\n",
    "    with rasterio.open(hag_path) as src:\n",
    "        hag_image = src.read(1)\n",
    "    with rasterio.open(dem_path) as src:\n",
    "        dem_image = src.read(1)\n",
    "    with rasterio.open(roads_path) as src:\n",
    "        roads_image = src.read(1)\n",
    "    with rasterio.open(rivers_path) as src:\n",
    "        rivers_image = src.read(1)\n",
    "\n",
    "    # Normalize and stack the images\n",
    "    hag_image = np.array(hag_image).astype('float32') / hag_max\n",
    "    dem_image = np.array(dem_image).astype('float32') / dem_max\n",
    "    roads_image = np.array(roads_image).astype('float32') / roads_max\n",
    "    rivers_image = np.array(rivers_image).astype('float32') / rivers_max\n",
    "\n",
    "    # Stack images along the last dimension\n",
    "    combined_image = np.stack([hag_image, dem_image, roads_image, rivers_image], axis=-1)\n",
    "\n",
    "    # Add batch dimension\n",
    "    combined_image = np.expand_dims(combined_image, axis=0)\n",
    "    return combined_image\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    tilename = identifier_list[i]\n",
    "    print(tilename)\n",
    "    input_hag_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\hag\\\\tile_{tilename}.tif\"\n",
    "    input_dem_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\dem\\\\tile_{tilename}.tif\"\n",
    "    input_roads_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\roads\\\\tile_{tilename}.tif\"\n",
    "    input_rivers_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\rivers\\\\tile_{tilename}.tif\"\n",
    "\n",
    "\n",
    "    input_image = load_and_preprocess_image(input_hag_path, input_dem_path, input_roads_path, input_rivers_path)\n",
    "    predicted_image = model.predict(input_image)\n",
    "    predicted_image = np.squeeze(predicted_image)\n",
    "    \n",
    "    # Debug print to check if all outputs are the same\n",
    "    print(\"Unique values in predicted output:\", np.unique(predicted_image))\n",
    "    \n",
    "    # Adjust the scaling factor based on how the labels were scaled during training\n",
    "    predicted_image *= 100\n",
    "    \n",
    "    output_image_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\testexample\\\\predictions\\\\predicted_tile_{tilename}.tif\"\n",
    "    \n",
    "    with rasterio.open(input_dem_path) as src: \n",
    "        profile = src.profile\n",
    "    \n",
    "    with rasterio.open(output_image_path, 'w', **profile) as dst:\n",
    "        dst.write(predicted_image.astype(rasterio.uint8), 1)\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03e521e-991f-4be7-975d-92c5e4fe9385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import rasterio\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "\n",
    "# tilename = '0-0'\n",
    "# input_dem_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\dem\\\\tile_{tilename}.tif\"\n",
    "# input_hag_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\hag\\\\tile_{tilename}.tif\"\n",
    "\n",
    "# def load_and_preprocess_image(dem_path, hag_path):\n",
    "#     with rasterio.open(dem_path) as src:\n",
    "#         dem_image = src.read(1)\n",
    "#     with rasterio.open(hag_path) as src:\n",
    "#         hag_image = src.read(1)\n",
    "    \n",
    "#     print(\"Max value in DEM image before normalization:\", np.max(dem_image))\n",
    "#     print(\"Max value in HAG image before normalization:\", np.max(hag_image))\n",
    "\n",
    "#     # Normalize each image independently\n",
    "#     dem_image = np.array(dem_image).astype('float32')\n",
    "#     hag_image = np.array(hag_image).astype('float32')\n",
    "\n",
    "#     if np.max(dem_image) > 0:  # Prevent division by zero\n",
    "#         dem_image /= np.max(dem_image)\n",
    "#     if np.max(hag_image) > 0:  # Prevent division by zero\n",
    "#         hag_image /= np.max(hag_image)\n",
    "\n",
    "#     # Stack images along the last dimension\n",
    "#     combined_image = np.stack([dem_image, hag_image], axis=-1)\n",
    "\n",
    "#     # Add batch dimension\n",
    "#     combined_image = np.expand_dims(combined_image, axis=0)\n",
    "#     return combined_image\n",
    "\n",
    "# input_image = load_and_preprocess_image(input_dem_path, input_hag_path)\n",
    "# predicted_image = model.predict(input_image)\n",
    "# predicted_image = np.squeeze(predicted_image)\n",
    "\n",
    "# # Debug print to check if all outputs are the same\n",
    "# print(\"Unique values in predicted output:\", np.unique(predicted_image))\n",
    "\n",
    "# # Adjust the scaling factor based on how the labels were scaled during training\n",
    "# predicted_image *= 100  # Adjust this factor according to your specific label scaling\n",
    "\n",
    "# output_image_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\testexample\\\\predictions\\\\predicted_tile_{tilename}2.tif\"\n",
    "# with rasterio.open(input_dem_path) as src:  # You can use either DEM or HAG profile, assuming they're identical\n",
    "#     profile = src.profile\n",
    "\n",
    "# with rasterio.open(output_image_path, 'w', **profile) as dst:\n",
    "#     dst.write(predicted_image.astype(rasterio.uint8), 1)\n",
    "\n",
    "# print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9452ebf-f635-4b3a-808f-a059dac43a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1fe9f2-cf33-4ea2-89c3-3fa5c3dc53ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### WORKS WITH NO BOARDER!\n",
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "\n",
    "#tif_dir = r\"D:\\GlobalPCL\\lidarHAG\"\n",
    "#output_tif = r\"D:\\GlobalPCL\\lidarHAG\\LIDAR_GEDI_CO.tif\"\n",
    "tif_dir = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\testexample\\predictions\"\n",
    "output_tif  = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\testexample\\merged\\predMerged_CO_500.tif\"\n",
    "\n",
    "\n",
    "tifs = glob.glob(os.path.join(tif_dir, \"*.tif\"))\n",
    "tifs = tifs[:300]\n",
    "\n",
    "# Prepare the gdal_merge command for HAG\n",
    "merge_command_hag = [\n",
    "    \"python\",\n",
    "    \"C:\\\\Users\\\\smdur\\\\anaconda3\\\\envs\\\\globalpcl\\\\Scripts\\\\gdal_merge.py\",\n",
    "    \"--config\", \"CHECK_DISK_FREE_SPACE\", \"FALSE\",\n",
    "    #\"-ot\", \"Byte\",\n",
    "    \"-o\", output_tif,\n",
    "    \"-n\", \"-9999\",\n",
    "    \"-a_nodata\",\"-9999\",\n",
    "    \n",
    "    \n",
    "] + tifs\n",
    "\n",
    "# Run the gdal_merge command for HAG and capture the output\n",
    "process_hag = subprocess.run(merge_command_hag, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "# Check if the command for HAG was successful\n",
    "if process_hag.returncode != 0:\n",
    "    # An error occurred, print the error\n",
    "    print(\"Error occurred while merging TIFF files HAG:\")\n",
    "    print(process_hag.stderr)\n",
    "else:\n",
    "    print(\"TIFF files merged successfully for HAG.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a38404-c71a-4ef5-8869-86035e2a7df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import glob\n",
    "# import subprocess\n",
    "\n",
    "# tif_dir = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\testexample\\predictions\"\n",
    "# output_tif  = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\testexample\\merged\\predMerged_CO.tif\"\n",
    "\n",
    "# # Get a list of TIFF files\n",
    "# tifs = glob.glob(os.path.join(tif_dir, \"*.tif\"))\n",
    "\n",
    "# # Write the list of TIFF files to a text file\n",
    "# tif_list_file = os.path.join(tif_dir, \"tif_list.txt\")\n",
    "# with open(tif_list_file, \"w\") as file:\n",
    "#     file.write(\"\\n\".join(tifs))\n",
    "\n",
    "# # Prepare the gdal_merge command with the text file\n",
    "# merge_command_hag = [\n",
    "#     \"python\",\n",
    "#     \"C:\\\\Users\\\\smdur\\\\anaconda3\\\\envs\\\\globalpcl\\\\Scripts\\\\gdal_merge.py\",\n",
    "#     \"--config\", \"CHECK_DISK_FREE_SPACE\", \"FALSE\",\n",
    "#     #\"-ot\", \"Byte\",\n",
    "#     \"-o\", output_tif,\n",
    "#     \"-n\", \"255\",\n",
    "#     \"-a_nodata\", \"255\",\n",
    "#     \"-of\", \"GTiff\",  # Specify output format\n",
    "#     \"-l\", f\"@{tif_list_file}\",  # Use the text file as input\n",
    "# ]\n",
    "\n",
    "# # Run the gdal_merge command and capture the output\n",
    "# process_hag = subprocess.run(merge_command_hag, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "# # Check if the command was successful\n",
    "# if process_hag.returncode != 0:\n",
    "#     # An error occurred, print the error\n",
    "#     print(\"Error occurred while merging TIFF files:\")\n",
    "#     print(process_hag.stderr)\n",
    "# else:\n",
    "#     print(\"TIFF files merged successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9558408-d5dc-4b67-ad5a-f89a6f24d3d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
