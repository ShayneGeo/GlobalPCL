{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5a5eba6-e53b-4287-9ee4-4849f36cbe1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2024-05-16 12:54:53.036834\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "print(f\"Start time: {start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869368c6-bbe2-42d7-9ca0-ffd43169ca77",
   "metadata": {},
   "source": [
    "# The folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72d6fe6b-66ea-4ac4-9058-c4d439d6660a",
   "metadata": {},
   "outputs": [],
   "source": [
    "THEFOLDER = \"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GlobalPCL17\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b169db-f902-48ed-a941-4c82019dbba9",
   "metadata": {},
   "source": [
    "# Tile western Conus PCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa307769-d9b4-43c3-bc83-2ab77132858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import product\n",
    "import rasterio as rio\n",
    "from rasterio import windows\n",
    "\n",
    "in_path = \"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\PCLCONUS\\\\Input\\\\PCL\\\\\"\n",
    "input_filename = 'pcl_west_wgs.tif'\n",
    "\n",
    "out_path = f\"{THEFOLDER}\\\\PCLTILES\\\\\"\n",
    "output_filename = 'pcltile_{}-{}.tif'\n",
    "\n",
    "widthtile = 5000\n",
    "heighttile = 5000\n",
    "\n",
    "def get_tiles(ds, width=widthtile, height=heighttile):\n",
    "    nols, nrows = ds.meta['width'], ds.meta['height']\n",
    "    offsets = product(range(0, nols, width), range(0, nrows, height))\n",
    "    big_window = windows.Window(col_off=0, row_off=0, width=nols, height=nrows)\n",
    "    for col_off, row_off in offsets:\n",
    "        window = windows.Window(col_off=col_off, row_off=row_off, width=width, height=height).intersection(big_window)\n",
    "        transform = windows.transform(window, ds.transform)\n",
    "        yield window, transform\n",
    "\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "tile_numbers = []\n",
    "\n",
    "with rio.open(os.path.join(in_path, input_filename)) as inds:\n",
    "    tile_width, tile_height = widthtile, heighttile\n",
    "    nodata = inds.nodata\n",
    "    meta = inds.meta.copy()\n",
    "    for window, transform in get_tiles(inds):\n",
    "        data = inds.read(window=window)\n",
    "        if nodata is not None and not (data == nodata).all():\n",
    "            meta['transform'] = transform\n",
    "            meta['width'], meta['height'] = window.width, window.height\n",
    "            tile_number = f\"{int(window.col_off)}-{int(window.row_off)}\"\n",
    "            tile_numbers.append(tile_number)\n",
    "            outpath = os.path.join(out_path, output_filename.format(int(window.col_off), int(window.row_off)))\n",
    "            with rio.open(outpath, 'w', **meta) as outds:\n",
    "                outds.write(data)\n",
    "\n",
    "# Print or store the tile numbers\n",
    "TILENUMBER = tile_numbers\n",
    "\n",
    "del in_path, input_filename, tile_numbers\n",
    "del out_path, output_filename, widthtile, heighttile, tile_width, tile_height\n",
    "del meta, nodata, window, inds, get_tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096633f3-7945-4a3d-9b10-ce58139d0a32",
   "metadata": {},
   "source": [
    "# Downlaod training data and create training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "974d7876-a5b6-4922-aa44-06356e1d96f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smdur\\anaconda3\\envs\\globalpcl\\lib\\site-packages\\pystac_client\\item_search.py:834: FutureWarning: get_items() is deprecated, use items() instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for tile 75000-35000 completed.\n",
      "Processing for tile 75000-40000 completed.\n",
      "Processing for tile 75000-45000 completed.\n",
      "All processing completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from osgeo import gdal\n",
    "import rioxarray\n",
    "import planetary_computer\n",
    "from pystac_client import Client\n",
    "import osmnx as ox\n",
    "import rasterio\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.windows import from_bounds, Window\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "from shapely.geometry import box, Point\n",
    "from geopandas import GeoDataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.plot import show\n",
    "\n",
    "TILENUMBER = ['75000-35000', '75000-40000', '75000-45000']\n",
    "CHIP_SIZE = 256  \n",
    "\n",
    "def delete_non_resampled_files(resampled_files, tif_dir):\n",
    "    for file in os.listdir(tif_dir):\n",
    "        if file not in resampled_files and file.endswith('.tif'):\n",
    "            file_path = os.path.join(tif_dir, file)\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted: {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete {file_path}: {e}\")\n",
    "\n",
    "def process_dem(tif_path, tif_dir, tile_number):\n",
    "    tif_data = rioxarray.open_rasterio(tif_path)\n",
    "    bbox_of_interest = tif_data.rio.bounds()\n",
    "    catalog = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
    "    search = catalog.search(collections=[\"cop-dem-glo-30\"], bbox=bbox_of_interest)\n",
    "    items = list(search.get_items())\n",
    "    \n",
    "    def process_item(item, idx):\n",
    "        signed_asset = planetary_computer.sign(item.assets[\"data\"])\n",
    "        data = rioxarray.open_rasterio(signed_asset.href).squeeze().drop(\"band\")\n",
    "        data.rio.write_crs(\"EPSG:4326\", inplace=True)\n",
    "        output_tif_path = os.path.join(tif_dir, f\"output_dataDEM_{idx}.tif\")\n",
    "        data.rio.to_raster(output_tif_path)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        for i, item in enumerate(items):\n",
    "            executor.submit(process_item, item, i)\n",
    "\n",
    "    output_tif = os.path.join(tif_dir, f\"outputtile_DEM_{tile_number}.tif\")\n",
    "    merge_command = [\n",
    "        \"python\", \"C:\\\\Users\\\\smdur\\\\anaconda3\\\\envs\\\\globalpcl\\\\Scripts\\\\gdal_merge.py\",\n",
    "        \"--config\", \"CHECK_DISK_FREE_SPACE\", \"FALSE\",\n",
    "        \"-o\", output_tif,\n",
    "        \"-n\", \"-9999\", \"-a_nodata\", \"-9999\"] + glob.glob(os.path.join(tif_dir, \"output_dataDEM_*.tif\"))\n",
    "\n",
    "    process_hag = subprocess.run(merge_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    if process_hag.returncode != 0:\n",
    "        print(f\"Error in merging DEM: {process_hag.stderr}\")\n",
    "        return None\n",
    "\n",
    "    src_ds = gdal.Open(output_tif, gdal.GA_ReadOnly)\n",
    "    target_ds = gdal.Open(tif_path, gdal.GA_ReadOnly)\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    output_resampled_path = os.path.join(tif_dir, f\"output_resampled_dataDEM_{tile_number}.tif\")\n",
    "    out_ds = driver.Create(output_resampled_path, target_ds.RasterXSize, target_ds.RasterYSize, 1, src_ds.GetRasterBand(1).DataType)\n",
    "    out_ds.SetGeoTransform(target_ds.GetGeoTransform())\n",
    "    out_ds.SetProjection(target_ds.GetProjection())\n",
    "    gdal.ReprojectImage(src_ds, out_ds, src_ds.GetProjection(), target_ds.GetProjection(), gdal.GRA_Bilinear)\n",
    "    src_ds, target_ds, out_ds = None, None, None\n",
    "\n",
    "    os.remove(output_tif)\n",
    "    for tif in glob.glob(os.path.join(tif_dir, \"output_dataDEM_*.tif\")):\n",
    "        try:\n",
    "            os.remove(tif)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {tif}: {e}\")\n",
    "\n",
    "    return output_resampled_path\n",
    "\n",
    "    del tif_data, bbox_of_interest, catalog, search, items\n",
    "    del output_tif, merge_command, process_hag\n",
    "    del src_ds, target_ds, driver, output_resampled_path\n",
    "\n",
    "\n",
    "def process_lidar(tif_path, tif_dir, tile_number):\n",
    "    lidar_dir = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GlobalData\\LIDAR2\"\n",
    "    lidar_tifs = glob.glob(os.path.join(lidar_dir, \"*.tif\"))\n",
    "\n",
    "    # Get the bounding box of the input tif_path\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        bbox = src.bounds\n",
    "        input_geom = box(bbox.left, bbox.bottom, bbox.right, bbox.top)\n",
    "\n",
    "    # Find overlapping LIDAR tiles\n",
    "    overlapping_tifs = []\n",
    "    for tif in lidar_tifs:\n",
    "        with rasterio.open(tif) as src:\n",
    "            lidar_bbox = src.bounds\n",
    "            lidar_geom = box(lidar_bbox.left, lidar_bbox.bottom, lidar_bbox.right, lidar_bbox.top)\n",
    "            if input_geom.intersects(lidar_geom):\n",
    "                overlapping_tifs.append(tif)\n",
    "\n",
    "    if not overlapping_tifs:\n",
    "        print(f\"No overlapping LIDAR tiles found for {tile_number}\")\n",
    "        return None\n",
    "\n",
    "    output_tif = os.path.join(tif_dir, f\"outputtile_lidar_{tile_number}.tif\")\n",
    "    merge_command = [\n",
    "        \"python\", \"C:\\\\Users\\\\smdur\\\\anaconda3\\\\envs\\\\globalpcl\\\\Scripts\\\\gdal_merge.py\",\n",
    "        \"--config\", \"CHECK_DISK_FREE_SPACE\", \"FALSE\",\n",
    "        \"-o\", output_tif,\n",
    "        \"-n\", \"255\", \"-a_nodata\", \"255\"] + overlapping_tifs\n",
    "\n",
    "    process_hag = subprocess.run(merge_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    if process_hag.returncode != 0:\n",
    "        print(f\"Error in merging LIDAR: {process_hag.stderr}\")\n",
    "        return None\n",
    "\n",
    "    src_ds = gdal.Open(output_tif, gdal.GA_ReadOnly)\n",
    "    target_ds = gdal.Open(tif_path, gdal.GA_ReadOnly)\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    output_resampled_path = os.path.join(tif_dir, f\"output_resampled_dataLIDAR_{tile_number}.tif\")\n",
    "    out_ds = driver.Create(output_resampled_path, target_ds.RasterXSize, target_ds.RasterYSize, 1, src_ds.GetRasterBand(1).DataType)\n",
    "    out_ds.SetGeoTransform(target_ds.GetGeoTransform())\n",
    "    out_ds.SetProjection(target_ds.GetProjection())\n",
    "    gdal.ReprojectImage(src_ds, out_ds, src_ds.GetProjection(), target_ds.GetProjection(), gdal.GRA_Bilinear)\n",
    "    src_ds, target_ds, out_ds = None, None, None\n",
    "\n",
    "    os.remove(output_tif)\n",
    "    return output_resampled_path\n",
    "\n",
    "    del lidar_tifs, bbox, input_geom, overlapping_tifs, lidar_bbox, lidar_geom\n",
    "    del output_tif, merge_command, process_hag\n",
    "    del src_ds, target_ds, driver, output_resampled_path\n",
    "\n",
    "\n",
    "def process_rivers(tif_path, tif_dir, tile_number):\n",
    "    dem_data = rioxarray.open_rasterio(tif_path)\n",
    "    bbox = dem_data.rio.bounds()\n",
    "    custom_filter = '[\"waterway\"~\"river\"]'\n",
    "    graph = ox.graph_from_bbox(bbox[3], bbox[1], bbox[2], bbox[0], custom_filter=custom_filter, simplify=True, retain_all=True, truncate_by_edge=True)\n",
    "    gdf = ox.graph_to_gdfs(graph, nodes=False)\n",
    "\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        window = from_bounds(*src.bounds, src.transform)\n",
    "        transform = rasterio.windows.transform(window, src.transform)\n",
    "        raster = np.zeros((int(window.height), int(window.width)), dtype=np.uint8)\n",
    "        shapes = ((geom, 1) for geom in gdf['geometry'])\n",
    "        burned = rasterize(shapes, out=raster, fill=0, transform=transform, all_touched=True)\n",
    "        distance_grid = scipy.ndimage.distance_transform_edt(burned == 0)\n",
    "        decay_grid = np.exp(-0.07 * distance_grid)\n",
    "\n",
    "        clipped_meta = src.meta.copy()\n",
    "        clipped_meta.update({\"driver\": \"GTiff\", \"height\": int(window.height), \"width\": int(window.width), \"transform\": transform, \"dtype\": rasterio.float32, \"count\": 1, \"compress\": 'lzw'})\n",
    "        output_path = os.path.join(tif_dir, f'exponential_decay_CO_river_{tile_number}.tif')\n",
    "        with rasterio.open(output_path, 'w', **clipped_meta) as dst:\n",
    "            dst.write(decay_grid.astype(np.float32), 1)\n",
    "\n",
    "    src_ds = gdal.Open(output_path, gdal.GA_ReadOnly)\n",
    "    target_ds = gdal.Open(tif_path, gdal.GA_ReadOnly)\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    output_resampled_path = os.path.join(tif_dir, f\"output_resampled_dataRivers_{tile_number}.tif\")\n",
    "    out_ds = driver.Create(output_resampled_path, target_ds.RasterXSize, target_ds.RasterYSize, 1, src_ds.GetRasterBand(1).DataType)\n",
    "    out_ds.SetGeoTransform(target_ds.GetGeoTransform())\n",
    "    out_ds.SetProjection(target_ds.GetProjection())\n",
    "    gdal.ReprojectImage(src_ds, out_ds, src_ds.GetProjection(), target_ds.GetProjection(), gdal.GRA_Bilinear)\n",
    "    src_ds, target_ds, out_ds = None, None, None\n",
    "    os.remove(output_path)\n",
    "\n",
    "    return output_resampled_path\n",
    "\n",
    "    del dem_data, bbox, custom_filter, graph, gdf\n",
    "    del window, transform, raster, shapes, burned, distance_grid, decay_grid\n",
    "    del clipped_meta, output_path\n",
    "    del src_ds, target_ds, driver, output_resampled_path\n",
    "\n",
    "\n",
    "def process_roads(tif_path, tif_dir, tile_number):\n",
    "    extent_data = rioxarray.open_rasterio(tif_path)\n",
    "    bbox = extent_data.rio.bounds()\n",
    "    graph = ox.graph_from_bbox(bbox[3], bbox[1], bbox[2], bbox[0], network_type='drive', simplify=True)\n",
    "    gdf = ox.graph_to_gdfs(graph, nodes=False)\n",
    "\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        window = from_bounds(*src.bounds, src.transform)\n",
    "        transform = rasterio.windows.transform(window, src.transform)\n",
    "        raster = np.zeros((int(window.height), int(window.width)), dtype=np.uint8)\n",
    "        shapes = ((geom, 1) for geom in gdf['geometry'])\n",
    "        burned = rasterize(shapes, out=raster, fill=0, transform=transform, all_touched=True)\n",
    "        distance_grid = scipy.ndimage.distance_transform_edt(burned == 0)\n",
    "        decay_grid = np.exp(-0.07 * distance_grid)\n",
    "\n",
    "        clipped_meta = src.meta.copy()\n",
    "        clipped_meta.update({\"driver\": \"GTiff\", \"height\": int(window.height), \"width\": int(window.width), \"transform\": transform, \"dtype\": rasterio.float32, \"count\": 1, \"compress\": 'lzw'})\n",
    "        output_path = os.path.join(tif_dir, f'exponential_decay_CO_roads_{tile_number}.tif')\n",
    "        with rasterio.open(output_path, 'w', **clipped_meta) as dst:\n",
    "            dst.write(decay_grid.astype(np.float32), 1)\n",
    "\n",
    "    src_ds = gdal.Open(output_path, gdal.GA_ReadOnly)\n",
    "    target_ds = gdal.Open(tif_path, gdal.GA_ReadOnly)\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    output_resampled_path = os.path.join(tif_dir, f\"output_resampled_dataRoads_{tile_number}.tif\")\n",
    "    out_ds = driver.Create(output_resampled_path, target_ds.RasterXSize, target_ds.RasterYSize, 1, src_ds.GetRasterBand(1).DataType)\n",
    "    out_ds.SetGeoTransform(target_ds.GetGeoTransform())\n",
    "    out_ds.SetProjection(target_ds.GetProjection())\n",
    "    gdal.ReprojectImage(src_ds, out_ds, src_ds.GetProjection(), target_ds.GetProjection(), gdal.GRA_Bilinear)\n",
    "    src_ds, target_ds, out_ds = None, None, None\n",
    "    os.remove(output_path)\n",
    "\n",
    "    return output_resampled_path\n",
    "\n",
    "    del extent_data, bbox, graph, gdf\n",
    "    del window, transform, raster, shapes, burned, distance_grid, decay_grid\n",
    "    del clipped_meta, output_path\n",
    "    del src_ds, target_ds, driver, output_resampled_path\n",
    "\n",
    "\n",
    "def generate_random_points(geometry, num_points):\n",
    "    points = []\n",
    "    min_x, min_y, max_x, max_y = geometry.bounds\n",
    "    while len(points) < num_points:\n",
    "        random_point = Point(np.random.uniform(min_x, max_x), np.random.uniform(min_y, max_y))\n",
    "        if random_point.within(geometry):\n",
    "            points.append(random_point)\n",
    "    return points\n",
    "\n",
    "    del bounds, crs, img, rect, buffered_rect, random_points\n",
    "    del gdf_points, gdf_points_wgs84, lat_long\n",
    "\n",
    "\n",
    "def process_chips(tif_path, tif_dir, lat_long, chip_size=128):\n",
    "    resampled_lidar_path = os.path.join(tif_dir, f\"output_resampled_dataLIDAR_{tile_number}.tif\")\n",
    "    resampled_dem_path = os.path.join(tif_dir, f\"output_resampled_dataDEM_{tile_number}.tif\")\n",
    "    resampled_rivers_path = os.path.join(tif_dir, f\"output_resampled_dataRivers_{tile_number}.tif\")\n",
    "    resampled_roads_path = os.path.join(tif_dir, f\"output_resampled_dataRoads_{tile_number}.tif\")\n",
    "\n",
    "    training_chips_dir = os.path.join(THEFOLDER, \"trainingchips\")\n",
    "    os.makedirs(training_chips_dir, exist_ok=True)\n",
    "\n",
    "    for i, (lat, lon) in enumerate(lat_long):\n",
    "        try:\n",
    "            paths = [resampled_lidar_path, resampled_dem_path, resampled_rivers_path, resampled_roads_path, tif_path]\n",
    "            labels = ['lidar', 'dem', 'rivers', 'roads', 'pcllabels']\n",
    "            \n",
    "            for path, label in zip(paths, labels):\n",
    "                with rasterio.open(path) as src:\n",
    "                    col, row = src.index(lon, lat)\n",
    "                    window = Window(col - chip_size // 2, row - chip_size // 2, chip_size, chip_size)\n",
    "                    chip_data = src.read(1, window=window)\n",
    "                    \n",
    "                    out_meta = src.meta.copy()\n",
    "                    out_meta.update({\n",
    "                        \"driver\": \"GTiff\",\n",
    "                        \"height\": chip_size,\n",
    "                        \"width\": chip_size,\n",
    "                        \"transform\": src.window_transform(window)\n",
    "                    })\n",
    "\n",
    "                    chip_output_dir = os.path.join(training_chips_dir, label)\n",
    "                    os.makedirs(chip_output_dir, exist_ok=True)\n",
    "                    \n",
    "                    chip_output_path = os.path.join(chip_output_dir, f\"{label.upper()}_Chip_{tile_number}_{i}.tif\")\n",
    "\n",
    "                    if chip_data.shape == (chip_size, chip_size) and np.any(chip_data != src.nodata):\n",
    "                        with rasterio.open(chip_output_path, \"w\", **out_meta) as dest:\n",
    "                            dest.write(chip_data, 1)\n",
    "                    else:\n",
    "                        print(f\"Skipping {label} chip {i} because it is not properly shaped or is filled with nodata.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing chip {i}: {e}\")\n",
    "            \n",
    "    del resampled_lidar_path, resampled_dem_path, resampled_rivers_path, resampled_roads_path\n",
    "    del training_chips_dir, paths, labels, col, row, window, chip_data, out_meta, chip_output_dir, chip_output_path\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for tile_number in TILENUMBER:\n",
    "        tif_path = f\"{THEFOLDER}\\\\PCLTILES\\\\pcltile_{tile_number}.tif\"\n",
    "        tif_dir = f\"{THEFOLDER}\\\\TIFFOUTPUT\\\\{tile_number}\"\n",
    "        os.makedirs(tif_dir, exist_ok=True)\n",
    "\n",
    "        resampled_files = [\n",
    "            process_dem(tif_path, tif_dir, tile_number),\n",
    "            process_lidar(tif_path, tif_dir, tile_number),\n",
    "            process_rivers(tif_path, tif_dir, tile_number),\n",
    "            process_roads(tif_path, tif_dir, tile_number)\n",
    "        ]\n",
    "\n",
    "        delete_non_resampled_files([os.path.basename(f) for f in resampled_files], tif_dir)\n",
    "\n",
    "        # Generate random points within the tile bounds\n",
    "        with rasterio.open(tif_path) as src:\n",
    "            bounds = src.bounds\n",
    "            crs = src.crs\n",
    "            img = src.read(1)\n",
    "\n",
    "        rect = box(bounds.left, bounds.bottom, bounds.right, bounds.top)\n",
    "        buffered_rect = rect.buffer(-0.15)\n",
    "        random_points = generate_random_points(buffered_rect, 100)\n",
    "        gdf_points = GeoDataFrame(geometry=random_points, crs=crs).to_crs(crs)\n",
    "        gdf_points_wgs84 = gdf_points.to_crs(epsg=4326)\n",
    "        lat_long = gdf_points_wgs84.geometry.apply(lambda geom: (geom.y, geom.x)).tolist()\n",
    "\n",
    "        process_chips(tif_path, tif_dir, lat_long)\n",
    "\n",
    "        print(f\"Processing for tile {tile_number} completed.\")\n",
    "\n",
    "    print(\"All processing completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23b74f6-f2f4-48b1-8d18-ea11a96ef951",
   "metadata": {},
   "source": [
    "# LoadChips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0c8c8a1-28c8-440c-aca7-e2ca6c83f36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import rasterio\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# training_chips_dir = os.path.join(THEFOLDER, \"trainingchips\")\n",
    "\n",
    "# # Paths to datasets\n",
    "# featurepath1 = os.path.join(training_chips_dir, \"lidar\")\n",
    "# featurepath2 = os.path.join(training_chips_dir, \"dem\")\n",
    "# featurepath3 = os.path.join(training_chips_dir, \"roads\")\n",
    "# featurepath4 = os.path.join(training_chips_dir, \"rivers\")\n",
    "# labelspath = os.path.join(training_chips_dir, \"pcllabels\")\n",
    "\n",
    "# # Function to load GeoTIFF images as numpy arrays\n",
    "# def load_geotiff(path):\n",
    "#     with rasterio.open(path) as src:\n",
    "#         return src.read(1)\n",
    "\n",
    "# # Function to load and print progress\n",
    "# def load_images(path):\n",
    "#     files = [f for f in os.listdir(path) if f.endswith('.tif')]\n",
    "#     images = []\n",
    "#     for i, f in enumerate(files):\n",
    "#         images.append(load_geotiff(os.path.join(path, f)))\n",
    "#         if (i + 1) % 5000 == 0:\n",
    "#             print(f\"Loaded {i + 1} images from {path}\")\n",
    "#     return images\n",
    "\n",
    "# # Load datasets\n",
    "# hag_images = load_images(featurepath1)\n",
    "# dem_images = load_images(featurepath2)\n",
    "# roads_images = load_images(featurepath3)\n",
    "# rivers_images = load_images(featurepath4)\n",
    "# label_images = load_images(labelspath)\n",
    "\n",
    "# # Convert lists to numpy arrays\n",
    "# hag_images = np.array(hag_images).astype('float32')\n",
    "# dem_images = np.array(dem_images).astype('float32')\n",
    "# roads_images = np.array(roads_images).astype('float32')\n",
    "# rivers_images = np.array(rivers_images).astype('float32')\n",
    "# label_images = np.array(label_images).astype('float32')\n",
    "\n",
    "# # Normalize images independently\n",
    "# hag_max = hag_images.max()\n",
    "# dem_max = dem_images.max()\n",
    "# roads_max = roads_images.max()\n",
    "# rivers_max = rivers_images.max()\n",
    "\n",
    "# hag_images /= hag_max\n",
    "# dem_images /= dem_max\n",
    "# roads_images /= roads_max\n",
    "# rivers_images /= rivers_max\n",
    "\n",
    "# #del labelspath, load_geotiff, load_images\n",
    "\n",
    "# print(f\"HAG max value: {hag_max}\")\n",
    "# print(f\"DEM max value: {dem_max}\")\n",
    "# print(f\"Roads max value: {roads_max}\")\n",
    "# print(f\"Rivers max value: {rivers_max}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93b9b678-4c79-4eb4-9076-8acaa530ae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing\n",
    "# max_workers = mult#iprocessing.cpu_count()\n",
    "# max_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9221572-3aa0-4ea9-b091-1187ab112c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAG max value: 33.0\n",
      "DEM max value: 4183.34423828125\n",
      "Roads max value: 1.0\n",
      "Rivers max value: 1.0\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import rasterio\n",
    "# import numpy as np\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# training_chips_dir = os.path.join(THEFOLDER, \"trainingchips\")\n",
    "\n",
    "# # Paths to datasets\n",
    "# featurepath1 = os.path.join(training_chips_dir, \"lidar\")\n",
    "# featurepath2 = os.path.join(training_chips_dir, \"dem\")\n",
    "# featurepath3 = os.path.join(training_chips_dir, \"roads\")\n",
    "# featurepath4 = os.path.join(training_chips_dir, \"rivers\")\n",
    "# labelspath = os.path.join(training_chips_dir, \"pcllabels\")\n",
    "\n",
    "# # Function to load GeoTIFF images as numpy arrays\n",
    "# def load_geotiff(path):\n",
    "#     with rasterio.open(path) as src:\n",
    "#         return src.read(1)\n",
    "\n",
    "# # Function to load images in parallel with progress printing\n",
    "# def load_images(path):\n",
    "#     files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.tif')]\n",
    "#     images = []\n",
    "\n",
    "#     def load_and_count(file):\n",
    "#         image = load_geotiff(file)\n",
    "#         if (load_and_count.counter + 1) % 5000 == 0:\n",
    "#             print(f\"Loaded {load_and_count.counter + 1} images from {path}\")\n",
    "#         load_and_count.counter += 1\n",
    "#         return image\n",
    "\n",
    "#     load_and_count.counter = 0\n",
    "\n",
    "#     with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "#         images = list(executor.map(load_and_count, files))\n",
    "\n",
    "#     return images\n",
    "\n",
    "# # Load datasets\n",
    "# hag_images = load_images(featurepath1)\n",
    "# dem_images = load_images(featurepath2)\n",
    "# roads_images = load_images(featurepath3)\n",
    "# rivers_images = load_images(featurepath4)\n",
    "# label_images = load_images(labelspath)\n",
    "\n",
    "# # Convert lists to numpy arrays\n",
    "# hag_images = np.array(hag_images).astype('float32')\n",
    "# dem_images = np.array(dem_images).astype('float32')\n",
    "# roads_images = np.array(roads_images).astype('float32')\n",
    "# rivers_images = np.array(rivers_images).astype('float32')\n",
    "# label_images = np.array(label_images).astype('float32')\n",
    "\n",
    "# # Normalize images independently\n",
    "# hag_max = hag_images.max()\n",
    "# dem_max = dem_images.max()\n",
    "# roads_max = roads_images.max()\n",
    "# rivers_max = rivers_images.max()\n",
    "\n",
    "# hag_images /= hag_max\n",
    "# dem_images /= dem_max\n",
    "# roads_images /= roads_max\n",
    "# rivers_images /= rivers_max\n",
    "\n",
    "# del labelspath, load_geotiff, load_images\n",
    "\n",
    "# print(f\"HAG max value: {hag_max}\")\n",
    "# print(f\"DEM max value: {dem_max}\")\n",
    "# print(f\"Roads max value: {roads_max}\")\n",
    "# print(f\"Rivers max value: {rivers_max}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Set the environment variable before importing gdal\n",
    "os.environ['USE_PATH_FOR_GDAL_PYTHON'] = 'YES'\n",
    "os.add_dll_directory(os.path.join(os.getenv('CONDA_PREFIX'), 'Library', 'bin'))\n",
    "\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "training_chips_dir = os.path.join(THEFOLDER, \"trainingchips\")\n",
    "\n",
    "# Paths to datasets\n",
    "featurepath1 = os.path.join(training_chips_dir, \"lidar\")\n",
    "featurepath2 = os.path.join(training_chips_dir, \"dem\")\n",
    "featurepath3 = os.path.join(training_chips_dir, \"roads\")\n",
    "featurepath4 = os.path.join(training_chips_dir, \"rivers\")\n",
    "labelspath = os.path.join(training_chips_dir, \"pcllabels\")\n",
    "\n",
    "# Function to load GeoTIFF images as numpy arrays\n",
    "def load_geotiff(path):\n",
    "    with rasterio.open(path) as src:\n",
    "        return src.read(1)\n",
    "\n",
    "# Function to load images in parallel with progress printing\n",
    "def load_images(path):\n",
    "    files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.tif')]\n",
    "    num_files = len(files)\n",
    "    images = []\n",
    "\n",
    "    def load_and_count(file):\n",
    "        image = load_geotiff(file)\n",
    "        if (load_and_count.counter + 1) % 5000 == 0:\n",
    "            print(f\"Loaded {load_and_count.counter + 1} images from {path}\")\n",
    "        load_and_count.counter += 1\n",
    "        return image\n",
    "\n",
    "    load_and_count.counter = 0\n",
    "\n",
    "    max_workers = min(32, os.cpu_count() + 4)  # Default value if not specified\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        images = list(executor.map(load_and_count, files))\n",
    "\n",
    "    return np.array(images).astype('float32')\n",
    "\n",
    "# Load datasets\n",
    "hag_images = load_images(featurepath1)\n",
    "dem_images = load_images(featurepath2)\n",
    "roads_images = load_images(featurepath3)\n",
    "rivers_images = load_images(featurepath4)\n",
    "label_images = load_images(labelspath)\n",
    "\n",
    "# Normalize images independently\n",
    "hag_max = hag_images.max()\n",
    "dem_max = dem_images.max()\n",
    "roads_max = roads_images.max()\n",
    "rivers_max = rivers_images.max()\n",
    "\n",
    "hag_images /= hag_max\n",
    "dem_images /= dem_max\n",
    "roads_images /= roads_max\n",
    "rivers_images /= rivers_max\n",
    "\n",
    "\n",
    "print(f\"HAG max value: {hag_max}\")\n",
    "print(f\"DEM max value: {dem_max}\")\n",
    "print(f\"Roads max value: {roads_max}\")\n",
    "print(f\"Rivers max value: {rivers_max}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4c045f5-9aa1-426f-8f1d-627b65c82f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAG max value: 33.0\n",
      "DEM max value: 4183.34423828125\n",
      "Roads max value: 1.0\n",
      "Rivers max value: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"HAG max value: {hag_max}\")\n",
    "print(f\"DEM max value: {dem_max}\")\n",
    "print(f\"Roads max value: {roads_max}\")\n",
    "print(f\"Rivers max value: {rivers_max}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c507c57-8311-43a2-8845-f0741666c2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.6060606 , 0.6060606 , 0.57575756, ..., 0.21212122,\n",
       "         0.33333334, 0.36363637],\n",
       "        [0.54545456, 0.54545456, 0.54545456, ..., 0.27272728,\n",
       "         0.33333334, 0.3939394 ],\n",
       "        [0.5151515 , 0.4848485 , 0.4848485 , ..., 0.36363637,\n",
       "         0.36363637, 0.3939394 ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ]],\n",
       "\n",
       "       [[0.24242425, 0.3939394 , 0.5151515 , ..., 0.42424244,\n",
       "         0.42424244, 0.45454547],\n",
       "        [0.45454547, 0.45454547, 0.5151515 , ..., 0.42424244,\n",
       "         0.42424244, 0.45454547],\n",
       "        [0.42424244, 0.4848485 , 0.5151515 , ..., 0.3939394 ,\n",
       "         0.42424244, 0.42424244],\n",
       "        ...,\n",
       "        [0.12121212, 0.09090909, 0.06060606, ..., 0.24242425,\n",
       "         0.18181819, 0.15151516],\n",
       "        [0.12121212, 0.06060606, 0.06060606, ..., 0.27272728,\n",
       "         0.18181819, 0.21212122],\n",
       "        [0.12121212, 0.06060606, 0.03030303, ..., 0.33333334,\n",
       "         0.21212122, 0.27272728]],\n",
       "\n",
       "       [[0.6363636 , 0.6666667 , 0.6363636 , ..., 0.45454547,\n",
       "         0.4848485 , 0.4848485 ],\n",
       "        [0.6666667 , 0.6666667 , 0.6666667 , ..., 0.4848485 ,\n",
       "         0.5151515 , 0.54545456],\n",
       "        [0.6666667 , 0.6666667 , 0.6969697 , ..., 0.54545456,\n",
       "         0.57575756, 0.57575756],\n",
       "        ...,\n",
       "        [0.09090909, 0.21212122, 0.3030303 , ..., 0.3939394 ,\n",
       "         0.42424244, 0.42424244],\n",
       "        [0.12121212, 0.15151516, 0.12121212, ..., 0.33333334,\n",
       "         0.33333334, 0.36363637],\n",
       "        [0.18181819, 0.15151516, 0.09090909, ..., 0.27272728,\n",
       "         0.27272728, 0.3030303 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.24242425, 0.12121212, 0.06060606, ..., 0.5151515 ,\n",
       "         0.57575756, 0.6060606 ],\n",
       "        [0.09090909, 0.03030303, 0.        , ..., 0.36363637,\n",
       "         0.57575756, 0.6060606 ],\n",
       "        [0.03030303, 0.        , 0.        , ..., 0.3939394 ,\n",
       "         0.54545456, 0.6060606 ],\n",
       "        ...,\n",
       "        [0.6363636 , 0.6060606 , 0.57575756, ..., 0.57575756,\n",
       "         0.57575756, 0.57575756],\n",
       "        [0.57575756, 0.5151515 , 0.45454547, ..., 0.6060606 ,\n",
       "         0.6060606 , 0.6060606 ],\n",
       "        [0.4848485 , 0.3939394 , 0.33333334, ..., 0.57575756,\n",
       "         0.6060606 , 0.6363636 ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ]],\n",
       "\n",
       "       [[0.09090909, 0.12121212, 0.15151516, ..., 0.75757575,\n",
       "         0.75757575, 0.72727275],\n",
       "        [0.12121212, 0.15151516, 0.18181819, ..., 0.75757575,\n",
       "         0.75757575, 0.72727275],\n",
       "        [0.24242425, 0.3030303 , 0.3030303 , ..., 0.72727275,\n",
       "         0.75757575, 0.75757575],\n",
       "        ...,\n",
       "        [0.6969697 , 0.72727275, 0.72727275, ..., 0.75757575,\n",
       "         0.72727275, 0.72727275],\n",
       "        [0.6969697 , 0.6969697 , 0.72727275, ..., 0.7878788 ,\n",
       "         0.75757575, 0.75757575],\n",
       "        [0.6969697 , 0.6969697 , 0.6969697 , ..., 0.7878788 ,\n",
       "         0.7878788 , 0.75757575]]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hag_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ddc68b-b186-4790-89a9-053847d2a1a6",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5e96702-f1f4-4400-960d-431ecbe862a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[230400,65536] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:StatelessRandomUniformV2] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m label_images \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(label_images, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Define the CNN model\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#Conv2D(16, (3, 3), activation='relu', input_shape=(128, 128, 4)),\u001b[39;49;00m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mCHIP_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCHIP_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMaxPooling2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMaxPooling2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mFlatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#Dense(128 * 128, activation='sigmoid'),\u001b[39;49;00m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#tf.keras.layers.Reshape((128, 128, 1))\u001b[39;49;00m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCHIP_SIZE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mCHIP_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msigmoid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCHIP_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCHIP_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# # Define custom weights for each feature\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# weights = np.array([1.0, 0.8, 0.5, 0.3])  \u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# sample_weights = np.dot(feature_images, weights)\u001b[39;00m\n\u001b[0;32m     47\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0006\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\globalpcl\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:204\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 204\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\globalpcl\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\globalpcl\\lib\\site-packages\\keras\\src\\backend.py:2102\u001b[0m, in \u001b[0;36mRandomGenerator.random_uniform\u001b[1;34m(self, shape, minval, maxval, dtype, nonce)\u001b[0m\n\u001b[0;32m   2100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonce:\n\u001b[0;32m   2101\u001b[0m         seed \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mstateless_fold_in(seed, nonce)\n\u001b[1;32m-> 2102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstateless_uniform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mminval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mminval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaxval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2108\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\n\u001b[0;32m   2110\u001b[0m     shape\u001b[38;5;241m=\u001b[39mshape,\n\u001b[0;32m   2111\u001b[0m     minval\u001b[38;5;241m=\u001b[39mminval,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2114\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_legacy_seed(),\n\u001b[0;32m   2115\u001b[0m )\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[230400,65536] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:StatelessRandomUniformV2] name: "
     ]
    }
   ],
   "source": [
    "# import gc\n",
    "# gc.collect()\n",
    "\n",
    "# # Stack features along the last dimension\n",
    "# feature_images = np.stack((hag_images, dem_images, roads_images, rivers_images), axis=-1)\n",
    "\n",
    "# Free up memory by deleting the original arrays\n",
    "# del hag_images\n",
    "# del dem_images\n",
    "# del roads_images\n",
    "# del rivers_images\n",
    "\n",
    "# If you want to ensure that the memory is freed immediately\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "CHIP_SIZE=256\n",
    "\n",
    "# Normalize labels if they range from 0 to 100\n",
    "label_images /= 100\n",
    "\n",
    "# Reshape labels for CNN input\n",
    "label_images = np.expand_dims(label_images, axis=-1)\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential([\n",
    "    #Conv2D(16, (3, 3), activation='relu', input_shape=(128, 128, 4)),\n",
    "    Conv2D(16, (3, 3), activation='relu', input_shape=(CHIP_SIZE, CHIP_SIZE, 4)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    #Dense(128 * 128, activation='sigmoid'),\n",
    "    #tf.keras.layers.Reshape((128, 128, 1))\n",
    "    Dense(CHIP_SIZE * CHIP_SIZE, activation='sigmoid'),\n",
    "    tf.keras.layers.Reshape((CHIP_SIZE, CHIP_SIZE, 1))\n",
    "])\n",
    "\n",
    "# # Define custom weights for each feature\n",
    "# weights = np.array([1.0, 0.8, 0.5, 0.3])  \n",
    "# sample_weights = np.dot(feature_images, weights)\n",
    "\n",
    "lr = 0.0006\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "# Compile and train the model with sample weights\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "model.fit(feature_images, label_images, batch_size=8, epochs=3, validation_split=0.3)#, sample_weight=sample_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7586c54b-5bcc-4a04-95fd-8a4cfb92dc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\SavedModels\\\\cnn_model_lr0001_5_15_24\")\n",
    "#model.save(\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\SavedModels\\\\cnn_model_lr0005_5_15_24\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3125a158-3f9e-43c6-835c-78a910a69f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = tf.keras.models.load_model(f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\SavedModels\\\\cnn_model_lr0005_5_15_24\")\n",
    "#model = tf.keras.models.load_model(f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\SavedModels\\\\cnn_model_lr0005_5_15_24\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca29acf-d06c-42ff-98f3-d7aa2092e063",
   "metadata": {},
   "source": [
    "# INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a623c6bb-dfac-4a64-a098-7980f3d199b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE DATA\n",
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from osgeo import gdal\n",
    "import rioxarray\n",
    "import planetary_computer\n",
    "from pystac_client import Client\n",
    "import osmnx as ox\n",
    "import rasterio\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.windows import from_bounds, Window\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "from shapely.geometry import box\n",
    "from geopandas import GeoDataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.plot import show\n",
    "import rasterio as rio\n",
    "from itertools import product\n",
    "from rasterio import windows\n",
    "\n",
    "TILENUMBER = ['70000-40000']\n",
    "CHIP_SIZE = 256  \n",
    "\n",
    "def delete_non_resampled_files(resampled_files, tif_dir):\n",
    "    for file in os.listdir(tif_dir):\n",
    "        if file not in resampled_files and file.endswith('.tif'):\n",
    "            file_path = os.path.join(tif_dir, file)\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted: {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete {file_path}: {e}\")\n",
    "\n",
    "def process_dem(tif_path, tif_dir, tile_number):\n",
    "    tif_data = rioxarray.open_rasterio(tif_path)\n",
    "    bbox_of_interest = tif_data.rio.bounds()\n",
    "    catalog = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
    "    search = catalog.search(collections=[\"cop-dem-glo-30\"], bbox=bbox_of_interest)\n",
    "    items = list(search.get_items())\n",
    "    \n",
    "    def process_item(item, idx):\n",
    "        signed_asset = planetary_computer.sign(item.assets[\"data\"])\n",
    "        data = rioxarray.open_rasterio(signed_asset.href).squeeze().drop(\"band\")\n",
    "        data.rio.write_crs(\"EPSG:4326\", inplace=True)\n",
    "        output_tif_path = os.path.join(tif_dir, f\"output_dataDEM_{idx}.tif\")\n",
    "        data.rio.to_raster(output_tif_path)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        for i, item in enumerate(items):\n",
    "            executor.submit(process_item, item, i)\n",
    "\n",
    "    output_tif = os.path.join(tif_dir, f\"outputtile_DEM_{tile_number}.tif\")\n",
    "    merge_command = [\n",
    "        \"python\", \"C:\\\\Users\\\\smdur\\\\anaconda3\\\\envs\\\\globalpcl\\\\Scripts\\\\gdal_merge.py\",\n",
    "        \"--config\", \"CHECK_DISK_FREE_SPACE\", \"FALSE\",\n",
    "        \"-o\", output_tif,\n",
    "        \"-n\", \"-9999\", \"-a_nodata\", \"-9999\"] + glob.glob(os.path.join(tif_dir, \"output_dataDEM_*.tif\"))\n",
    "\n",
    "    process_hag = subprocess.run(merge_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    if process_hag.returncode != 0:\n",
    "        print(f\"Error in merging DEM: {process_hag.stderr}\")\n",
    "        return None\n",
    "\n",
    "    src_ds = gdal.Open(output_tif, gdal.GA_ReadOnly)\n",
    "    target_ds = gdal.Open(tif_path, gdal.GA_ReadOnly)\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    output_resampled_path = os.path.join(tif_dir, f\"output_resampled_dataDEM_{tile_number}.tif\")\n",
    "    out_ds = driver.Create(output_resampled_path, target_ds.RasterXSize, target_ds.RasterYSize, 1, src_ds.GetRasterBand(1).DataType)\n",
    "    out_ds.SetGeoTransform(target_ds.GetGeoTransform())\n",
    "    out_ds.SetProjection(target_ds.GetProjection())\n",
    "    gdal.ReprojectImage(src_ds, out_ds, src_ds.GetProjection(), target_ds.GetProjection(), gdal.GRA_Bilinear)\n",
    "    src_ds, target_ds, out_ds = None, None, None\n",
    "\n",
    "    os.remove(output_tif)\n",
    "    for tif in glob.glob(os.path.join(tif_dir, \"output_dataDEM_*.tif\")):\n",
    "        try:\n",
    "            os.remove(tif)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {tif}: {e}\")\n",
    "\n",
    "    return output_resampled_path\n",
    "    del tif_data, bbox_of_interest, catalog, search, items\n",
    "    del output_tif, merge_command, process_hag\n",
    "    del src_ds, target_ds, driver, output_resampled_path\n",
    "\n",
    "def process_lidar(tif_path, tif_dir, tile_number):\n",
    "    lidar_dir = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GlobalData\\LIDAR2\"\n",
    "    lidar_tifs = glob.glob(os.path.join(lidar_dir, \"*.tif\"))\n",
    "\n",
    "    # Get the bounding box of the input tif_path\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        bbox = src.bounds\n",
    "        input_geom = box(bbox.left, bbox.bottom, bbox.right, bbox.top)\n",
    "\n",
    "    # Find overlapping LIDAR tiles\n",
    "    overlapping_tifs = []\n",
    "    for tif in lidar_tifs:\n",
    "        with rasterio.open(tif) as src:\n",
    "            lidar_bbox = src.bounds\n",
    "            lidar_geom = box(lidar_bbox.left, lidar_bbox.bottom, lidar_bbox.right, lidar_bbox.top)\n",
    "            if input_geom.intersects(lidar_geom):\n",
    "                overlapping_tifs.append(tif)\n",
    "\n",
    "    if not overlapping_tifs:\n",
    "        print(f\"No overlapping LIDAR tiles found for {tile_number}\")\n",
    "        return None\n",
    "\n",
    "    output_tif = os.path.join(tif_dir, f\"outputtile_lidar_{tile_number}.tif\")\n",
    "    merge_command = [\n",
    "        \"python\", \"C:\\\\Users\\\\smdur\\\\anaconda3\\\\envs\\\\globalpcl\\\\Scripts\\\\gdal_merge.py\",\n",
    "        \"--config\", \"CHECK_DISK_FREE_SPACE\", \"FALSE\",\n",
    "        \"-o\", output_tif,\n",
    "        \"-n\", \"255\", \"-a_nodata\", \"255\"] + overlapping_tifs\n",
    "\n",
    "    process_hag = subprocess.run(merge_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    if process_hag.returncode != 0:\n",
    "        print(f\"Error in merging LIDAR: {process_hag.stderr}\")\n",
    "        return None\n",
    "\n",
    "    src_ds = gdal.Open(output_tif, gdal.GA_ReadOnly)\n",
    "    target_ds = gdal.Open(tif_path, gdal.GA_ReadOnly)\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    output_resampled_path = os.path.join(tif_dir, f\"output_resampled_dataLIDAR_{tile_number}.tif\")\n",
    "    out_ds = driver.Create(output_resampled_path, target_ds.RasterXSize, target_ds.RasterYSize, 1, src_ds.GetRasterBand(1).DataType)\n",
    "    out_ds.SetGeoTransform(target_ds.GetGeoTransform())\n",
    "    out_ds.SetProjection(target_ds.GetProjection())\n",
    "    gdal.ReprojectImage(src_ds, out_ds, src_ds.GetProjection(), target_ds.GetProjection(), gdal.GRA_Bilinear)\n",
    "    src_ds, target_ds, out_ds = None, None, None\n",
    "\n",
    "    os.remove(output_tif)\n",
    "    return output_resampled_path\n",
    "    del lidar_tifs, bbox, input_geom, overlapping_tifs, lidar_bbox, lidar_geom\n",
    "    del output_tif, merge_command, process_hag\n",
    "    del src_ds, target_ds, driver, output_resampled_path\n",
    "\n",
    "def process_rivers(tif_path, tif_dir, tile_number):\n",
    "    dem_data = rioxarray.open_rasterio(tif_path)\n",
    "    bbox = dem_data.rio.bounds()\n",
    "    custom_filter = '[\"waterway\"~\"river\"]'\n",
    "    graph = ox.graph_from_bbox(bbox[3], bbox[1], bbox[2], bbox[0], custom_filter=custom_filter, simplify=True, retain_all=True, truncate_by_edge=True)\n",
    "    gdf = ox.graph_to_gdfs(graph, nodes=False)\n",
    "\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        window = from_bounds(*src.bounds, src.transform)\n",
    "        transform = rasterio.windows.transform(window, src.transform)\n",
    "        raster = np.zeros((int(window.height), int(window.width)), dtype=np.uint8)\n",
    "        shapes = ((geom, 1) for geom in gdf['geometry'])\n",
    "        burned = rasterize(shapes, out=raster, fill=0, transform=transform, all_touched=True)\n",
    "        distance_grid = scipy.ndimage.distance_transform_edt(burned == 0)\n",
    "        decay_grid = np.exp(-0.07 * distance_grid)\n",
    "\n",
    "        clipped_meta = src.meta.copy()\n",
    "        clipped_meta.update({\"driver\": \"GTiff\", \"height\": int(window.height), \"width\": int(window.width), \"transform\": transform, \"dtype\": rasterio.float32, \"count\": 1, \"compress\": 'lzw'})\n",
    "        output_path = os.path.join(tif_dir, f'exponential_decay_CO_river_{tile_number}.tif')\n",
    "        with rasterio.open(output_path, 'w', **clipped_meta) as dst:\n",
    "            dst.write(decay_grid.astype(np.float32), 1)\n",
    "\n",
    "    src_ds = gdal.Open(output_path, gdal.GA_ReadOnly)\n",
    "    target_ds = gdal.Open(tif_path, gdal.GA_ReadOnly)\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    output_resampled_path = os.path.join(tif_dir, f\"output_resampled_dataRivers_{tile_number}.tif\")\n",
    "    out_ds = driver.Create(output_resampled_path, target_ds.RasterXSize, target_ds.RasterYSize, 1, src_ds.GetRasterBand(1).DataType)\n",
    "    out_ds.SetGeoTransform(target_ds.GetGeoTransform())\n",
    "    out_ds.SetProjection(target_ds.GetProjection())\n",
    "    gdal.ReprojectImage(src_ds, out_ds, src_ds.GetProjection(), target_ds.GetProjection(), gdal.GRA_Bilinear)\n",
    "    src_ds, target_ds, out_ds = None, None, None\n",
    "    os.remove(output_path)\n",
    "\n",
    "    return output_resampled_path\n",
    "    del dem_data, bbox, custom_filter, graph, gdf\n",
    "    del window, transform, raster, shapes, burned, distance_grid, decay_grid\n",
    "    del clipped_meta, output_path\n",
    "    del src_ds, target_ds, driver, output_resampled_path\n",
    "\n",
    "def process_roads(tif_path, tif_dir, tile_number):\n",
    "    extent_data = rioxarray.open_rasterio(tif_path)\n",
    "    bbox = extent_data.rio.bounds()\n",
    "    graph = ox.graph_from_bbox(bbox[3], bbox[1], bbox[2], bbox[0], network_type='drive', simplify=True)\n",
    "    gdf = ox.graph_to_gdfs(graph, nodes=False)\n",
    "\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        window = from_bounds(*src.bounds, src.transform)\n",
    "        transform = rasterio.windows.transform(window, src.transform)\n",
    "        raster = np.zeros((int(window.height), int(window.width)), dtype=np.uint8)\n",
    "        shapes = ((geom, 1) for geom in gdf['geometry'])\n",
    "        burned = rasterize(shapes, out=raster, fill=0, transform=transform, all_touched=True)\n",
    "        distance_grid = scipy.ndimage.distance_transform_edt(burned == 0)\n",
    "        decay_grid = np.exp(-0.07 * distance_grid)\n",
    "\n",
    "        clipped_meta = src.meta.copy()\n",
    "        clipped_meta.update({\"driver\": \"GTiff\", \"height\": int(window.height), \"width\": int(window.width), \"transform\": transform, \"dtype\": rasterio.float32, \"count\": 1, \"compress\": 'lzw'})\n",
    "        output_path = os.path.join(tif_dir, f'exponential_decay_CO_roads_{tile_number}.tif')\n",
    "        with rasterio.open(output_path, 'w', **clipped_meta) as dst:\n",
    "            dst.write(decay_grid.astype(np.float32), 1)\n",
    "\n",
    "    src_ds = gdal.Open(output_path, gdal.GA_ReadOnly)\n",
    "    target_ds = gdal.Open(tif_path, gdal.GA_ReadOnly)\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    output_resampled_path = os.path.join(tif_dir, f\"output_resampled_dataRoads_{tile_number}.tif\")\n",
    "    out_ds = driver.Create(output_resampled_path, target_ds.RasterXSize, target_ds.RasterYSize, 1, src_ds.GetRasterBand(1).DataType)\n",
    "    out_ds.SetGeoTransform(target_ds.GetGeoTransform())\n",
    "    out_ds.SetProjection(target_ds.GetProjection())\n",
    "    gdal.ReprojectImage(src_ds, out_ds, src_ds.GetProjection(), target_ds.GetProjection(), gdal.GRA_Bilinear)\n",
    "    src_ds, target_ds, out_ds = None, None, None\n",
    "    os.remove(output_path)\n",
    "\n",
    "    return output_resampled_path\n",
    "    del extent_data, bbox, graph, gdf\n",
    "    del window, transform, raster, shapes, burned, distance_grid, decay_grid\n",
    "    del clipped_meta, output_path\n",
    "    del src_ds, target_ds, driver, output_resampled_path\n",
    "\n",
    "def get_tiles(ds, width=CHIP_SIZE, height=CHIP_SIZE):\n",
    "    nols, nrows = ds.meta['width'], ds.meta['height']\n",
    "    offsets = product(range(0, nols, width), range(0, nrows, height))\n",
    "    big_window = windows.Window(col_off=0, row_off=0, width=nols, height=nrows)\n",
    "    for col_off, row_off in offsets:\n",
    "        window = windows.Window(col_off=col_off, row_off=row_off, width=width, height=height).intersection(big_window)\n",
    "        transform = windows.transform(window, ds.transform)\n",
    "        yield window, transform\n",
    "\n",
    "def process_file(label, input_filepath, output_folder):\n",
    "    with rio.open(input_filepath) as inds:\n",
    "        nodata = inds.nodata  # Get the NoData value from the dataset\n",
    "        meta = inds.meta.copy()\n",
    "        \n",
    "        for window, transform in get_tiles(inds):\n",
    "            if window.width == CHIP_SIZE and window.height == CHIP_SIZE:  # Check if the tile dimensions are as expected\n",
    "                data = inds.read(window=window)\n",
    "                if nodata is not None:\n",
    "                    valid_data_mask = (data != nodata)\n",
    "                else:\n",
    "                    valid_data_mask = (data == data)\n",
    "                \n",
    "                if valid_data_mask.any():  # Check if there's any valid data within the tile\n",
    "                    meta['transform'] = transform\n",
    "                    meta['width'], meta['height'] = window.width, window.height\n",
    "                    outpath = os.path.join(output_folder, output_filename.format(int(window.col_off), int(window.row_off)))\n",
    "                    with rio.open(outpath, 'w', **meta) as outds:\n",
    "                        outds.write(data)\n",
    "    print(f\"Processing for {label} completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for tile_number in TILENUMBER:\n",
    "        tif_path = f\"{THEFOLDER}\\\\PCLTILES\\\\pcltile_{tile_number}.tif\"\n",
    "        tif_dir = f\"{THEFOLDER}\\\\INFERENCETILES2\\\\{tile_number}\"\n",
    "        os.makedirs(tif_dir, exist_ok=True)\n",
    "\n",
    "        resampled_files = [\n",
    "            process_dem(tif_path, tif_dir, tile_number),\n",
    "            process_lidar(tif_path, tif_dir, tile_number),\n",
    "            process_rivers(tif_path, tif_dir, tile_number),\n",
    "            process_roads(tif_path, tif_dir, tile_number)\n",
    "        ]\n",
    "\n",
    "        delete_non_resampled_files([os.path.basename(f) for f in resampled_files], tif_dir)\n",
    "\n",
    "        # Define input files as a dictionary\n",
    "        input_files = {\n",
    "            'lidar': f'output_resampled_dataLIDAR_{tile_number}.tif',\n",
    "            'dem': f'output_resampled_dataDEM_{tile_number}.tif',\n",
    "            'roads': f'output_resampled_dataRoads_{tile_number}.tif',\n",
    "            'rivers': f'output_resampled_dataRivers_{tile_number}.tif'\n",
    "        }\n",
    "        output_filename = 'tile_{}-{}.tif'\n",
    "\n",
    "        # Define the base output path\n",
    "        out_base_path = f\"{THEFOLDER}\\\\INFERENCETILES3\"\n",
    "        os.makedirs(out_base_path, exist_ok=True)\n",
    "\n",
    "        # Process each file\n",
    "        for label, filename in input_files.items():\n",
    "            input_filepath = os.path.join(tif_dir, filename)\n",
    "            output_folder = os.path.join(out_base_path, label)\n",
    "            os.makedirs(output_folder, exist_ok=True)\n",
    "            process_file(label, input_filepath, output_folder)\n",
    "\n",
    "        print(f\"Processing for tile {tile_number} completed.\")\n",
    "\n",
    "    print(\"All processing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bbe760-08e9-4d27-941f-cba20154f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Define the directory path\n",
    "#directory_path = 'C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\PCLCONUS\\\\Input\\\\inferencetiles\\\\hag'\n",
    "\n",
    "# Regular expression to extract the identifier part of the filename 'tile_{identifier}.tif'\n",
    "pattern = re.compile(r'tile_(\\d+-\\d+)\\.tif')\n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(output_folder)\n",
    "\n",
    "# Use a set to avoid duplicate identifiers\n",
    "identifiers = set()\n",
    "\n",
    "# Extract identifiers from filenames\n",
    "for file in files:\n",
    "    match = pattern.search(file)\n",
    "    if match:\n",
    "        identifiers.add(match.group(1))\n",
    "\n",
    "# Convert the set to a sorted list\n",
    "identifier_list = sorted(list(identifiers))\n",
    "print(len(identifier_list))\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faae5a32-d577-427f-8d96-1ec12b9c4051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hag_max = 30.0\n",
    "# dem_max = 4379.1279296875\n",
    "# roads_max = 1.0\n",
    "# rivers_max = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68eb199-0bfb-4fa7-aa20-c6acae178492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#tilename = '0-0'\n",
    "# input_hag_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\hag\\\\tile_{tilename}.tif\"\n",
    "# input_dem_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\dem\\\\tile_{tilename}.tif\"\n",
    "# input_roads_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\roads\\\\tile_{tilename}.tif\"\n",
    "# input_rivers_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\rivers\\\\tile_{tilename}.tif\"\n",
    "\n",
    "def load_and_preprocess_image(hag_path, dem_path, roads_path, rivers_path):\n",
    "    with rasterio.open(hag_path) as src:\n",
    "        hag_image = src.read(1)\n",
    "    with rasterio.open(dem_path) as src:\n",
    "        dem_image = src.read(1)\n",
    "    with rasterio.open(roads_path) as src:\n",
    "        roads_image = src.read(1)\n",
    "    with rasterio.open(rivers_path) as src:\n",
    "        rivers_image = src.read(1)\n",
    "\n",
    "    # Normalize and stack the images\n",
    "    hag_image = np.array(hag_image).astype('float32') / hag_max\n",
    "    dem_image = np.array(dem_image).astype('float32') / dem_max\n",
    "    roads_image = np.array(roads_image).astype('float32') / roads_max\n",
    "    rivers_image = np.array(rivers_image).astype('float32') / rivers_max\n",
    "\n",
    "    # Stack images along the last dimension\n",
    "    combined_image = np.stack([hag_image, dem_image, roads_image, rivers_image], axis=-1)\n",
    "\n",
    "    # Add batch dimension\n",
    "    combined_image = np.expand_dims(combined_image, axis=0)\n",
    "    return combined_image\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(identifier_list)):\n",
    "    tilename = identifier_list[i]\n",
    "    #print(tilename)\n",
    "    #input_hag_path = f\"{out_base_path}\\\\lidar\\\\tile_{tilename}.tif\"\n",
    "    #input_dem_path = f\"{out_base_path}\\\\dem\\\\tile_{tilename}.tif\"\n",
    "    #input_roads_path = f\"{out_base_path}\\\\roads\\\\tile_{tilename}.tif\"\n",
    "    #input_rivers_path = f\"{out_base_path}\\\\tile_{tilename}.tif\"\n",
    "\n",
    "    input_hag_path = os.path.join(out_base_path, \"lidar\", f\"tile_{tilename}.tif\")\n",
    "    input_dem_path = os.path.join(out_base_path, \"dem\", f\"tile_{tilename}.tif\")\n",
    "    input_roads_path = os.path.join(out_base_path, \"roads\", f\"tile_{tilename}.tif\")\n",
    "    input_rivers_path = os.path.join(out_base_path, \"rivers\", f\"tile_{tilename}.tif\")\n",
    "\n",
    "\n",
    "    input_image = load_and_preprocess_image(input_hag_path, input_dem_path, input_roads_path, input_rivers_path)\n",
    "    predicted_image = model.predict(input_image)\n",
    "    predicted_image = np.squeeze(predicted_image)\n",
    "    \n",
    "    # Debug print to check if all outputs are the same\n",
    "    #print(\"Unique values in predicted output:\", np.unique(predicted_image))\n",
    "    \n",
    "    # Adjust the scaling factor based on how the labels were scaled during training\n",
    "    predicted_image *= 100\n",
    "    \n",
    "    #output_image_path = f\"{THEFOLDER}\\\\predictions\\\\predicted_tile_{tilename}.tif\"\n",
    "    predictions_folder = os.path.join(THEFOLDER, \"predictions3\")\n",
    "    os.makedirs(predictions_folder, exist_ok=True)\n",
    "    output_image_path = os.path.join(predictions_folder, f\"predicted_tile_{tilename}.tif\")\n",
    "\n",
    "    \n",
    "    with rasterio.open(input_dem_path) as src: \n",
    "        profile = src.profile\n",
    "    \n",
    "    with rasterio.open(output_image_path, 'w', **profile) as dst:\n",
    "        dst.write(predicted_image.astype(rasterio.uint8), 1)\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b158d6d-6f99-48bb-806c-62572da2a9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "\n",
    "# Define the base folder and output paths\n",
    "#THEFOLDER = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GlobalPCL\"\n",
    "predictions_folder = os.path.join(THEFOLDER, \"predictions3\")\n",
    "output_dir = os.path.join(THEFOLDER, \"mergedoutput\")\n",
    "\n",
    "# Create necessary directories if they don't exist\n",
    "os.makedirs(predictions_folder, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_base_name = \"predMerged_\"  # Base name for output files\n",
    "\n",
    "# Get a list of TIFF files\n",
    "tifs = glob.glob(os.path.join(predictions_folder, \"*.tif\"))\n",
    "\n",
    "# Define chunk size for processing\n",
    "chunk_size = 300\n",
    "\n",
    "# Calculate the number of chunks needed\n",
    "num_chunks = len(tifs) // chunk_size\n",
    "if len(tifs) % chunk_size != 0:\n",
    "    num_chunks += 1  # Add one more chunk for the remaining files\n",
    "\n",
    "# Loop through the TIFF files in chunks\n",
    "for chunk_id in range(num_chunks):\n",
    "    start_idx = chunk_id * chunk_size\n",
    "    end_idx = min((chunk_id + 1) * chunk_size, len(tifs))\n",
    "    chunk_tifs = tifs[start_idx:end_idx]\n",
    "    \n",
    "    output_tif = os.path.join(output_dir, f\"{output_base_name}{chunk_id + 1}.tif\")\n",
    "\n",
    "    merge_command_hag = [\n",
    "        \"python\",\n",
    "        \"C:\\\\Users\\\\smdur\\\\anaconda3\\\\envs\\\\globalpcl\\\\Scripts\\\\gdal_merge.py\",\n",
    "        \"--config\", \"CHECK_DISK_FREE_SPACE\", \"FALSE\",\n",
    "        \"-o\", output_tif,\n",
    "        \"-n\", \"-9999\",\n",
    "        \"-a_nodata\", \"-9999\",\n",
    "    ] + chunk_tifs\n",
    "\n",
    "    # Run the gdal_merge command for the current chunk\n",
    "    process_hag = subprocess.run(merge_command_hag, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    # Check if the command for the current chunk was successful\n",
    "    if process_hag.returncode != 0:\n",
    "        # An error occurred, print the error\n",
    "        print(f\"Error occurred while merging TIFF files for chunk {chunk_id + 1}:\")\n",
    "        print(process_hag.stderr)\n",
    "    else:\n",
    "        print(f\"TIFF files merged successfully for chunk {chunk_id + 1}. Output: {output_tif}\")\n",
    "\n",
    "# Merge all chunks into a final output file\n",
    "final_output_tif = os.path.join(THEFOLDER, \"FINALOUTPUTTILES\", f\"predMerged_PCL_{TILENUMBER}.tif\")\n",
    "os.makedirs(os.path.dirname(final_output_tif), exist_ok=True)\n",
    "\n",
    "chunk_tifs = glob.glob(os.path.join(output_dir, \"*.tif\"))\n",
    "\n",
    "merge_command_final = [\n",
    "    \"python\",\n",
    "    \"C:\\\\Users\\\\smdur\\\\anaconda3\\\\envs\\\\globalpcl\\\\Scripts\\\\gdal_merge.py\",\n",
    "    \"--config\", \"CHECK_DISK_FREE_SPACE\", \"FALSE\",\n",
    "    \"-o\", final_output_tif,\n",
    "    \"-n\", \"-9999\",\n",
    "    \"-a_nodata\", \"-9999\",\n",
    "] + chunk_tifs\n",
    "\n",
    "# Run the gdal_merge command for the final merge\n",
    "process_final = subprocess.run(merge_command_final, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "# Check if the command for the final merge was successful\n",
    "if process_final.returncode != 0:\n",
    "    # An error occurred, print the error\n",
    "    print(\"Error occurred while merging final TIFF files:\")\n",
    "    print(process_final.stderr)\n",
    "else:\n",
    "    print(\"Final TIFF files merged successfully.\")\n",
    "\n",
    "# Clean up temporary chunk files\n",
    "for tif in chunk_tifs:\n",
    "    try:\n",
    "        os.remove(tif)\n",
    "        print(f\"Deleted {tif}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to delete {tif}: {e}\")\n",
    "\n",
    "print(\"Done\")\n",
    "\n",
    "import shutil\n",
    "\n",
    "if os.path.exists(predictions_folder):\n",
    "    shutil.rmtree(predictions_folder)\n",
    "os.makedirs(predictions_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780d7821-1511-426b-b2a0-43160359c7f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
