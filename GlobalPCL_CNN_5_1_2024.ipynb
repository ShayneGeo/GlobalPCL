{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d8fa8e-b4c8-4e72-a960-48136c30a96d",
   "metadata": {},
   "source": [
    "# Tile Conus PCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dac025b-fb9a-4380-80c4-8f1edde7cf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import product\n",
    "import rasterio as rio\n",
    "from rasterio import windows\n",
    "\n",
    "in_path = \"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\PCLCONUS\\\\Input\\\\PCL\\\\\"\n",
    "input_filename = 'pcl_west_wgs.tif'\n",
    "\n",
    "out_path = \"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\PCLCONUS\\\\Input\\\\PCLTILES\\\\\"\n",
    "output_filename = 'pcltile_{}-{}.tif'\n",
    "\n",
    "widthtile = 5000\n",
    "heighttile = 5000\n",
    "\n",
    "def get_tiles(ds, width=widthtile, height=heighttile):\n",
    "    nols, nrows = ds.meta['width'], ds.meta['height']\n",
    "    offsets = product(range(0, nols, width), range(0, nrows, height))\n",
    "    big_window = windows.Window(col_off=0, row_off=0, width=nols, height=nrows)\n",
    "    for col_off, row_off in  offsets:\n",
    "        window =windows.Window(col_off=col_off, row_off=row_off, width=width, height=height).intersection(big_window)\n",
    "        transform = windows.transform(window, ds.transform)\n",
    "        yield window, transform\n",
    "                   \n",
    "with rio.open(os.path.join(in_path, input_filename)) as inds:\n",
    "    tile_width, tile_height = widthtile, heighttile\n",
    "    nodata = inds.nodata  \n",
    "    meta = inds.meta.copy()\n",
    "    for window, transform in get_tiles(inds):\n",
    "        print(window)\n",
    "        data = inds.read(window=window)\n",
    "        if nodata is not None and not (data == nodata).all():\n",
    "            meta['transform'] = transform\n",
    "            meta['width'], meta['height'] = window.width, window.height\n",
    "            outpath = os.path.join(out_path, output_filename.format(int(window.col_off), int(window.row_off)))\n",
    "            with rio.open(outpath, 'w', **meta) as outds:\n",
    "                outds.write(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe035094-d0ab-49a2-9aac-b1b92b7175aa",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b28d4e1-1619-4262-a6e6-e65a76813ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### for DEM\n",
    "import rioxarray\n",
    "from pystac_client import Client\n",
    "import planetary_computer\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import rioxarray\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "from osgeo import gdal, ogr, osr\n",
    "\n",
    "# Path to the GeoTIFF file\n",
    "tif_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\PCLCONUS\\Input\\PCLTILES\\pcltile_75000-35000.tif\"\n",
    "\n",
    "# Directory for TIFF files\n",
    "tif_dir = r'C:\\Users\\smdur\\OneDrive\\Desktop\\PCLCONUS\\Input\\scratch\\dem'\n",
    "\n",
    "# Read the GeoTIFF file using rioxarray\n",
    "tif_data = rioxarray.open_rasterio(tif_path)\n",
    "\n",
    "# Get the spatial extent of the GeoTIFF\n",
    "min_lon, min_lat, max_lon, max_lat = tif_data.rio.bounds()\n",
    "bbox_of_interest = [min_lon, min_lat, max_lon, max_lat]  # Replace with your actual bounding box coordinates\n",
    "\n",
    "# Create a catalog search for the specified collection and bounding box\n",
    "catalog = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
    "search = catalog.search(collections=[\"cop-dem-glo-30\"], bbox=bbox_of_interest)\n",
    "items = list(search.get_items())\n",
    "print(f\"Returned {len(items)} items\")\n",
    "\n",
    "def process_item(item, idx):\n",
    "    #print(f\"Processing item {idx+1}/{len(items)}\")\n",
    "    signed_asset = planetary_computer.sign(item.assets[\"data\"])\n",
    "    data = (\n",
    "        rioxarray.open_rasterio(signed_asset.href)\n",
    "        .squeeze()\n",
    "        .drop(\"band\")\n",
    "    )\n",
    "    data.rio.write_crs(\"EPSG:4326\", inplace=True)\n",
    "\n",
    "    output_tif_path = os.path.join(tif_dir, f\"output_dataDEM_{idx}.tif\")\n",
    "    data.rio.to_raster(output_tif_path)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:  # Adjust max_workers based on your CPU\n",
    "    for i, item in enumerate(items):  # Change this line to process only the last three items\n",
    "        executor.submit(process_item, item, len(items) - 3 + i)  # Adjust index for correct labeling\n",
    "\n",
    "output_tif = os.path.join(tif_dir, f\"outputtile_DEM.tif\")\n",
    "\n",
    "# Get the list of TIFF files\n",
    "tifs = glob.glob(os.path.join(tif_dir, \"*.tif\"))\n",
    "\n",
    "# Prepare the gdal_merge command\n",
    "merge_command_hag = [\n",
    "    \"python\",\n",
    "    \"C:\\\\Users\\\\smdur\\\\anaconda3\\\\envs\\\\globalpcl\\\\Scripts\\\\gdal_merge.py\",\n",
    "    \"--config\", \"CHECK_DISK_FREE_SPACE\", \"FALSE\",\n",
    "    \"-o\", output_tif,\n",
    "    \"-n\", \"-9999\",\n",
    "    \"-a_nodata\", \"-9999\"\n",
    "] + tifs\n",
    "\n",
    "# Run the gdal_merge command and capture the output\n",
    "process_hag = subprocess.run(merge_command_hag, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "\n",
    "\n",
    "# print(output_tif)\n",
    "# print(tif_path)\n",
    "# output_tif = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\DEMTIF\\DEM_CO.tif\"\n",
    "# tif_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\PCLCONUS\\Input\\PCLTILES\\pcltile_75000-35000.tif\"\n",
    "\n",
    "# Open the source (lidarHAG) and target (pcl) datasets\n",
    "src_ds = gdal.Open(output_tif, gdal.GA_ReadOnly)\n",
    "target_ds = gdal.Open(tif_path, gdal.GA_ReadOnly)\n",
    "\n",
    "# Get the Geotransform and Projection from the target dataset\n",
    "target_transform = target_ds.GetGeoTransform()\n",
    "target_projection = target_ds.GetProjection()\n",
    "target_cols = target_ds.RasterXSize\n",
    "target_rows = target_ds.RasterYSize\n",
    "\n",
    "# Create a new dataset for output with the same size and projection as the target\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "output_path = os.path.join(tif_dir, f\"output_resampled_dataDEM.tif\")\n",
    "\n",
    "out_ds = driver.Create(output_path, target_cols, target_rows, 1, src_ds.GetRasterBand(1).DataType)\n",
    "out_ds.SetGeoTransform(target_transform)\n",
    "out_ds.SetProjection(target_projection)\n",
    "\n",
    "# Perform the resampling\n",
    "gdal.ReprojectImage(src_ds, out_ds, src_ds.GetProjection(), target_projection, gdal.GRA_Bilinear)\n",
    "\n",
    "src_ds = None\n",
    "target_ds = None\n",
    "out_ds = None\n",
    "\n",
    "os.remove(output_tif)\n",
    "\n",
    "if process_hag.returncode != 0:\n",
    "    print(process_hag.stderr)\n",
    "else:\n",
    "    for tif in tifs:\n",
    "        try:\n",
    "            os.remove(tif)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {tif}: {e}\")\n",
    "\n",
    "print(\"Resampling completed. Output saved at:\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fdb49b-7686-4dcf-af94-9a27b5ccd552",
   "metadata": {},
   "outputs": [],
   "source": [
    "### for LIDAR\n",
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "from osgeo import gdal, ogr, osr\n",
    "\n",
    "tif_dir = r'C:\\Users\\smdur\\OneDrive\\Desktop\\PCLCONUS\\Input\\scratch\\lidar'\n",
    "\n",
    "tifs = glob.glob(os.path.join(tif_dir, \"*.tif\"))\n",
    "\n",
    "output_tif = os.path.join(tif_dir, f\"outputtile_lidar.tif\")\n",
    "\n",
    "# Get the list of TIFF files\n",
    "tifs = glob.glob(os.path.join(tif_dir, \"*.tif\"))\n",
    "\n",
    "# Prepare the gdal_merge command\n",
    "merge_command_hag = [\n",
    "    \"python\",\n",
    "    \"C:\\\\Users\\\\smdur\\\\anaconda3\\\\envs\\\\globalpcl\\\\Scripts\\\\gdal_merge.py\",\n",
    "    \"--config\", \"CHECK_DISK_FREE_SPACE\", \"FALSE\",\n",
    "    \"-o\", output_tif,\n",
    "    \"-n\", \"255\",\n",
    "    \"-a_nodata\", \"255\"\n",
    "] + tifs\n",
    "\n",
    "# Run the gdal_merge command and capture the output\n",
    "process_hag = subprocess.run(merge_command_hag, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "# Open the source (lidarHAG) and target (pcl) datasets\n",
    "src_ds = gdal.Open(output_tif, gdal.GA_ReadOnly)\n",
    "target_ds = gdal.Open(tif_path, gdal.GA_ReadOnly)\n",
    "\n",
    "# Get the Geotransform and Projection from the target dataset\n",
    "target_transform = target_ds.GetGeoTransform()\n",
    "target_projection = target_ds.GetProjection()\n",
    "target_cols = target_ds.RasterXSize\n",
    "target_rows = target_ds.RasterYSize\n",
    "\n",
    "# Create a new dataset for output with the same size and projection as the target\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "#output_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\CNNPCLDEMO\\Input\\Resampled_DEM_CO.tif\"\n",
    "output_path = os.path.join(tif_dir, f\"output_resampled_dataLIDAR.tif\")\n",
    "\n",
    "out_ds = driver.Create(output_path, target_cols, target_rows, 1, src_ds.GetRasterBand(1).DataType)\n",
    "out_ds.SetGeoTransform(target_transform)\n",
    "out_ds.SetProjection(target_projection)\n",
    "\n",
    "# Perform the resampling\n",
    "gdal.ReprojectImage(src_ds, out_ds, src_ds.GetProjection(), target_projection, gdal.GRA_Bilinear)\n",
    "\n",
    "# Close the datasets\n",
    "src_ds = None\n",
    "target_ds = None\n",
    "out_ds = None\n",
    "\n",
    "os.remove(output_tif)\n",
    "\n",
    "print(\"Resampling completed. Output saved at:\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d25ad6-eb23-4b31-9e26-ab7f2f3755e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Rivers\n",
    "import rioxarray\n",
    "import osmnx as ox\n",
    "\n",
    "# Path to the GeoTIFF file\n",
    "tif_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\PCLCONUS\\Input\\PCLTILES\\pcltile_75000-35000.tif\"\n",
    "\n",
    "# Directory for TIFF files\n",
    "#tif_dir = r'C:\\Users\\smdur\\OneDrive\\Desktop\\PCLCONUS\\Input\\scratch\\dem'\n",
    "\n",
    "# Read the GeoTIFF file using rioxarray\n",
    "dem_data = rioxarray.open_rasterio(tif_path)\n",
    "\n",
    "# Get the spatial extent of the GeoTIFF\n",
    "min_lon, min_lat, max_lon, max_lat = dem_data.rio.bounds()\n",
    "\n",
    "#custom_filter = '[\"waterway\"~\"river|streams|canals\"]'\n",
    "custom_filter = '[\"waterway\"~\"river\"]'\n",
    "\n",
    "bbox = (max_lat, min_lat, max_lon, min_lon)  \n",
    "graph = ox.graph_from_bbox(bbox=bbox, custom_filter=custom_filter, simplify=True, retain_all=True, truncate_by_edge=True)\n",
    "\n",
    "\n",
    "gdf = ox.graph_to_gdfs(graph, nodes=False)\n",
    "\n",
    "# Convert the graph to GeoDataFrames\n",
    "nodes, edges = ox.graph_to_gdfs(graph)\n",
    "\n",
    "import rasterio\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.windows import from_bounds\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "\n",
    "# Define the decay constant\n",
    "lambda_decay = 0.03  # This value might need adjustment based on your specific use case\n",
    "\n",
    "# Load the reference TIFF to use for transformations and metadata\n",
    "with rasterio.open(tif_path) as src:\n",
    "\n",
    "    min_lon, min_lat, max_lon, max_lat = src.bounds\n",
    "    \n",
    "    meta = src.meta.copy()\n",
    "\n",
    "    # Convert geographic coordinates to pixel coordinates\n",
    "    window = from_bounds(min_lon, min_lat, max_lon, max_lat, src.transform)\n",
    "    transform = rasterio.windows.transform(window, src.transform)\n",
    "\n",
    "    # Correct the window dimensions to integers\n",
    "    window_width = int(window.width)\n",
    "    window_height = int(window.height)\n",
    "\n",
    "    # Create an empty raster array for the size of the window\n",
    "    raster = np.zeros((window_height, window_width), dtype=np.uint8)\n",
    "\n",
    "    # Rasterize directly into the window shape\n",
    "    shapes = ((geom, 1) for geom in gdf['geometry'])\n",
    "    burned = rasterize(shapes, out=raster, fill=0, transform=transform, all_touched=True)\n",
    "\n",
    "    # Compute the Euclidean distance from each cell to the nearest non-zero cell\n",
    "    distance_grid = scipy.ndimage.distance_transform_edt(burned == 0)\n",
    "\n",
    "    # Apply exponential decay to the distance grid\n",
    "    decay_grid = np.exp(-lambda_decay * distance_grid)\n",
    "\n",
    "    # Prepare metadata for saving the decay grid\n",
    "    clipped_meta = meta.copy()\n",
    "    clipped_meta.update({\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": window_height,\n",
    "        \"width\": window_width,\n",
    "        \"transform\": transform,\n",
    "        \"dtype\": rasterio.float32,\n",
    "        \"count\": 1,\n",
    "        \"compress\": 'lzw'\n",
    "    })\n",
    "\n",
    "    # Save the decay grid to a new TIFF file\n",
    "    output_path = r'C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\PCLCONUS\\\\Input\\\\scratch\\\\rivers\\\\exponential_decay_CO_river.tif'\n",
    "    with rasterio.open(output_path, 'w', **clipped_meta) as dst:\n",
    "        dst.write(decay_grid.astype(np.float32), 1)\n",
    "\n",
    "from osgeo import gdal, ogr, osr\n",
    "\n",
    "# Define the paths to your rasters\n",
    "#lidarHAG_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\Input\\exponential_decay_CO_river.tif\"\n",
    "#pcl_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\Input\\pcl_west_wgs_CO_3.tif\"\n",
    "\n",
    "# Open the source (lidarHAG) and target (pcl) datasets\n",
    "src_ds = gdal.Open(output_path, gdal.GA_ReadOnly)\n",
    "target_ds = gdal.Open(tif_path, gdal.GA_ReadOnly)\n",
    "\n",
    "\n",
    "# Get the Geotransform and Projection from the target dataset\n",
    "target_transform = target_ds.GetGeoTransform()\n",
    "target_projection = target_ds.GetProjection()\n",
    "target_cols = target_ds.RasterXSize\n",
    "target_rows = target_ds.RasterYSize\n",
    "\n",
    "# Create a new dataset for output with the same size and projection as the target\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "output_path_resample = r\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\PCLCONUS\\\\Input\\\\scratch\\\\rivers\\\\output_resampled_dataRivers.tif\"\n",
    "out_ds = driver.Create(output_path_resample, target_cols, target_rows, 1, src_ds.GetRasterBand(1).DataType)\n",
    "out_ds.SetGeoTransform(target_transform)\n",
    "out_ds.SetProjection(target_projection)\n",
    "\n",
    "# Perform the resampling\n",
    "gdal.ReprojectImage(src_ds, out_ds, src_ds.GetProjection(), target_projection, gdal.GRA_Bilinear)\n",
    "\n",
    "# Close the datasets\n",
    "src_ds = None\n",
    "target_ds = None\n",
    "out_ds = None\n",
    "\n",
    "print(\"Resampling completed. Output saved at:\", output_path)\n",
    "\n",
    "# Check if the file exists before trying to delete it\n",
    "if os.path.exists(output_path):\n",
    "    try:\n",
    "        os.remove(output_path)\n",
    "        print(f\"File {output_path} deleted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to delete {output_path}: {e}\")\n",
    "else:\n",
    "    print(f\"No file found at {output_path}, deletion not required.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b736ba3e-7733-40a2-99eb-1bd23df4960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Roads\n",
    "import rioxarray\n",
    "# Path to the GeoTIFF file\n",
    "tif_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\PCLCONUS\\Input\\PCLTILES\\pcltile_75000-35000.tif\"\n",
    "\n",
    "# Directory for TIFF files\n",
    "#tif_dir = r'C:\\Users\\smdur\\OneDrive\\Desktop\\PCLCONUS\\Input\\scratch\\dem'\n",
    "\n",
    "# Read the GeoTIFF file using rioxarray\n",
    "extent_data = rioxarray.open_rasterio(tif_path)\n",
    "\n",
    "# Get the spatial extent of the GeoTIFF\n",
    "min_lon, min_lat, max_lon, max_lat = extent_data.rio.bounds()\n",
    "\n",
    "graph = ox.graph_from_bbox(max_lat, min_lat, max_lon, min_lon, network_type='drive', simplify=True)\n",
    "\n",
    "gdf = ox.graph_to_gdfs(graph, nodes=False)\n",
    "\n",
    "# Convert the graph to GeoDataFrames\n",
    "nodes, edges = ox.graph_to_gdfs(graph)\n",
    "\n",
    "# Plot the waterway network\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "edges.plot(ax=ax, linewidth=1, edgecolor='blue')\n",
    "#plt.title('Waterways in Fort Collins')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import rasterio\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.windows import from_bounds\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "\n",
    "# Define the decay constant\n",
    "lambda_decay = 0.03  # This value might need adjustment based on your specific use case\n",
    "\n",
    "# Load the reference TIFF to use for transformations and metadata\n",
    "with rasterio.open(tif_path) as src:\n",
    "\n",
    "    min_lon, min_lat, max_lon, max_lat = src.bounds\n",
    "    \n",
    "    meta = src.meta.copy()\n",
    "\n",
    "    # Convert geographic coordinates to pixel coordinates\n",
    "    window = from_bounds(min_lon, min_lat, max_lon, max_lat, src.transform)\n",
    "    transform = rasterio.windows.transform(window, src.transform)\n",
    "\n",
    "    # Correct the window dimensions to integers\n",
    "    window_width = int(window.width)\n",
    "    window_height = int(window.height)\n",
    "\n",
    "    # Create an empty raster array for the size of the window\n",
    "    raster = np.zeros((window_height, window_width), dtype=np.uint8)\n",
    "\n",
    "    # Rasterize directly into the window shape\n",
    "    shapes = ((geom, 1) for geom in gdf['geometry'])\n",
    "    burned = rasterize(shapes, out=raster, fill=0, transform=transform, all_touched=True)\n",
    "\n",
    "    # Compute the Euclidean distance from each cell to the nearest non-zero cell\n",
    "    distance_grid = scipy.ndimage.distance_transform_edt(burned == 0)\n",
    "\n",
    "    # Apply exponential decay to the distance grid\n",
    "    decay_grid = np.exp(-lambda_decay * distance_grid)\n",
    "\n",
    "    # Prepare metadata for saving the decay grid\n",
    "    clipped_meta = meta.copy()\n",
    "    clipped_meta.update({\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": window_height,\n",
    "        \"width\": window_width,\n",
    "        \"transform\": transform,\n",
    "        \"dtype\": rasterio.float32,\n",
    "        \"count\": 1,\n",
    "        \"compress\": 'lzw'\n",
    "    })\n",
    "\n",
    "    # Save the decay grid to a new TIFF file\n",
    "    output_path = r'C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\PCLCONUS\\\\Input\\\\scratch\\\\roads\\\\exponential_decay_CO_roads.tif'\n",
    "    with rasterio.open(output_path, 'w', **clipped_meta) as dst:\n",
    "        dst.write(decay_grid.astype(np.float32), 1)\n",
    "\n",
    "from osgeo import gdal, ogr, osr\n",
    "\n",
    "# Define the paths to your rasters\n",
    "#lidarHAG_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\Input\\exponential_decay_CO_river.tif\"\n",
    "#pcl_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\Input\\pcl_west_wgs_CO_3.tif\"\n",
    "\n",
    "# Open the source (lidarHAG) and target (pcl) datasets\n",
    "src_ds = gdal.Open(output_path, gdal.GA_ReadOnly)\n",
    "target_ds = gdal.Open(tif_path, gdal.GA_ReadOnly)\n",
    "\n",
    "\n",
    "# Get the Geotransform and Projection from the target dataset\n",
    "target_transform = target_ds.GetGeoTransform()\n",
    "target_projection = target_ds.GetProjection()\n",
    "target_cols = target_ds.RasterXSize\n",
    "target_rows = target_ds.RasterYSize\n",
    "\n",
    "# Create a new dataset for output with the same size and projection as the target\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "output_path_resample = r\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\PCLCONUS\\\\Input\\\\scratch\\\\roads\\\\output_resampled_dataRoads.tif\"\n",
    "out_ds = driver.Create(output_path_resample, target_cols, target_rows, 1, src_ds.GetRasterBand(1).DataType)\n",
    "out_ds.SetGeoTransform(target_transform)\n",
    "out_ds.SetProjection(target_projection)\n",
    "\n",
    "# Perform the resampling\n",
    "gdal.ReprojectImage(src_ds, out_ds, src_ds.GetProjection(), target_projection, gdal.GRA_Bilinear)\n",
    "\n",
    "# Close the datasets\n",
    "src_ds = None\n",
    "target_ds = None\n",
    "out_ds = None\n",
    "\n",
    "print(\"Resampling completed. Output saved at:\", output_path)\n",
    "\n",
    "# Check if the file exists before trying to delete it\n",
    "if os.path.exists(output_path):\n",
    "    try:\n",
    "        os.remove(output_path)\n",
    "        print(f\"File {output_path} deleted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to delete {output_path}: {e}\")\n",
    "else:\n",
    "    print(f\"No file found at {output_path}, deletion not required.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a223bfc4-d483-47cf-af67-2bab2dc4f318",
   "metadata": {},
   "outputs": [],
   "source": [
    "### for Sentinel\n",
    "import rioxarray\n",
    "from pystac_client import Client\n",
    "import planetary_computer\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import rioxarray\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "from osgeo import gdal, ogr, osr\n",
    "from pystac.extensions.eo import EOExtension as eo\n",
    "import pyproj\n",
    "import stackstac\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "# Path to the GeoTIFF file\n",
    "tif_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\PCLCONUS\\Input\\PCLTILES\\pcltile_75000-40000.tif\"\n",
    "\n",
    "# Directory for TIFF files\n",
    "tif_dir = r'C:\\Users\\smdur\\OneDrive\\Desktop\\scratch'\n",
    "\n",
    "# Read the GeoTIFF file using rioxarray\n",
    "tif_data = rioxarray.open_rasterio(tif_path)\n",
    "\n",
    "# Get the spatial extent of the GeoTIFF\n",
    "min_lon, min_lat, max_lon, max_lat = tif_data.rio.bounds()\n",
    "bbox_of_interest = [min_lon, min_lat, max_lon, max_lat] \n",
    "\n",
    "# Create a catalog search for the specified collection and bounding box\n",
    "catalog = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
    "\n",
    "time_of_interest = \"2021-07-01/2021-08-01\"\n",
    "\n",
    "search = catalog.search(\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    bbox=bbox_of_interest,\n",
    "    datetime=time_of_interest,\n",
    "    query={\"eo:cloud_cover\": {\"lt\": 10}},\n",
    ")\n",
    "items = list(search.get_items())\n",
    "print(f\"Returned {len(items)} items\")\n",
    "\n",
    "items = search.item_collection()\n",
    "\n",
    "base_crs = tif_data.rio.crs\n",
    "base_transform = tif_data.rio.transform()\n",
    "base_resolution = (tif_data.rio.width, tif_data.rio.height)\n",
    "print(\"export\")\n",
    "\n",
    "# def process_item(item, idx):\n",
    "#     signed_asset1 = planetary_computer.sign(item.assets[\"B08\"])\n",
    "#     data1 = rioxarray.open_rasterio(signed_asset1.href).squeeze().drop(\"band\")\n",
    "#     output_tif_path1 = os.path.join(tif_dir, f\"SenDataBand8_data_{idx}.tif\")\n",
    "#     data1.rio.to_raster(output_tif_path1)\n",
    "\n",
    "#     signed_asset2 = planetary_computer.sign(item.assets[\"B04\"])\n",
    "#     data2 = rioxarray.open_rasterio(signed_asset2.href).squeeze().drop(\"band\")\n",
    "#     output_tif_path2 = os.path.join(tif_dir, f\"SenDataBand4_data_{idx}.tif\")\n",
    "#     data2.rio.to_raster(output_tif_path2)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def process_item(item, idx):\n",
    "    # Sign the assets using the planetary_computer token\n",
    "    signed_asset1 = planetary_computer.sign(item.assets[\"B08\"])\n",
    "    signed_asset2 = planetary_computer.sign(item.assets[\"B04\"])\n",
    "    \n",
    "    # Open the raster data and drop the band dimension\n",
    "    data1 = rioxarray.open_rasterio(signed_asset1.href).squeeze().drop(\"band\")\n",
    "    data2 = rioxarray.open_rasterio(signed_asset2.href).squeeze().drop(\"band\")\n",
    "    \n",
    "    output_tif_path1 = os.path.join(tif_dir, f\"SSSSenDataBand8_data_{idx}.tif\")\n",
    "    output_tif_path2 = os.path.join(tif_dir, f\"SSSSenDataBand4_data_{idx}.tif\")\n",
    "    \n",
    "    # Check for non-zero data in the first dataset\n",
    "    if np.any(data1.values != 0):\n",
    "        print(f\"Data for Band 8 at index {idx} contains non-zero values.\")\n",
    "        data1.rio.to_raster(output_tif_path1)  # Uncomment this to save when needed\n",
    "    else:\n",
    "        print(f\"Skipping saving Band 8 data at index {idx} due to only zero values.\")\n",
    "\n",
    "    # Check for non-zero data in the second dataset\n",
    "    if np.any(data2.values != 0):\n",
    "        print(f\"Data for Band 4 at index {idx} contains non-zero values.\")\n",
    "        data2.rio.to_raster(output_tif_path2)  # Uncomment this to save when needed\n",
    "    else:\n",
    "        print(f\"Skipping saving Band 4 data at index {idx} due to only zero values.\")\n",
    "\n",
    "\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:  # Adjust max_workers based on your CPU\n",
    "    for i, item in enumerate(items):  # Change this line to process only the last three items\n",
    "        executor.submit(process_item, item, len(items) - 3 + i)  # Adjust index for correct labeling\n",
    "        gc.collect()\n",
    "        \n",
    "print('merge')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import os\n",
    "# import glob\n",
    "# import numpy as np\n",
    "# from osgeo import gdal\n",
    "\n",
    "# def read_raster(file_path):\n",
    "#     return gdal.Open(file_path, gdal.GA_ReadOnly)\n",
    "\n",
    "# def create_output_raster(template_ds, output_path):\n",
    "#     driver = gdal.GetDriverByName('GTiff')\n",
    "#     out_ds = driver.CreateCopy(output_path, template_ds, 0)\n",
    "#     out_ds.GetRasterBand(1).Fill(0)  # Initialize with the lowest value expected\n",
    "#     return out_ds\n",
    "\n",
    "# tif_dir = \"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\scratch\\\\\"\n",
    "# output_tif_path = os.path.join(tif_dir, \"median_merged_0check.tif\")\n",
    "\n",
    "# # Get the list of TIFF files\n",
    "# tif_files = glob.glob(os.path.join(tif_dir, \"SenDataBand8_data_*.tif\"))\n",
    "# if not tif_files:\n",
    "#     raise FileNotFoundError(\"No TIFF files found in the specified directory.\")\n",
    "\n",
    "# # Initialize the output raster using the first TIFF as a template\n",
    "# template_ds = read_raster(tif_files[0])\n",
    "# output_ds = create_output_raster(template_ds, output_tif_path)\n",
    "# output_band = output_ds.GetRasterBand(1)\n",
    "\n",
    "# # Read data from all TIFFs\n",
    "# data_arrays = []\n",
    "# for tif_path in tif_files:\n",
    "#     ds = read_raster(tif_path)\n",
    "#     if ds:\n",
    "#         band = ds.GetRasterBand(1)\n",
    "#         data = band.ReadAsArray()\n",
    "#         data_arrays.append(data)\n",
    "#     ds = None  # Ensure the file is closed after reading\n",
    "\n",
    "# # Stack arrays and compute the median\n",
    "# if data_arrays:\n",
    "#     stacked_data = np.dstack(data_arrays)  # Stack along the third dimension\n",
    "#     median_data = np.median(stacked_data, axis=2)  # Compute median along the third dimension\n",
    "\n",
    "#     # Write the result back to the output raster\n",
    "#     output_band.WriteArray(median_data)\n",
    "\n",
    "# Finalize\n",
    "output_band.FlushCache()\n",
    "output_ds = None  # Close and save the dataset\n",
    "print(f\"Merged TIFF saved to {output_tif_path}\")\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "\n",
    "def read_raster(file_path):\n",
    "    \"\"\"Opens a raster file for reading.\"\"\"\n",
    "    return gdal.Open(file_path, gdal.GA_ReadOnly)\n",
    "\n",
    "def create_output_raster(template_ds, output_path):\n",
    "    \"\"\"Creates a new raster file based on the template with initialization.\"\"\"\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    out_ds = driver.CreateCopy(output_path, template_ds, 0)\n",
    "    out_ds.GetRasterBand(1).Fill(0)  # Initialize with the lowest value expected\n",
    "    return out_ds\n",
    "\n",
    "tif_dir = \"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\scratch\\\\\"\n",
    "output_tif_path = os.path.join(tif_dir, \"median_merged_0check.tif\")\n",
    "\n",
    "# Get the list of TIFF files\n",
    "tif_files = glob.glob(os.path.join(tif_dir, \"SenDataBand8_data_*.tif\"))\n",
    "if not tif_files:\n",
    "    raise FileNotFoundError(\"No TIFF files found in the specified directory.\")\n",
    "\n",
    "# Initialize the output raster using the first TIFF as a template\n",
    "template_ds = read_raster(tif_files[0])\n",
    "output_ds = create_output_raster(template_ds, output_tif_path)\n",
    "output_band = output_ds.GetRasterBand(1)\n",
    "\n",
    "# Read data from all TIFFs and filter out all-zero files\n",
    "data_arrays = []\n",
    "for tif_path in tif_files:\n",
    "    ds = read_raster(tif_path)\n",
    "    if ds:\n",
    "        band = ds.GetRasterBand(1)\n",
    "        data = band.ReadAsArray()\n",
    "        if np.any(data != 0):  # Check if there's any non-zero data\n",
    "            data_arrays.append(data)\n",
    "        else:\n",
    "            print(f\"Skipped {tif_path} because it contains only zeros.\")\n",
    "        ds = None  # Ensure the file is closed after reading\n",
    "\n",
    "# Stack arrays and compute the median if there are valid datasets\n",
    "if data_arrays:\n",
    "    stacked_data = np.dstack(data_arrays)  # Stack along the third dimension\n",
    "    median_data = np.median(stacked_data, axis=2)  # Compute median along the third dimension\n",
    "    output_band.WriteArray(median_data)  # Write the result back to the output raster\n",
    "else:\n",
    "    print(\"No valid data found in any TIFF files.\")\n",
    "\n",
    "# Finalize\n",
    "output_band.FlushCache()\n",
    "output_ds = None  # Close and save the dataset\n",
    "print(f\"Merged TIFF saved to {output_tif_path}\")\n",
    "\n",
    "\n",
    "# ########################################################################################################################\n",
    "# ########################################################################################################################\n",
    "# ########################################################################################################################\n",
    "# tif_dir = \"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\scratch\\\\\"\n",
    "\n",
    "# output_tif1 = os.path.join(tif_dir, f\"outputtile_sentinel_8_3.tif\")\n",
    "# print(output_tif1)\n",
    "# # Get the list of TIFF files\n",
    "# tifs = glob.glob(os.path.join(tif_dir, \"SenDataBand8_data_*.tif\"))\n",
    "\n",
    "# # Prepare the gdal_merge command\n",
    "# merge_command_hag = [\n",
    "#     \"python\",\n",
    "#     \"C:\\\\Users\\\\smdur\\\\anaconda3\\\\envs\\\\globalpcl\\\\Scripts\\\\gdal_merge.py\",\n",
    "#     \"--config\", \"CHECK_DISK_FREE_SPACE\", \"FALSE\",\n",
    "#     \"-o\", output_tif1,\n",
    "#     #\"-n\", \"-9999\",\n",
    "#     #\"-a_nodata\", \"-9999\"\n",
    "#     \"-n\", \"0\",\n",
    "#     \"-a_nodata\", \"0\"\n",
    "# ] + tifs\n",
    "\n",
    "# # Run the gdal_merge command and capture the output\n",
    "# process_hag = subprocess.run(merge_command_hag, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "# # # Open the source (lidarHAG) and target (pcl) datasets\n",
    "# src_ds = gdal.Open(output_tif1, gdal.GA_ReadOnly)\n",
    "# target_ds = gdal.Open(tif_path, gdal.GA_ReadOnly)\n",
    "\n",
    "# # Get the Geotransform and Projection from the target dataset\n",
    "# target_transform = target_ds.GetGeoTransform()\n",
    "# target_projection = target_ds.GetProjection()\n",
    "# target_cols = target_ds.RasterXSize\n",
    "# target_rows = target_ds.RasterYSize\n",
    "\n",
    "# # Create a new dataset for output with the same size and projection as the target\n",
    "# driver = gdal.GetDriverByName('GTiff')\n",
    "# output_path1 = os.path.join(tif_dir, f\"output_resampled_dataSEN_8.tif\")\n",
    "\n",
    "# out_ds = driver.Create(output_path1, target_cols, target_rows, 1, src_ds.GetRasterBand(1).DataType)\n",
    "# out_ds.SetGeoTransform(target_transform)\n",
    "# out_ds.SetProjection(target_projection)\n",
    "\n",
    "# # Perform the resampling\n",
    "# gdal.ReprojectImage(src_ds, out_ds, src_ds.GetProjection(), target_projection, gdal.GRA_Bilinear)\n",
    "\n",
    "# src_ds = None\n",
    "# target_ds = None\n",
    "# out_ds = None\n",
    "\n",
    "# os.remove(output_tif1)\n",
    "\n",
    "# if process_hag.returncode != 0:\n",
    "#     print(process_hag.stderr)\n",
    "# else:\n",
    "#     for tif in tifs:\n",
    "#         try:\n",
    "#             os.remove(tif)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Failed to delete {tif}: {e}\")\n",
    "\n",
    "# print(\"Resampling completed. Output saved at:\", output_path1)\n",
    "\n",
    "# ########################################################################################################################\n",
    "# ########################################################################################################################\n",
    "# ########################################################################################################################\n",
    "# output_tif2 = os.path.join(tif_dir, f\"outputtile_sentinel_4.tif\")\n",
    "\n",
    "# # Get the list of TIFF files\n",
    "# tifs = glob.glob(os.path.join(tif_dir, \"SenDataBand4_data_*.tif\"))\n",
    "\n",
    "# # Prepare the gdal_merge command\n",
    "# merge_command_hag = [\n",
    "#     \"python\",\n",
    "#     \"C:\\\\Users\\\\smdur\\\\anaconda3\\\\envs\\\\globalpcl\\\\Scripts\\\\gdal_merge.py\",\n",
    "#     \"--config\", \"CHECK_DISK_FREE_SPACE\", \"FALSE\",\n",
    "#     \"-o\", output_tif2,\n",
    "#     #\"-n\", \"-9999\",\n",
    "#     #\"-a_nodata\", \"-9999\"\n",
    "#     \"-n\", \"0\",\n",
    "#     \"-a_nodata\", \"0\"\n",
    "# ] + tifs\n",
    "\n",
    "# # Run the gdal_merge command and capture the output\n",
    "# process_hag = subprocess.run(merge_command_hag, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "# # Open the source (lidarHAG) and target (pcl) datasets\n",
    "# src_ds = gdal.Open(output_tif2, gdal.GA_ReadOnly)\n",
    "# target_ds = gdal.Open(tif_path, gdal.GA_ReadOnly)\n",
    "\n",
    "# # Get the Geotransform and Projection from the target dataset\n",
    "# target_transform = target_ds.GetGeoTransform()\n",
    "# target_projection = target_ds.GetProjection()\n",
    "# target_cols = target_ds.RasterXSize\n",
    "# target_rows = target_ds.RasterYSize\n",
    "\n",
    "# # Create a new dataset for output with the same size and projection as the target\n",
    "# driver = gdal.GetDriverByName('GTiff')\n",
    "# output_path2 = os.path.join(tif_dir, f\"output_resampled_dataSEN_4.tif\")\n",
    "\n",
    "# out_ds = driver.Create(output_path2, target_cols, target_rows, 1, src_ds.GetRasterBand(1).DataType)\n",
    "# out_ds.SetGeoTransform(target_transform)\n",
    "# out_ds.SetProjection(target_projection)\n",
    "\n",
    "# # Perform the resampling\n",
    "# gdal.ReprojectImage(src_ds, out_ds, src_ds.GetProjection(), target_projection, gdal.GRA_Bilinear)\n",
    "\n",
    "# src_ds = None\n",
    "# target_ds = None\n",
    "# out_ds = None\n",
    "\n",
    "# os.remove(output_tif2)\n",
    "\n",
    "# if process_hag.returncode != 0:\n",
    "#     print(process_hag.stderr)\n",
    "# else:\n",
    "#     for tif in tifs:\n",
    "#         try:\n",
    "#             os.remove(tif)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Failed to delete {tif}: {e}\")\n",
    "\n",
    "# print(\"Resampling completed. Output saved at:\", output_path2)\n",
    "# ########################################################################################################################\n",
    "# ########################################################################################################################\n",
    "# ########################################################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e49ef1-56f3-4092-b2dc-2554786b3961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray\n",
    "import os\n",
    "\n",
    "# Path to the output NDVI TIFF\n",
    "output_tif3 = os.path.join(tif_dir, \"outputtile_sentinel_NDVI.tif\")\n",
    "\n",
    "# Load the NIR and Red band data\n",
    "nir_data = rioxarray.open_rasterio(output_path1)\n",
    "red_data = rioxarray.open_rasterio(output_path2)\n",
    "\n",
    "# Calculate NDVI\n",
    "ndvi = (nir_data - red_data) / (nir_data + red_data)\n",
    "ndvi = ndvi.fillna(0)  # Optional: handle NaNs by replacing them with zero\n",
    "\n",
    "# Set nodata values if necessary\n",
    "ndvi.rio.write_nodata(-9999, inplace=True)\n",
    "\n",
    "# Save the NDVI to a new TIFF file\n",
    "ndvi.rio.to_raster(output_tif3)\n",
    "\n",
    "print(f\"NDVI calculated and saved to {output_tif3}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2763694-3222-4d9a-bc25-f8712adc3412",
   "metadata": {},
   "source": [
    "# Generate Random Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7fc962-3719-4330-9d45-0c7869e60374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.plot import show\n",
    "from shapely.geometry import box, Point\n",
    "from geopandas import GeoDataFrame\n",
    "import numpy as np\n",
    "\n",
    "# Load the GeoTIFF file\n",
    "file_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\PCLCONUS\\Input\\PCLTILES\\pcltile_75000-35000.tif\"\n",
    "with rasterio.open(file_path) as src:\n",
    "    bounds = src.bounds\n",
    "    crs = src.crs\n",
    "    img = src.read(1)  # Read the first band\n",
    "\n",
    "# Create a box from the bounds\n",
    "rect = box(bounds.left, bounds.bottom, bounds.right, bounds.top)\n",
    "\n",
    "# Apply a negative buffer to contract the boundary of the rectangle\n",
    "buffered_rect = rect.buffer(-0.15)  # Adjust buffer size as needed\n",
    "\n",
    "# Generate random points within the buffered rectangle\n",
    "def generate_random_points(geometry, num_points):\n",
    "    points = []\n",
    "    min_x, min_y, max_x, max_y = geometry.bounds\n",
    "    while len(points) < num_points:\n",
    "        random_point = Point(np.random.uniform(min_x, max_x), np.random.uniform(min_y, max_y))\n",
    "        if random_point.within(geometry):\n",
    "            points.append(random_point)\n",
    "    return points\n",
    "\n",
    "random_points = generate_random_points(buffered_rect, 10000)\n",
    "\n",
    "# Convert these points to a GeoDataFrame\n",
    "gdf_points = GeoDataFrame(geometry=random_points, crs=crs)\n",
    "\n",
    "# Convert points GeoDataFrame to the same CRS as the raster for accurate overlay\n",
    "gdf_points = gdf_points.to_crs(crs)\n",
    "\n",
    "# Convert geometries from the CRS to WGS84 for latitude and longitude\n",
    "gdf_points_wgs84 = gdf_points.to_crs(epsg=4326)\n",
    "\n",
    "# Extract the latitude and longitude\n",
    "lat_long = gdf_points_wgs84.geometry.apply(lambda geom: (geom.y, geom.x)).tolist()\n",
    "\n",
    "# Plotting the raster and the points\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "show(img, ax=ax, transform=src.transform, cmap='gray')  # Show the raster\n",
    "gdf_points.plot(ax=ax, color='red', markersize=10)  # Plot the points\n",
    "plt.title('Random Points on Raster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09be9d7-e84a-4c54-806b-7a8b10471d32",
   "metadata": {},
   "source": [
    "# Create a training chip dataset using those random points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86edbc6-bd71-4e07-b419-1ffb0878b78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "import numpy as np\n",
    "\n",
    "# Paths to the raster files\n",
    "resampled_lidar_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\PCLCONUS\\Input\\scratch\\lidar\\output_resampled_dataLIDAR.tif\"\n",
    "resampled_dem_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\PCLCONUS\\Input\\scratch\\dem\\output_resampled_dataDEM.tif\"\n",
    "resampled_rivers_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\PCLCONUS\\Input\\scratch\\rivers\\output_resampled_dataRivers.tif\"\n",
    "resampled_roads_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\PCLCONUS\\Input\\scratch\\roads\\output_resampled_dataRoads.tif\"\n",
    "PCL_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\PCLCONUS\\Input\\PCLTILES\\pcltile_75000-35000.tif\"\n",
    "\n",
    "chip_size = 128  \n",
    "\n",
    "for i, (lat, lon) in enumerate(lat_long):\n",
    "    try:\n",
    "        paths = [resampled_lidar_path, resampled_dem_path, resampled_rivers_path, resampled_roads_path, PCL_path]\n",
    "        labels = ['lidar', 'dem', 'rivers', 'roads', 'pcllabels']\n",
    "        \n",
    "        for path, label in zip(paths, labels):\n",
    "            with rasterio.open(path) as src:\n",
    "                # Convert the geographic coordinates to pixel coordinates\n",
    "                col, row = src.index(lon, lat)\n",
    "                \n",
    "                # Calculate the window\n",
    "                window = Window(col - chip_size // 2, row - chip_size // 2, chip_size, chip_size)\n",
    "                \n",
    "                # Read the data within the window\n",
    "                chip_data = src.read(1, window=window)\n",
    "                \n",
    "                # Prepare metadata for the chip raster\n",
    "                out_meta = src.meta.copy()\n",
    "                out_meta.update({\n",
    "                    \"driver\": \"GTiff\",\n",
    "                    \"height\": chip_size,\n",
    "                    \"width\": chip_size,\n",
    "                    \"transform\": src.window_transform(window)\n",
    "                })\n",
    "\n",
    "                # Save the chip data to a new file\n",
    "                chip_output_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\PCLCONUS\\\\Input\\\\inputchips\\\\{label}\\\\{label.upper()}_Chip_{i}.tif\"\n",
    "\n",
    "                # Check if the chip data is valid before saving\n",
    "                if chip_data.shape == (chip_size, chip_size) and np.any(chip_data != src.nodata):\n",
    "                    with rasterio.open(chip_output_path, \"w\", **out_meta) as dest:\n",
    "                        dest.write(chip_data, 1)\n",
    "                    #print(f\"{label.capitalize()} chip {i} created and saved to: {chip_output_path}\")\n",
    "                else:\n",
    "                    print(f\"Skipping {label} chip {i} because it is not properly shaped or is filled with nodata.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing chip {i}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da40df41-7394-4712-b453-2ed8dc3aeffd",
   "metadata": {},
   "source": [
    "# Load chips and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f012c527-5962-48c0-bea3-ffe9fd6b4898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Paths to datasets\n",
    "featurepath1 = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\PCLCONUS\\Input\\inputchips\\LIDAR\"\n",
    "featurepath2 = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\PCLCONUS\\Input\\inputchips\\DEM\"\n",
    "featurepath3 = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\PCLCONUS\\Input\\inputchips\\ROADS\"\n",
    "featurepath4 = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\PCLCONUS\\Input\\inputchips\\RIVERS\"\n",
    "\n",
    "\n",
    "labelspath = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\PCLCONUS\\Input\\inputchips\\pcllabels\"\n",
    "\n",
    "# Function to load GeoTIFF images as numpy arrays\n",
    "def load_geotiff(path):\n",
    "    with rasterio.open(path) as src:\n",
    "        return src.read(1)\n",
    "\n",
    "# Load datasets\n",
    "hag_images = [load_geotiff(os.path.join(featurepath1, f)) for f in os.listdir(featurepath1) if f.endswith('.tif')]\n",
    "dem_images = [load_geotiff(os.path.join(featurepath2, f)) for f in os.listdir(featurepath2) if f.endswith('.tif')]\n",
    "roads_images = [load_geotiff(os.path.join(featurepath3, f)) for f in os.listdir(featurepath3) if f.endswith('.tif')]\n",
    "rivers_images = [load_geotiff(os.path.join(featurepath4, f)) for f in os.listdir(featurepath4) if f.endswith('.tif')]\n",
    "\n",
    "label_images = [load_geotiff(os.path.join(labelspath, f)) for f in os.listdir(labelspath) if f.endswith('.tif')]\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "hag_images = np.array(hag_images).astype('float32')\n",
    "dem_images = np.array(dem_images).astype('float32')\n",
    "roads_images = np.array(roads_images).astype('float32')\n",
    "rivers_images = np.array(rivers_images).astype('float32')\n",
    "\n",
    "label_images = np.array(label_images).astype('float32')\n",
    "\n",
    "# Normalize images independently\n",
    "hag_max = hag_images.max()\n",
    "dem_max = dem_images.max()\n",
    "roads_max = roads_images.max()\n",
    "rivers_max = rivers_images.max()\n",
    "\n",
    "\n",
    "hag_images /= hag_max\n",
    "dem_images /= dem_max\n",
    "roads_images /= roads_max\n",
    "rivers_images /= rivers_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c50041-d791-4b2e-85c6-eb36ef07b92f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f207be3-1fb3-41c9-b5b1-35f56cc01060",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Stack features along the last dimension\n",
    "feature_images = np.stack((hag_images, dem_images, roads_images, rivers_images), axis=-1)\n",
    "\n",
    "# Normalize labels if they range from 0 to 100\n",
    "label_images /= 100\n",
    "\n",
    "# Reshape labels for CNN input\n",
    "label_images = np.expand_dims(label_images, axis=-1)\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(16, (3, 3), activation='relu', input_shape=(128, 128, 4)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(128 * 128, activation='sigmoid'),\n",
    "    tf.keras.layers.Reshape((128, 128, 1))\n",
    "])\n",
    "\n",
    "# Define custom weights for each feature\n",
    "weights = np.array([1.0, 0.8, 0.5, 0.3])  # Example weights for each feature\n",
    "\n",
    "# Create sample weights based on the custom weights\n",
    "sample_weights = np.dot(feature_images, weights)\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "# Compile and train the model with sample weights\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "model.fit(feature_images, label_images, batch_size=128, epochs=10, validation_split=0.3, sample_weight=sample_weights)\n",
    "\n",
    "\n",
    "# lr = 0.01\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "# # Compile and train the model\n",
    "# model.compile(optimizer=optimizer, loss='mse')\n",
    "# model.fit(feature_images, label_images, batch_size=128, epochs=10, validation_split=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58fa79d-4b63-4f8c-803b-48dab9bc6dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hag_max)\n",
    "print(dem_max)\n",
    "print(roads_max)\n",
    "print(rivers_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42149e27-67e2-41ef-88e6-2bafbabe6369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec86c1f3-f52a-42f0-948b-6ff547bac159",
   "metadata": {},
   "source": [
    "# Load or save models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ba4a89-1aa0-4774-93d4-dd2efa7940d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --no-cache-dir --force-reinstall h5py\n",
    "\n",
    "#model.save('C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\SavedModels\\\\model_4_30_LR01.h5')  # Saves the model in HDF5 format\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "import h5py\n",
    "\n",
    "# Path to the model\n",
    "model_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\SavedModels\\model_4_30_LR01.h5\"\n",
    "#model_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\SavedModels\\model_4_30.h5\"\n",
    "#model_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\SavedModels\\model_4_30_LR0001.h5\"\n",
    "#model_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\SavedModels\\model_4_29.h5\"\n",
    "\n",
    "# Load the model\n",
    "model = load_model(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c48daae-cd37-4a70-aa38-330b8ba539d1",
   "metadata": {},
   "source": [
    "# Prep data for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c34cc6-1e31-4318-bf12-1f991f398bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this ensures all tiles are widthtile by heighttile\n",
    "import os\n",
    "from itertools import product\n",
    "import rasterio as rio\n",
    "from rasterio import windows\n",
    "\n",
    "in_path = 'C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\PCLCONUS\\\\Input\\\\scratch\\\\'\n",
    "input_filename1 = 'lidar\\\\output_resampled_dataLIDAR.tif'\n",
    "input_filename2 = 'dem\\\\output_resampled_dataDEM.tif'\n",
    "input_filename3 = 'roads\\\\output_resampled_dataRoads.tif'\n",
    "input_filename4 = 'rivers\\\\output_resampled_dataRivers.tif'\n",
    "\n",
    "out_path = 'C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\PCLCONUS\\\\Input\\\\inferencetiles\\\\hag\\\\'\n",
    "output_filename = 'tile_{}-{}.tif'\n",
    "widthtile = 128\n",
    "heighttile = 128\n",
    "\n",
    "\n",
    "def get_tiles(ds, width=widthtile, height=heighttile):\n",
    "    nols, nrows = ds.meta['width'], ds.meta['height']\n",
    "    offsets = product(range(0, nols, width), range(0, nrows, height))\n",
    "    #offsets = product(range(0, nols, 1000), range(0, nrows, 1000))\n",
    "\n",
    "    big_window = windows.Window(col_off=0, row_off=0, width=nols, height=nrows)\n",
    "    for col_off, row_off in offsets:\n",
    "        window = windows.Window(col_off=col_off, row_off=row_off, width=width, height=height).intersection(big_window)\n",
    "        transform = windows.transform(window, ds.transform)\n",
    "        yield window, transform\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with rio.open(os.path.join(in_path, input_filename1)) as inds:\n",
    "    tile_width, tile_height = widthtile, heighttile\n",
    "    nodata = inds.nodata  # Get the NoData value from the dataset\n",
    "    meta = inds.meta.copy()\n",
    "\n",
    "    for window, transform in get_tiles(inds):\n",
    "        if window.width == tile_width and window.height == tile_height:  # Check if the tile dimensions are as expected\n",
    "            data = inds.read(window=window)\n",
    "            if nodata is not None:\n",
    "                # Modified check for NoData to include tolerance for floating-point rasters\n",
    "                valid_data_mask = (data != nodata)\n",
    "            else:\n",
    "                # If NoData value is not set, consider all data as valid\n",
    "                valid_data_mask = (data == data)\n",
    "\n",
    "            if valid_data_mask.any():  # Check if there's any valid data within the tile\n",
    "                meta['transform'] = transform\n",
    "                meta['width'], meta['height'] = window.width, window.height\n",
    "                outpath = os.path.join(out_path, output_filename.format(int(window.col_off), int(window.row_off)))\n",
    "                with rio.open(outpath, 'w', **meta) as outds:\n",
    "                    outds.write(data)\n",
    "\n",
    "\n",
    "\n",
    "out_path = 'C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\PCLCONUS\\\\Input\\\\inferencetiles\\\\dem\\\\'\n",
    "#output_filename = 'tile_{}-{}.tif'\n",
    "#widthtile = 128\n",
    "#heighttile = 128\n",
    "\n",
    "with rio.open(os.path.join(in_path, input_filename2)) as inds:\n",
    "    tile_width, tile_height = widthtile, heighttile\n",
    "    nodata = inds.nodata  # Get the NoData value from the dataset\n",
    "    meta = inds.meta.copy()\n",
    "\n",
    "    for window, transform in get_tiles(inds):\n",
    "        if window.width == tile_width and window.height == tile_height:  # Check if the tile dimensions are as expected\n",
    "            data = inds.read(window=window)\n",
    "            if nodata is not None:\n",
    "                # Modified check for NoData to include tolerance for floating-point rasters\n",
    "                valid_data_mask = (data != nodata)\n",
    "            else:\n",
    "                # If NoData value is not set, consider all data as valid\n",
    "                valid_data_mask = (data == data)\n",
    "\n",
    "            if valid_data_mask.any():  # Check if there's any valid data within the tile\n",
    "                meta['transform'] = transform\n",
    "                meta['width'], meta['height'] = window.width, window.height\n",
    "                outpath = os.path.join(out_path, output_filename.format(int(window.col_off), int(window.row_off)))\n",
    "                with rio.open(outpath, 'w', **meta) as outds:\n",
    "                    outds.write(data)\n",
    "\n",
    "out_path = 'C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\PCLCONUS\\\\Input\\\\inferencetiles\\\\roads\\\\'\n",
    "#output_filename = 'tile_{}-{}.tif'\n",
    "#widthtile = 128\n",
    "#heighttile = 128\n",
    "\n",
    "with rio.open(os.path.join(in_path, input_filename3)) as inds:\n",
    "    tile_width, tile_height = widthtile, heighttile\n",
    "    nodata = inds.nodata  # Get the NoData value from the dataset\n",
    "    meta = inds.meta.copy()\n",
    "\n",
    "    for window, transform in get_tiles(inds):\n",
    "        if window.width == tile_width and window.height == tile_height:  # Check if the tile dimensions are as expected\n",
    "            data = inds.read(window=window)\n",
    "            if nodata is not None:\n",
    "                # Modified check for NoData to include tolerance for floating-point rasters\n",
    "                valid_data_mask = (data != nodata)\n",
    "            else:\n",
    "                # If NoData value is not set, consider all data as valid\n",
    "                valid_data_mask = (data == data)\n",
    "\n",
    "            if valid_data_mask.any():  # Check if there's any valid data within the tile\n",
    "                meta['transform'] = transform\n",
    "                meta['width'], meta['height'] = window.width, window.height\n",
    "                outpath = os.path.join(out_path, output_filename.format(int(window.col_off), int(window.row_off)))\n",
    "                with rio.open(outpath, 'w', **meta) as outds:\n",
    "                    outds.write(data)\n",
    "\n",
    "out_path = 'C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\PCLCONUS\\\\Input\\\\inferencetiles\\\\rivers\\\\'\n",
    "#output_filename = 'tile_{}-{}.tif'\n",
    "#widthtile = 128\n",
    "#heighttile = 128\n",
    "\n",
    "with rio.open(os.path.join(in_path, input_filename4)) as inds:\n",
    "    tile_width, tile_height = widthtile, heighttile\n",
    "    nodata = inds.nodata  # Get the NoData value from the dataset\n",
    "    meta = inds.meta.copy()\n",
    "\n",
    "    for window, transform in get_tiles(inds):\n",
    "        if window.width == tile_width and window.height == tile_height:  # Check if the tile dimensions are as expected\n",
    "            data = inds.read(window=window)\n",
    "            if nodata is not None:\n",
    "                # Modified check for NoData to include tolerance for floating-point rasters\n",
    "                valid_data_mask = (data != nodata)\n",
    "            else:\n",
    "                # If NoData value is not set, consider all data as valid\n",
    "                valid_data_mask = (data == data)\n",
    "\n",
    "            if valid_data_mask.any():  # Check if there's any valid data within the tile\n",
    "                meta['transform'] = transform\n",
    "                meta['width'], meta['height'] = window.width, window.height\n",
    "                outpath = os.path.join(out_path, output_filename.format(int(window.col_off), int(window.row_off)))\n",
    "                with rio.open(outpath, 'w', **meta) as outds:\n",
    "                    outds.write(data)\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538fc3ee-7f95-411c-8921-ca1d5d1ebe19",
   "metadata": {},
   "source": [
    "# Make a list of tile names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906a8b2b-64fc-46c0-b98a-60f08b6ec84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Define the directory path\n",
    "directory_path = 'C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\PCLCONUS\\\\Input\\\\inferencetiles\\\\hag'\n",
    "\n",
    "# Regular expression to extract the identifier part of the filename 'tile_{identifier}.tif'\n",
    "pattern = re.compile(r'tile_(\\d+-\\d+)\\.tif')\n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(directory_path)\n",
    "\n",
    "# Use a set to avoid duplicate identifiers\n",
    "identifiers = set()\n",
    "\n",
    "# Extract identifiers from filenames\n",
    "for file in files:\n",
    "    match = pattern.search(file)\n",
    "    if match:\n",
    "        identifiers.add(match.group(1))\n",
    "\n",
    "# Convert the set to a sorted list\n",
    "identifier_list = sorted(list(identifiers))\n",
    "print(len(identifier_list))\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64f6db5-0ca7-4db2-9128-f1541552b4bf",
   "metadata": {},
   "source": [
    "# Make Inference on those tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4276af70-f6d6-41d4-a89c-79eb04366988",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#tilename = '0-0'\n",
    "# input_hag_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\hag\\\\tile_{tilename}.tif\"\n",
    "# input_dem_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\dem\\\\tile_{tilename}.tif\"\n",
    "# input_roads_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\roads\\\\tile_{tilename}.tif\"\n",
    "# input_rivers_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\rivers\\\\tile_{tilename}.tif\"\n",
    "\n",
    "def load_and_preprocess_image(hag_path, dem_path, roads_path, rivers_path):\n",
    "    with rasterio.open(hag_path) as src:\n",
    "        hag_image = src.read(1)\n",
    "    with rasterio.open(dem_path) as src:\n",
    "        dem_image = src.read(1)\n",
    "    with rasterio.open(roads_path) as src:\n",
    "        roads_image = src.read(1)\n",
    "    with rasterio.open(rivers_path) as src:\n",
    "        rivers_image = src.read(1)\n",
    "\n",
    "    # Normalize and stack the images\n",
    "    hag_image = np.array(hag_image).astype('float32') / hag_max\n",
    "    dem_image = np.array(dem_image).astype('float32') / dem_max\n",
    "    roads_image = np.array(roads_image).astype('float32') / roads_max\n",
    "    rivers_image = np.array(rivers_image).astype('float32') / rivers_max\n",
    "\n",
    "    # Stack images along the last dimension\n",
    "    combined_image = np.stack([hag_image, dem_image, roads_image, rivers_image], axis=-1)\n",
    "\n",
    "    # Add batch dimension\n",
    "    combined_image = np.expand_dims(combined_image, axis=0)\n",
    "    return combined_image\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(identifier_list)):\n",
    "    tilename = identifier_list[i]\n",
    "    #print(tilename)\n",
    "    input_hag_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\PCLCONUS\\\\Input\\\\inferencetiles\\\\hag\\\\tile_{tilename}.tif\"\n",
    "    input_dem_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\PCLCONUS\\\\Input\\\\inferencetiles\\\\dem\\\\tile_{tilename}.tif\"\n",
    "    input_roads_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\PCLCONUS\\\\Input\\\\inferencetiles\\\\roads\\\\tile_{tilename}.tif\"\n",
    "    input_rivers_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\PCLCONUS\\\\Input\\\\inferencetiles\\\\rivers\\\\tile_{tilename}.tif\"\n",
    "\n",
    "\n",
    "    input_image = load_and_preprocess_image(input_hag_path, input_dem_path, input_roads_path, input_rivers_path)\n",
    "    predicted_image = model.predict(input_image)\n",
    "    predicted_image = np.squeeze(predicted_image)\n",
    "    \n",
    "    # Debug print to check if all outputs are the same\n",
    "    #print(\"Unique values in predicted output:\", np.unique(predicted_image))\n",
    "    \n",
    "    # Adjust the scaling factor based on how the labels were scaled during training\n",
    "    predicted_image *= 100\n",
    "    \n",
    "    output_image_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\PCLCONUS\\\\Input\\\\predictions\\\\predicted_tile_{tilename}.tif\"\n",
    "    \n",
    "    with rasterio.open(input_dem_path) as src: \n",
    "        profile = src.profile\n",
    "    \n",
    "    with rasterio.open(output_image_path, 'w', **profile) as dst:\n",
    "        dst.write(predicted_image.astype(rasterio.uint8), 1)\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8408231c-2510-41ce-b0de-4ebd5d9047ae",
   "metadata": {},
   "source": [
    "# Merge files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c239cb3-91c8-4641-9e0b-b3665dbe731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "\n",
    "# Input and output directories\n",
    "tif_dir = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\PCLCONUS\\Input\\predictions\"\n",
    "output_dir = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\PCLCONUS\\Input\\merged\"\n",
    "output_base_name = \"predMerged_CO_\"  # Base name for output files\n",
    "\n",
    "# Get a list of TIFF files\n",
    "tifs = glob.glob(os.path.join(tif_dir, \"*.tif\"))\n",
    "\n",
    "# Define chunk size for processing\n",
    "chunk_size = 300\n",
    "\n",
    "# Calculate the number of chunks needed\n",
    "num_chunks = len(tifs) // chunk_size\n",
    "if len(tifs) % chunk_size != 0:\n",
    "    num_chunks += 1  # Add one more chunk for the remaining files\n",
    "\n",
    "# Loop through the TIFF files in chunks\n",
    "for chunk_id in range(num_chunks):\n",
    "    # Calculate the start and end indices for the current chunk\n",
    "    start_idx = chunk_id * chunk_size\n",
    "    end_idx = min((chunk_id + 1) * chunk_size, len(tifs))\n",
    "\n",
    "    # Get the current chunk of TIFF files\n",
    "    chunk_tifs = tifs[start_idx:end_idx]\n",
    "    \n",
    "    # Prepare the output file name\n",
    "    output_tif = os.path.join(output_dir, f\"{output_base_name}{chunk_id + 1}.tif\")\n",
    "\n",
    "    # Prepare the gdal_merge command for the current chunk\n",
    "    merge_command_hag = [\n",
    "        \"python\",\n",
    "        \"C:\\\\Users\\\\smdur\\\\anaconda3\\\\envs\\\\globalpcl\\\\Scripts\\\\gdal_merge.py\",\n",
    "        \"--config\", \"CHECK_DISK_FREE_SPACE\", \"FALSE\",\n",
    "        #\"-ot\", \"Byte\",\n",
    "        \"-o\", output_tif,\n",
    "        \"-n\", \"-9999\",\n",
    "        \"-a_nodata\", \"-9999\",\n",
    "    ] + chunk_tifs\n",
    "\n",
    "    # Run the gdal_merge command for the current chunk\n",
    "    process_hag = subprocess.run(merge_command_hag, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    # Check if the command for the current chunk was successful\n",
    "    if process_hag.returncode != 0:\n",
    "        # An error occurred, print the error\n",
    "        print(f\"Error occurred while merging TIFF files for chunk {chunk_id + 1}:\")\n",
    "        print(process_hag.stderr)\n",
    "    else:\n",
    "        print(f\"TIFF files merged successfully for chunk {chunk_id + 1}. Output: {output_tif}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a83a5a-d3bd-4eb3-b6b3-d18336e31c1d",
   "metadata": {},
   "source": [
    "# Merge the Merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5a0b66-68f6-402e-9722-ec83eb90ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WORKS WITH NO BOARDER!\n",
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "\n",
    "#tif_dir = r\"D:\\GlobalPCL\\lidarHAG\"\n",
    "#output_tif = r\"D:\\GlobalPCL\\lidarHAG\\LIDAR_GEDI_CO.tif\"\n",
    "tif_dir = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\PCLCONUS\\Input\\merged\"\n",
    "output_tif  = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\PCLCONUS\\Input\\merged\\predMerged_PCL_london.tif\"\n",
    "\n",
    "\n",
    "tifs = glob.glob(os.path.join(tif_dir, \"*.tif\"))\n",
    "print(len(tifs))\n",
    "\n",
    "# Prepare the gdal_merge command for HAG\n",
    "merge_command_hag = [\n",
    "    \"python\",\n",
    "    \"C:\\\\Users\\\\smdur\\\\anaconda3\\\\envs\\\\globalpcl\\\\Scripts\\\\gdal_merge.py\",\n",
    "    \"--config\", \"CHECK_DISK_FREE_SPACE\", \"FALSE\",\n",
    "    #\"-ot\", \"Byte\",\n",
    "    \"-o\", output_tif,\n",
    "    \"-n\", \"-9999\",\n",
    "    \"-a_nodata\",\"-9999\",    \n",
    "] + tifs\n",
    "\n",
    "# Run the gdal_merge command for HAG and capture the output\n",
    "process_hag = subprocess.run(merge_command_hag, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "# Check if the command for HAG was successful\n",
    "if process_hag.returncode != 0:\n",
    "    # An error occurred, print the error\n",
    "    print(\"Error occurred while merging TIFF files HAG:\")\n",
    "    print(process_hag.stderr)\n",
    "else:\n",
    "    print(\"TIFF files merged successfully for HAG.\")\n",
    "\n",
    "for tif in tifs:\n",
    "    try:\n",
    "        os.remove(tif)\n",
    "        print(f\"Deleted {tif}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to delete {tif}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab80b07c-4b40-42f1-8164-b9d826bd7ace",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d5519a-aee7-4f77-a35c-0472e4792f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "# Path to the first TIFF file\n",
    "#tif_path1 = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\PCLCONUS\\Input\\merged\\predMerged_PCL.tif\"\n",
    "tif_path1 = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\PCLCONUS\\Input\\predMerged_PCL_lr0001.tif\"\n",
    "# Path to the second TIFF file\n",
    "tif_path2 = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\PCLCONUS\\Input\\PCLTILES\\pcltile_75000-35000.tif\"\n",
    "\n",
    "# Open the first TIFF file\n",
    "with rasterio.open(tif_path1) as src1:\n",
    "    # Read the raster data\n",
    "    raster_data1 = src1.read(1)\n",
    "    # Get the colormap for the first raster\n",
    "    cmap1 = plt.cm.viridis\n",
    "    \n",
    "# Open the second TIFF file\n",
    "with rasterio.open(tif_path2) as src2:\n",
    "    # Read the raster data\n",
    "    raster_data2 = src2.read(1)\n",
    "    # Get the colormap for the second raster\n",
    "    cmap2 = plt.cm.viridis\n",
    "    \n",
    "# Create a figure with subplots for the first raster\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))  # Reduced height to 8 inches\n",
    "\n",
    "# Plot the histogram for the first raster\n",
    "axes[0, 0].hist(raster_data1.flatten(), bins=60, color='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Pixel Values')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Histogram of predicted PCL Values')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Plot the first raster image\n",
    "img1 = axes[0, 1].imshow(raster_data1, cmap=cmap1, norm=Normalize(vmin=raster_data1.min(), vmax=raster_data1.max()))\n",
    "#axes[0, 1].set_title('Raster 1 Image')\n",
    "axes[0, 1].set_xlabel('X')\n",
    "axes[0, 1].set_ylabel('Y')\n",
    "fig.colorbar(img1, ax=axes[0, 1], label='Pixel Value')\n",
    "\n",
    "# Plot the histogram for the second raster\n",
    "axes[1, 0].hist(raster_data2.flatten(), bins=60, color='black', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Pixel Values')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Histogram of actual PCL Values')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Plot the second raster image\n",
    "img2 = axes[1, 1].imshow(raster_data2, cmap=cmap2, norm=Normalize(vmin=raster_data1.min(), vmax=raster_data1.max()))\n",
    "#axes[1, 1].set_title('Raster 2 Image')\n",
    "axes[1, 1].set_xlabel('X')\n",
    "axes[1, 1].set_ylabel('Y')\n",
    "fig.colorbar(img2, ax=axes[1, 1], label='Pixel Value')\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(r\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\gifs_figs\\\\modelpred1LR0001.png\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd370d96-2795-40bd-89f4-af7b27fc66fc",
   "metadata": {},
   "source": [
    "# GLOBAL INFRARENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75534b03-22ac-44e0-9d89-ffef95312975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.warp import transform\n",
    "import numpy as np\n",
    "import rioxarray\n",
    "from pystac_client import Client\n",
    "import planetary_computer\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "from osgeo import gdal\n",
    "\n",
    "def create_tile(center_lat, center_lon, reference_raster_path, output_tile_path):\n",
    "    # Open the reference raster to get its properties\n",
    "    with rasterio.open(reference_raster_path) as ref_raster:\n",
    "        ref_transform = ref_raster.transform\n",
    "        ref_crs = ref_raster.crs\n",
    "        ref_res = ref_transform[0]\n",
    "        ref_width = ref_raster.width\n",
    "        ref_height = ref_raster.height\n",
    "        new_profile = ref_raster.profile.copy()\n",
    "    \n",
    "    # Convert lat/lon to the same coordinate system as the reference raster\n",
    "    dst_crs = ref_crs\n",
    "    src_crs = 'EPSG:4326'\n",
    "    center_x, center_y = transform(src_crs, dst_crs, [center_lon], [center_lat])\n",
    "    center_x, center_y = center_x[0], center_y[0]\n",
    "    \n",
    "    # Define the new transform for the 5000x5000 tile\n",
    "    tile_width = 5000\n",
    "    tile_height = 5000\n",
    "    tile_transform = from_origin(\n",
    "        center_x - (tile_width // 2) * ref_res,\n",
    "        center_y + (tile_height // 2) * ref_res,\n",
    "        ref_res,\n",
    "        ref_res\n",
    "    )\n",
    "    \n",
    "    # Create the new raster data (a tile filled with ones)\n",
    "    tile_data = np.ones((tile_height, tile_width), dtype=np.uint8)\n",
    "    \n",
    "    # Define the new raster profile\n",
    "    new_profile.update({\n",
    "        'height': tile_height,\n",
    "        'width': tile_width,\n",
    "        'transform': tile_transform,\n",
    "        'dtype': 'uint8'\n",
    "    })\n",
    "    \n",
    "    # Write the new tile to a file\n",
    "    with rasterio.open(output_tile_path, 'w', **new_profile) as dst_raster:\n",
    "        dst_raster.write(tile_data, 1)\n",
    "\n",
    "# Define the center latitude and longitude\n",
    "#center_lat = 40.7128  # Example: New York City latitude\n",
    "#center_lon = -74.0060  # Example: New York City longitude\n",
    "\n",
    "# Define the center latitude and longitude\n",
    "center_lat = 51.5074  # Example: London\n",
    "center_lon = -0.1278  # Example: UK\n",
    "\n",
    "# Define the reference raster path and output tile path\n",
    "reference_raster_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\PCLCONUS\\Input\\PCLTILES\\pcltile_75000-40000.tif\"\n",
    "output_tile_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\scratch2\\outputs\\tileLondon.tif\"\n",
    "\n",
    "# Create the tile\n",
    "create_tile(center_lat, center_lon, reference_raster_path, output_tile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7ba355-a6d9-41f8-b754-a364e1f5c3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### for DEM_GLOBAL\n",
    "import rioxarray\n",
    "from pystac_client import Client\n",
    "import planetary_computer\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import rioxarray\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "from osgeo import gdal, ogr, osr\n",
    "\n",
    "# Path to the GeoTIFF file\n",
    "tif_path = output_tile_path\n",
    "\n",
    "# Directory for TIFF files\n",
    "tif_dir = r'C:\\Users\\smdur\\OneDrive\\Desktop\\scratch2\\dem'\n",
    "\n",
    "# Read the GeoTIFF file using rioxarray\n",
    "tif_data = rioxarray.open_rasterio(tif_path)\n",
    "\n",
    "# Get the spatial extent of the GeoTIFF\n",
    "min_lon, min_lat, max_lon, max_lat = tif_data.rio.bounds()\n",
    "bbox_of_interest = [min_lon, min_lat, max_lon, max_lat]  # Replace with your actual bounding box coordinates\n",
    "\n",
    "# Create a catalog search for the specified collection and bounding box\n",
    "catalog = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
    "search = catalog.search(collections=[\"cop-dem-glo-30\"], bbox=bbox_of_interest)\n",
    "items = list(search.get_items())\n",
    "print(f\"Returned {len(items)} items\")\n",
    "\n",
    "def process_item(item, idx):\n",
    "    #print(f\"Processing item {idx+1}/{len(items)}\")\n",
    "    signed_asset = planetary_computer.sign(item.assets[\"data\"])\n",
    "    data = (\n",
    "        rioxarray.open_rasterio(signed_asset.href)\n",
    "        .squeeze()\n",
    "        .drop(\"band\")\n",
    "    )\n",
    "    data.rio.write_crs(\"EPSG:4326\", inplace=True)\n",
    "\n",
    "    output_tif_path = os.path.join(tif_dir, f\"output_dataDEM_{idx}.tif\")\n",
    "    data.rio.to_raster(output_tif_path)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:  # Adjust max_workers based on your CPU\n",
    "    for i, item in enumerate(items):  # Change this line to process only the last three items\n",
    "        executor.submit(process_item, item, len(items) - 3 + i)  # Adjust index for correct labeling\n",
    "\n",
    "output_tif = os.path.join(tif_dir, f\"outputtile_DEM.tif\")\n",
    "\n",
    "# Get the list of TIFF files\n",
    "tifs = glob.glob(os.path.join(tif_dir, \"*.tif\"))\n",
    "\n",
    "# Prepare the gdal_merge command\n",
    "merge_command_hag = [\n",
    "    \"python\",\n",
    "    \"C:\\\\Users\\\\smdur\\\\anaconda3\\\\envs\\\\globalpcl\\\\Scripts\\\\gdal_merge.py\",\n",
    "    \"--config\", \"CHECK_DISK_FREE_SPACE\", \"FALSE\",\n",
    "    \"-o\", output_tif,\n",
    "    \"-n\", \"-9999\",\n",
    "    \"-a_nodata\", \"-9999\"\n",
    "] + tifs\n",
    "\n",
    "# Run the gdal_merge command and capture the output\n",
    "process_hag = subprocess.run(merge_command_hag, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "# Open the source (lidarHAG) and target (pcl) datasets\n",
    "src_ds = gdal.Open(output_tif, gdal.GA_ReadOnly)\n",
    "target_ds = gdal.Open(tif_path, gdal.GA_ReadOnly)\n",
    "\n",
    "# Get the Geotransform and Projection from the target dataset\n",
    "target_transform = target_ds.GetGeoTransform()\n",
    "target_projection = target_ds.GetProjection()\n",
    "target_cols = target_ds.RasterXSize\n",
    "target_rows = target_ds.RasterYSize\n",
    "\n",
    "# Create a new dataset for output with the same size and projection as the target\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "output_path = os.path.join(tif_dir, f\"output_resampled_dataDEMlondon.tif\")\n",
    "\n",
    "out_ds = driver.Create(output_path, target_cols, target_rows, 1, src_ds.GetRasterBand(1).DataType)\n",
    "out_ds.SetGeoTransform(target_transform)\n",
    "out_ds.SetProjection(target_projection)\n",
    "\n",
    "# Perform the resampling\n",
    "gdal.ReprojectImage(src_ds, out_ds, src_ds.GetProjection(), target_projection, gdal.GRA_Bilinear)\n",
    "\n",
    "src_ds = None\n",
    "target_ds = None\n",
    "out_ds = None\n",
    "\n",
    "\n",
    "if process_hag.returncode != 0:\n",
    "    print(process_hag.stderr)\n",
    "else:\n",
    "    for tif in tifs:\n",
    "        try:\n",
    "            os.remove(tif)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {tif}: {e}\")\n",
    "\n",
    "print(\"Resampling completed. Output saved at:\", output_path)\n",
    "\n",
    "os.remove(output_tif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f28936-2ab4-4d14-8831-bde87b5a81d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### for LIDAR_GLOBAL\n",
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "from osgeo import gdal, ogr, osr\n",
    "\n",
    "tif_dir = r\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\scratch2\\\\lidar2\"\n",
    "\n",
    "tifs = glob.glob(os.path.join(tif_dir, \"*.tif\"))\n",
    "\n",
    "output_tif = os.path.join(tif_dir, f\"outputtile_lidar.tif\")\n",
    "\n",
    "# Get the list of TIFF files\n",
    "tifs = glob.glob(os.path.join(tif_dir, \"*.tif\"))\n",
    "\n",
    "# Prepare the gdal_merge command\n",
    "merge_command_hag = [\n",
    "    \"python\",\n",
    "    \"C:\\\\Users\\\\smdur\\\\anaconda3\\\\envs\\\\globalpcl\\\\Scripts\\\\gdal_merge.py\",\n",
    "    \"--config\", \"CHECK_DISK_FREE_SPACE\", \"FALSE\",\n",
    "    \"-o\", output_tif,\n",
    "    \"-n\", \"255\",\n",
    "    \"-a_nodata\", \"255\"\n",
    "] + tifs\n",
    "\n",
    "# Run the gdal_merge command and capture the output\n",
    "process_hag = subprocess.run(merge_command_hag, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "# Open the source (lidarHAG) and target (pcl) datasets\n",
    "src_ds = gdal.Open(output_tif, gdal.GA_ReadOnly)\n",
    "target_ds = gdal.Open(tif_path, gdal.GA_ReadOnly)\n",
    "\n",
    "# Get the Geotransform and Projection from the target dataset\n",
    "target_transform = target_ds.GetGeoTransform()\n",
    "target_projection = target_ds.GetProjection()\n",
    "target_cols = target_ds.RasterXSize\n",
    "target_rows = target_ds.RasterYSize\n",
    "\n",
    "# Create a new dataset for output with the same size and projection as the target\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "#output_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\CNNPCLDEMO\\Input\\Resampled_DEM_CO.tif\"\n",
    "output_path = os.path.join(tif_dir, f\"output_resampled_dataLIDARlondon.tif\")\n",
    "\n",
    "out_ds = driver.Create(output_path, target_cols, target_rows, 1, src_ds.GetRasterBand(1).DataType)\n",
    "out_ds.SetGeoTransform(target_transform)\n",
    "out_ds.SetProjection(target_projection)\n",
    "\n",
    "# Perform the resampling\n",
    "gdal.ReprojectImage(src_ds, out_ds, src_ds.GetProjection(), target_projection, gdal.GRA_Bilinear)\n",
    "\n",
    "# Close the datasets\n",
    "src_ds = None\n",
    "target_ds = None\n",
    "out_ds = None\n",
    "\n",
    "os.remove(output_tif)\n",
    "\n",
    "print(\"Resampling completed. Output saved at:\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae1080a-7c3d-4f57-91d4-d9364ebbf774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Rivers\n",
    "import rioxarray\n",
    "import osmnx as ox\n",
    "\n",
    "# Path to the GeoTIFF file\n",
    "#tif_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\scratch2\\outputs\\tile3london.tif\"\n",
    "tif_path = output_tile_path\n",
    "# Directory for TIFF files\n",
    "#tif_dir = r'C:\\Users\\smdur\\OneDrive\\Desktop\\PCLCONUS\\Input\\scratch\\dem'\n",
    "\n",
    "# Read the GeoTIFF file using rioxarray\n",
    "dem_data = rioxarray.open_rasterio(tif_path)\n",
    "\n",
    "# Get the spatial extent of the GeoTIFF\n",
    "min_lon, min_lat, max_lon, max_lat = dem_data.rio.bounds()\n",
    "\n",
    "#custom_filter = '[\"waterway\"~\"river|streams|canals\"]'\n",
    "custom_filter = '[\"waterway\"~\"river\"]'\n",
    "\n",
    "bbox = (max_lat, min_lat, max_lon, min_lon)  \n",
    "graph = ox.graph_from_bbox(bbox=bbox, custom_filter=custom_filter, simplify=True, retain_all=True, truncate_by_edge=True)\n",
    "\n",
    "\n",
    "gdf = ox.graph_to_gdfs(graph, nodes=False)\n",
    "\n",
    "# Convert the graph to GeoDataFrames\n",
    "nodes, edges = ox.graph_to_gdfs(graph)\n",
    "\n",
    "import rasterio\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.windows import from_bounds\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "\n",
    "# Define the decay constant\n",
    "lambda_decay = 0.03  # This value might need adjustment based on your specific use case\n",
    "\n",
    "# Load the reference TIFF to use for transformations and metadata\n",
    "with rasterio.open(tif_path) as src:\n",
    "\n",
    "    min_lon, min_lat, max_lon, max_lat = src.bounds\n",
    "    \n",
    "    meta = src.meta.copy()\n",
    "\n",
    "    # Convert geographic coordinates to pixel coordinates\n",
    "    window = from_bounds(min_lon, min_lat, max_lon, max_lat, src.transform)\n",
    "    transform = rasterio.windows.transform(window, src.transform)\n",
    "\n",
    "    # Correct the window dimensions to integers\n",
    "    window_width = int(window.width)\n",
    "    window_height = int(window.height)\n",
    "\n",
    "    # Create an empty raster array for the size of the window\n",
    "    raster = np.zeros((window_height, window_width), dtype=np.uint8)\n",
    "\n",
    "    # Rasterize directly into the window shape\n",
    "    shapes = ((geom, 1) for geom in gdf['geometry'])\n",
    "    burned = rasterize(shapes, out=raster, fill=0, transform=transform, all_touched=True)\n",
    "\n",
    "    # Compute the Euclidean distance from each cell to the nearest non-zero cell\n",
    "    distance_grid = scipy.ndimage.distance_transform_edt(burned == 0)\n",
    "\n",
    "    # Apply exponential decay to the distance grid\n",
    "    decay_grid = np.exp(-lambda_decay * distance_grid)\n",
    "\n",
    "    # Prepare metadata for saving the decay grid\n",
    "    clipped_meta = meta.copy()\n",
    "    clipped_meta.update({\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": window_height,\n",
    "        \"width\": window_width,\n",
    "        \"transform\": transform,\n",
    "        \"dtype\": rasterio.float32,\n",
    "        \"count\": 1,\n",
    "        \"compress\": 'lzw'\n",
    "    })\n",
    "\n",
    "    # Save the decay grid to a new TIFF file\n",
    "    output_path = r'C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\scratch2\\\\rivers\\\\exponential_decay_CO_river.tif'\n",
    "    with rasterio.open(output_path, 'w', **clipped_meta) as dst:\n",
    "        dst.write(decay_grid.astype(np.float32), 1)\n",
    "\n",
    "from osgeo import gdal, ogr, osr\n",
    "\n",
    "# Define the paths to your rasters\n",
    "#lidarHAG_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\Input\\exponential_decay_CO_river.tif\"\n",
    "#pcl_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\Input\\pcl_west_wgs_CO_3.tif\"\n",
    "\n",
    "# Open the source (lidarHAG) and target (pcl) datasets\n",
    "src_ds = gdal.Open(output_path, gdal.GA_ReadOnly)\n",
    "target_ds = gdal.Open(tif_path, gdal.GA_ReadOnly)\n",
    "\n",
    "\n",
    "# Get the Geotransform and Projection from the target dataset\n",
    "target_transform = target_ds.GetGeoTransform()\n",
    "target_projection = target_ds.GetProjection()\n",
    "target_cols = target_ds.RasterXSize\n",
    "target_rows = target_ds.RasterYSize\n",
    "\n",
    "# Create a new dataset for output with the same size and projection as the target\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "output_path_resample = r\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\scratch2\\\\rivers\\\\output_resampled_dataRiverslondon.tif\"\n",
    "out_ds = driver.Create(output_path_resample, target_cols, target_rows, 1, src_ds.GetRasterBand(1).DataType)\n",
    "out_ds.SetGeoTransform(target_transform)\n",
    "out_ds.SetProjection(target_projection)\n",
    "\n",
    "# Perform the resampling\n",
    "gdal.ReprojectImage(src_ds, out_ds, src_ds.GetProjection(), target_projection, gdal.GRA_Bilinear)\n",
    "\n",
    "# Close the datasets\n",
    "src_ds = None\n",
    "target_ds = None\n",
    "out_ds = None\n",
    "\n",
    "print(\"Resampling completed. Output saved at:\", output_path)\n",
    "\n",
    "# Check if the file exists before trying to delete it\n",
    "if os.path.exists(output_path):\n",
    "    try:\n",
    "        os.remove(output_path)\n",
    "        print(f\"File {output_path} deleted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to delete {output_path}: {e}\")\n",
    "else:\n",
    "    print(f\"No file found at {output_path}, deletion not required.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd99b88c-39fd-468d-8095-dff455132575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Roads_GLOBAL\n",
    "import rioxarray\n",
    "# Path to the GeoTIFF file\n",
    "#tif_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\scratch2\\outputs\\tile3.tif\"\n",
    "tif_path = output_tile_path\n",
    "# Directory for TIFF files\n",
    "#tif_dir = r'C:\\Users\\smdur\\OneDrive\\Desktop\\PCLCONUS\\Input\\scratch\\dem'\n",
    "\n",
    "# Read the GeoTIFF file using rioxarray\n",
    "extent_data = rioxarray.open_rasterio(tif_path)\n",
    "\n",
    "# Get the spatial extent of the GeoTIFF\n",
    "min_lon, min_lat, max_lon, max_lat = extent_data.rio.bounds()\n",
    "\n",
    "graph = ox.graph_from_bbox(max_lat, min_lat, max_lon, min_lon, network_type='drive', simplify=True)\n",
    "\n",
    "gdf = ox.graph_to_gdfs(graph, nodes=False)\n",
    "\n",
    "# Convert the graph to GeoDataFrames\n",
    "nodes, edges = ox.graph_to_gdfs(graph)\n",
    "\n",
    "# Plot the waterway network\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "edges.plot(ax=ax, linewidth=1, edgecolor='blue')\n",
    "#plt.title('Waterways in Fort Collins')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import rasterio\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.windows import from_bounds\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "\n",
    "# Define the decay constant\n",
    "lambda_decay = 0.03  # This value might need adjustment based on your specific use case\n",
    "\n",
    "# Load the reference TIFF to use for transformations and metadata\n",
    "with rasterio.open(tif_path) as src:\n",
    "\n",
    "    min_lon, min_lat, max_lon, max_lat = src.bounds\n",
    "    \n",
    "    meta = src.meta.copy()\n",
    "\n",
    "    # Convert geographic coordinates to pixel coordinates\n",
    "    window = from_bounds(min_lon, min_lat, max_lon, max_lat, src.transform)\n",
    "    transform = rasterio.windows.transform(window, src.transform)\n",
    "\n",
    "    # Correct the window dimensions to integers\n",
    "    window_width = int(window.width)\n",
    "    window_height = int(window.height)\n",
    "\n",
    "    # Create an empty raster array for the size of the window\n",
    "    raster = np.zeros((window_height, window_width), dtype=np.uint8)\n",
    "\n",
    "    # Rasterize directly into the window shape\n",
    "    shapes = ((geom, 1) for geom in gdf['geometry'])\n",
    "    burned = rasterize(shapes, out=raster, fill=0, transform=transform, all_touched=True)\n",
    "\n",
    "    # Compute the Euclidean distance from each cell to the nearest non-zero cell\n",
    "    distance_grid = scipy.ndimage.distance_transform_edt(burned == 0)\n",
    "\n",
    "    # Apply exponential decay to the distance grid\n",
    "    decay_grid = np.exp(-lambda_decay * distance_grid)\n",
    "\n",
    "    # Prepare metadata for saving the decay grid\n",
    "    clipped_meta = meta.copy()\n",
    "    clipped_meta.update({\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": window_height,\n",
    "        \"width\": window_width,\n",
    "        \"transform\": transform,\n",
    "        \"dtype\": rasterio.float32,\n",
    "        \"count\": 1,\n",
    "        \"compress\": 'lzw'\n",
    "    })\n",
    "\n",
    "    # Save the decay grid to a new TIFF file\n",
    "    output_path = r'C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\scratch2\\\\roads\\\\exponential_decay_CO_roads.tif'\n",
    "    with rasterio.open(output_path, 'w', **clipped_meta) as dst:\n",
    "        dst.write(decay_grid.astype(np.float32), 1)\n",
    "\n",
    "from osgeo import gdal, ogr, osr\n",
    "\n",
    "# Open the source (lidarHAG) and target (pcl) datasets\n",
    "src_ds = gdal.Open(output_path, gdal.GA_ReadOnly)\n",
    "target_ds = gdal.Open(tif_path, gdal.GA_ReadOnly)\n",
    "\n",
    "\n",
    "# Get the Geotransform and Projection from the target dataset\n",
    "target_transform = target_ds.GetGeoTransform()\n",
    "target_projection = target_ds.GetProjection()\n",
    "target_cols = target_ds.RasterXSize\n",
    "target_rows = target_ds.RasterYSize\n",
    "\n",
    "# Create a new dataset for output with the same size and projection as the target\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "output_path_resample = r\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\scratch2\\\\roads\\\\output_resampled_dataRoadslondon.tif\"\n",
    "out_ds = driver.Create(output_path_resample, target_cols, target_rows, 1, src_ds.GetRasterBand(1).DataType)\n",
    "out_ds.SetGeoTransform(target_transform)\n",
    "out_ds.SetProjection(target_projection)\n",
    "\n",
    "# Perform the resampling\n",
    "gdal.ReprojectImage(src_ds, out_ds, src_ds.GetProjection(), target_projection, gdal.GRA_Bilinear)\n",
    "\n",
    "# Close the datasets\n",
    "src_ds = None\n",
    "target_ds = None\n",
    "out_ds = None\n",
    "\n",
    "print(\"Resampling completed. Output saved at:\", output_path)\n",
    "\n",
    "# Check if the file exists before trying to delete it\n",
    "if os.path.exists(output_path):\n",
    "    try:\n",
    "        os.remove(output_path)\n",
    "        print(f\"File {output_path} deleted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to delete {output_path}: {e}\")\n",
    "else:\n",
    "    print(f\"No file found at {output_path}, deletion not required.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ceaffc-7fc6-467e-8ea1-07ff1fdf227b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c567a2-9770-4394-b5f8-89b6e17c4495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this ensures all tiles are widthtile by heighttile\n",
    "import os\n",
    "from itertools import product\n",
    "import rasterio as rio\n",
    "from rasterio import windows\n",
    "\n",
    "in_path = 'C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\scratch2\\\\'\n",
    "input_filename1 = 'lidar\\\\output_resampled_dataLIDARlondon.tif'\n",
    "input_filename2 = 'dem\\\\output_resampled_dataDEMlondon.tif'\n",
    "input_filename3 = 'roads\\\\output_resampled_dataRoads.tif'\n",
    "input_filename4 = 'rivers\\\\output_resampled_dataRivers.tif'\n",
    "\n",
    "out_path = 'C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\scratch2\\\\outputinferencetiles\\\\hag2\\\\'\n",
    "output_filename = 'tile_{}-{}.tif'\n",
    "widthtile = 128\n",
    "heighttile = 128\n",
    "\n",
    "\n",
    "def get_tiles(ds, width=widthtile, height=heighttile):\n",
    "    nols, nrows = ds.meta['width'], ds.meta['height']\n",
    "    offsets = product(range(0, nols, width), range(0, nrows, height))\n",
    "    #offsets = product(range(0, nols, 1000), range(0, nrows, 1000))\n",
    "\n",
    "    big_window = windows.Window(col_off=0, row_off=0, width=nols, height=nrows)\n",
    "    for col_off, row_off in offsets:\n",
    "        window = windows.Window(col_off=col_off, row_off=row_off, width=width, height=height).intersection(big_window)\n",
    "        transform = windows.transform(window, ds.transform)\n",
    "        yield window, transform\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with rio.open(os.path.join(in_path, input_filename1)) as inds:\n",
    "    tile_width, tile_height = widthtile, heighttile\n",
    "    nodata = inds.nodata  # Get the NoData value from the dataset\n",
    "    meta = inds.meta.copy()\n",
    "\n",
    "    for window, transform in get_tiles(inds):\n",
    "        if window.width == tile_width and window.height == tile_height:  # Check if the tile dimensions are as expected\n",
    "            data = inds.read(window=window)\n",
    "            if nodata is not None:\n",
    "                # Modified check for NoData to include tolerance for floating-point rasters\n",
    "                valid_data_mask = (data != nodata)\n",
    "            else:\n",
    "                # If NoData value is not set, consider all data as valid\n",
    "                valid_data_mask = (data == data)\n",
    "\n",
    "            if valid_data_mask.any():  # Check if there's any valid data within the tile\n",
    "                meta['transform'] = transform\n",
    "                meta['width'], meta['height'] = window.width, window.height\n",
    "                outpath = os.path.join(out_path, output_filename.format(int(window.col_off), int(window.row_off)))\n",
    "                with rio.open(outpath, 'w', **meta) as outds:\n",
    "                    outds.write(data)\n",
    "\n",
    "\n",
    "\n",
    "out_path = 'C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\scratch2\\\\outputinferencetiles\\\\dem2\\\\'\n",
    "#output_filename = 'tile_{}-{}.tif'\n",
    "#widthtile = 128\n",
    "#heighttile = 128\n",
    "\n",
    "with rio.open(os.path.join(in_path, input_filename2)) as inds:\n",
    "    tile_width, tile_height = widthtile, heighttile\n",
    "    nodata = inds.nodata  # Get the NoData value from the dataset\n",
    "    meta = inds.meta.copy()\n",
    "\n",
    "    for window, transform in get_tiles(inds):\n",
    "        if window.width == tile_width and window.height == tile_height:  # Check if the tile dimensions are as expected\n",
    "            data = inds.read(window=window)\n",
    "            if nodata is not None:\n",
    "                # Modified check for NoData to include tolerance for floating-point rasters\n",
    "                valid_data_mask = (data != nodata)\n",
    "            else:\n",
    "                # If NoData value is not set, consider all data as valid\n",
    "                valid_data_mask = (data == data)\n",
    "\n",
    "            if valid_data_mask.any():  # Check if there's any valid data within the tile\n",
    "                meta['transform'] = transform\n",
    "                meta['width'], meta['height'] = window.width, window.height\n",
    "                outpath = os.path.join(out_path, output_filename.format(int(window.col_off), int(window.row_off)))\n",
    "                with rio.open(outpath, 'w', **meta) as outds:\n",
    "                    outds.write(data)\n",
    "\n",
    "out_path = 'C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\scratch2\\\\outputinferencetiles\\\\roads2\\\\'\n",
    "#output_filename = 'tile_{}-{}.tif'\n",
    "#widthtile = 128\n",
    "#heighttile = 128\n",
    "\n",
    "with rio.open(os.path.join(in_path, input_filename3)) as inds:\n",
    "    tile_width, tile_height = widthtile, heighttile\n",
    "    nodata = inds.nodata  # Get the NoData value from the dataset\n",
    "    meta = inds.meta.copy()\n",
    "\n",
    "    for window, transform in get_tiles(inds):\n",
    "        if window.width == tile_width and window.height == tile_height:  # Check if the tile dimensions are as expected\n",
    "            data = inds.read(window=window)\n",
    "            if nodata is not None:\n",
    "                # Modified check for NoData to include tolerance for floating-point rasters\n",
    "                valid_data_mask = (data != nodata)\n",
    "            else:\n",
    "                # If NoData value is not set, consider all data as valid\n",
    "                valid_data_mask = (data == data)\n",
    "\n",
    "            if valid_data_mask.any():  # Check if there's any valid data within the tile\n",
    "                meta['transform'] = transform\n",
    "                meta['width'], meta['height'] = window.width, window.height\n",
    "                outpath = os.path.join(out_path, output_filename.format(int(window.col_off), int(window.row_off)))\n",
    "                with rio.open(outpath, 'w', **meta) as outds:\n",
    "                    outds.write(data)\n",
    "\n",
    "out_path = 'C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\scratch2\\\\outputinferencetiles\\\\rivers2\\\\'\n",
    "#output_filename = 'tile_{}-{}.tif'\n",
    "#widthtile = 128\n",
    "#heighttile = 128\n",
    "\n",
    "with rio.open(os.path.join(in_path, input_filename4)) as inds:\n",
    "    tile_width, tile_height = widthtile, heighttile\n",
    "    nodata = inds.nodata  # Get the NoData value from the dataset\n",
    "    meta = inds.meta.copy()\n",
    "\n",
    "    for window, transform in get_tiles(inds):\n",
    "        if window.width == tile_width and window.height == tile_height:  # Check if the tile dimensions are as expected\n",
    "            data = inds.read(window=window)\n",
    "            if nodata is not None:\n",
    "                # Modified check for NoData to include tolerance for floating-point rasters\n",
    "                valid_data_mask = (data != nodata)\n",
    "            else:\n",
    "                # If NoData value is not set, consider all data as valid\n",
    "                valid_data_mask = (data == data)\n",
    "\n",
    "            if valid_data_mask.any():  # Check if there's any valid data within the tile\n",
    "                meta['transform'] = transform\n",
    "                meta['width'], meta['height'] = window.width, window.height\n",
    "                outpath = os.path.join(out_path, output_filename.format(int(window.col_off), int(window.row_off)))\n",
    "                with rio.open(outpath, 'w', **meta) as outds:\n",
    "                    outds.write(data)\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d921dae0-8497-45ac-b03f-9affbcd39ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Define the directory path\n",
    "directory_path = 'C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\scratch2\\\\outputinferencetiles\\\\hag2'\n",
    "\n",
    "# Regular expression to extract the identifier part of the filename 'tile_{identifier}.tif'\n",
    "pattern = re.compile(r'tile_(\\d+-\\d+)\\.tif')\n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(directory_path)\n",
    "\n",
    "# Use a set to avoid duplicate identifiers\n",
    "identifiers = set()\n",
    "\n",
    "# Extract identifiers from filenames\n",
    "for file in files:\n",
    "    match = pattern.search(file)\n",
    "    if match:\n",
    "        identifiers.add(match.group(1))\n",
    "\n",
    "# Convert the set to a sorted list\n",
    "identifier_list = sorted(list(identifiers))\n",
    "print(len(identifier_list))\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691d668c-923f-4101-878b-ca6587f6a5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b00ff71-932d-4ab1-851d-962955f09298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import h5py\n",
    "\n",
    "# Path to the model\n",
    "#model_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\SavedModels\\model_4_30_LR01.h5\"\n",
    "#model_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\SavedModels\\model_4_30.h5\"\n",
    "#model_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\SavedModels\\model_4_30_LR0001.h5\"\n",
    "#model_path = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\GLOBALPCL\\SavedModels\\model_4_29.h5\"\n",
    "\n",
    "#model_path = r\"D:\\SandraComputer\\GLOBALPCL\\SavedModels\\model_4_30.h5\"\n",
    "#model_path = r\"D:\\SandraComputer\\GLOBALPCL\\SavedModels\\model_4_30_LR0001.h5\"\n",
    "model_path = r\"D:\\SandraComputer\\GLOBALPCL\\SavedModels\\model_4_30_LR01.h5\"\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336e416f-ba3f-4599-a08d-8657edc988f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#tilename = '0-0'\n",
    "# input_hag_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\hag\\\\tile_{tilename}.tif\"\n",
    "# input_dem_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\dem\\\\tile_{tilename}.tif\"\n",
    "# input_roads_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\roads\\\\tile_{tilename}.tif\"\n",
    "# input_rivers_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\GLOBALPCL\\\\CNNPCLDEMO\\\\inferencetiles\\\\rivers\\\\tile_{tilename}.tif\"\n",
    "\n",
    "def load_and_preprocess_image(hag_path, dem_path, roads_path, rivers_path):\n",
    "    with rasterio.open(hag_path) as src:\n",
    "        hag_image = src.read(1)\n",
    "    with rasterio.open(dem_path) as src:\n",
    "        dem_image = src.read(1)\n",
    "    with rasterio.open(roads_path) as src:\n",
    "        roads_image = src.read(1)\n",
    "    with rasterio.open(rivers_path) as src:\n",
    "        rivers_image = src.read(1)\n",
    "\n",
    "    # Normalize and stack the images\n",
    "    hag_image = np.array(hag_image).astype('float32') / hag_max\n",
    "    dem_image = np.array(dem_image).astype('float32') / dem_max\n",
    "    roads_image = np.array(roads_image).astype('float32') / roads_max\n",
    "    rivers_image = np.array(rivers_image).astype('float32') / rivers_max\n",
    "\n",
    "    # Stack images along the last dimension\n",
    "    combined_image = np.stack([hag_image, dem_image, roads_image, rivers_image], axis=-1)\n",
    "\n",
    "    # Add batch dimension\n",
    "    combined_image = np.expand_dims(combined_image, axis=0)\n",
    "    return combined_image\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(identifier_list)):\n",
    "    tilename = identifier_list[i]\n",
    "    #print(tilename)\n",
    "    input_hag_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\scratch2\\\\outputinferencetiles\\\\hag2\\\\tile_{tilename}.tif\"\n",
    "    input_dem_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\scratch2\\\\outputinferencetiles\\\\dem2\\\\tile_{tilename}.tif\"\n",
    "    input_roads_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\scratch2\\\\outputinferencetiles\\\\roads2\\\\tile_{tilename}.tif\"\n",
    "    input_rivers_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\scratch2\\\\outputinferencetiles\\\\rivers2\\\\tile_{tilename}.tif\"\n",
    "\n",
    "\n",
    "    input_image = load_and_preprocess_image(input_hag_path, input_dem_path, input_roads_path, input_rivers_path)\n",
    "    predicted_image = model.predict(input_image)\n",
    "    predicted_image = np.squeeze(predicted_image)\n",
    "    \n",
    "    # Debug print to check if all outputs are the same\n",
    "    #print(\"Unique values in predicted output:\", np.unique(predicted_image))\n",
    "    \n",
    "    # Adjust the scaling factor based on how the labels were scaled during training\n",
    "    predicted_image *= 100\n",
    "    \n",
    "    output_image_path = f\"C:\\\\Users\\\\smdur\\\\OneDrive\\\\Desktop\\\\scratch2\\\\predictions\\\\predicted_tile_{tilename}.tif\"\n",
    "    \n",
    "    with rasterio.open(input_dem_path) as src: \n",
    "        profile = src.profile\n",
    "    \n",
    "    with rasterio.open(output_image_path, 'w', **profile) as dst:\n",
    "        dst.write(predicted_image.astype(rasterio.uint8), 1)\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82528c16-3006-4c71-a959-5990951948dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "\n",
    "# Input and output directories\n",
    "tif_dir = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\scratch2\\predictions\"\n",
    "output_dir = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\scratch2\\outputpred\"\n",
    "output_base_name = \"predMerged_CO_\"  # Base name for output files\n",
    "\n",
    "# Get a list of TIFF files\n",
    "tifs = glob.glob(os.path.join(tif_dir, \"*.tif\"))\n",
    "\n",
    "# Define chunk size for processing\n",
    "chunk_size = 300\n",
    "\n",
    "# Calculate the number of chunks needed\n",
    "num_chunks = len(tifs) // chunk_size\n",
    "if len(tifs) % chunk_size != 0:\n",
    "    num_chunks += 1  # Add one more chunk for the remaining files\n",
    "\n",
    "# Loop through the TIFF files in chunks\n",
    "for chunk_id in range(num_chunks):\n",
    "    # Calculate the start and end indices for the current chunk\n",
    "    start_idx = chunk_id * chunk_size\n",
    "    end_idx = min((chunk_id + 1) * chunk_size, len(tifs))\n",
    "\n",
    "    # Get the current chunk of TIFF files\n",
    "    chunk_tifs = tifs[start_idx:end_idx]\n",
    "    \n",
    "    # Prepare the output file name\n",
    "    output_tif = os.path.join(output_dir, f\"{output_base_name}{chunk_id + 1}.tif\")\n",
    "\n",
    "    # Prepare the gdal_merge command for the current chunk\n",
    "    merge_command_hag = [\n",
    "        \"python\",\n",
    "        \"C:\\\\Users\\\\smdur\\\\anaconda3\\\\envs\\\\globalpcl\\\\Scripts\\\\gdal_merge.py\",\n",
    "        \"--config\", \"CHECK_DISK_FREE_SPACE\", \"FALSE\",\n",
    "        #\"-ot\", \"Byte\",\n",
    "        \"-o\", output_tif,\n",
    "        \"-n\", \"-9999\",\n",
    "        \"-a_nodata\", \"-9999\",\n",
    "    ] + chunk_tifs\n",
    "\n",
    "    # Run the gdal_merge command for the current chunk\n",
    "    process_hag = subprocess.run(merge_command_hag, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    # Check if the command for the current chunk was successful\n",
    "    if process_hag.returncode != 0:\n",
    "        # An error occurred, print the error\n",
    "        print(f\"Error occurred while merging TIFF files for chunk {chunk_id + 1}:\")\n",
    "        print(process_hag.stderr)\n",
    "    else:\n",
    "        print(f\"TIFF files merged successfully for chunk {chunk_id + 1}. Output: {output_tif}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3fc771-d7b3-446d-9776-a8a57bcd4edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### WORKS WITH NO BOARDER!\n",
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "\n",
    "#tif_dir = r\"D:\\GlobalPCL\\lidarHAG\"\n",
    "#output_tif = r\"D:\\GlobalPCL\\lidarHAG\\LIDAR_GEDI_CO.tif\"\n",
    "tif_dir = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\scratch2\\outputpred\"\n",
    "output_tif  = r\"C:\\Users\\smdur\\OneDrive\\Desktop\\scratch2\\outputs\\predMerged_PCL_5_13_m1_london.tif\"\n",
    "\n",
    "\n",
    "tifs = glob.glob(os.path.join(tif_dir, \"*.tif\"))\n",
    "print(len(tifs))\n",
    "\n",
    "# Prepare the gdal_merge command for HAG\n",
    "merge_command_hag = [\n",
    "    \"python\",\n",
    "    \"C:\\\\Users\\\\smdur\\\\anaconda3\\\\envs\\\\globalpcl\\\\Scripts\\\\gdal_merge.py\",\n",
    "    \"--config\", \"CHECK_DISK_FREE_SPACE\", \"FALSE\",\n",
    "    #\"-ot\", \"Byte\",\n",
    "    \"-o\", output_tif,\n",
    "    \"-n\", \"-9999\",\n",
    "    \"-a_nodata\",\"-9999\",    \n",
    "] + tifs\n",
    "\n",
    "# Run the gdal_merge command for HAG and capture the output\n",
    "process_hag = subprocess.run(merge_command_hag, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "# Check if the command for HAG was successful\n",
    "if process_hag.returncode != 0:\n",
    "    # An error occurred, print the error\n",
    "    print(\"Error occurred while merging TIFF files HAG:\")\n",
    "    print(process_hag.stderr)\n",
    "else:\n",
    "    print(\"TIFF files merged successfully for HAG.\")\n",
    "\n",
    "for tif in tifs:\n",
    "    try:\n",
    "        os.remove(tif)\n",
    "        print(f\"Deleted {tif}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to delete {tif}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ca643e-382a-4144-b1d4-3f277b1b360f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
